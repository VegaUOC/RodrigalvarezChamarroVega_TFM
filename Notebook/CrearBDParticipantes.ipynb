{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FyHUPeXI_q7"
      },
      "outputs": [],
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Cambiar el directorio de trabajo\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Define the TFM directory path\n",
        "TFM_PATH = '/content/drive/My Drive/TFM'\n",
        "\n",
        "# Change the current working directory to the TFM directory\n",
        "os.chdir(TFM_PATH)\n",
        "print(f\"Current working directory changed to: {os.getcwd()}\")\n",
        "\n",
        "# Add the TFM directory to the Python system path\n",
        "if TFM_PATH not in sys.path:\n",
        "    sys.path.append(TFM_PATH)\n",
        "    print(f\"'{TFM_PATH}' added to Python system path.\")\n",
        "else:\n",
        "    print(f\"'{TFM_PATH}' is already in Python system path.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35m9BEgAHAYa"
      },
      "outputs": [],
      "source": [
        "# Importar Librerias\n",
        "import extractData\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HHT6mBZHf-N"
      },
      "outputs": [],
      "source": [
        "# Definición de variables\n",
        "data_path = TFM_PATH + '/data/'\n",
        "survey_path = 'surveys/'\n",
        "participant_data_path = 'participant_genetic_data'\n",
        "\n",
        "participants_hypertension_file = \"participantes_hipertension.csv\"\n",
        "participants_control_file = \"participantes_control.csv\"\n",
        "metadata_file = \"public_genetic_data.csv\"\n",
        "\n",
        "participants_generalsurvey_file = \"PGPParticipantSurvey-20181010220019.csv\"\n",
        "participants_basicPhenoSurvey_file = 'PGPBasicPhenotypesSurvey2015-20181010214636.csv'\n",
        "participants_circulatory_file = \"PGPTrait&DiseaseSurvey2012_CirculatorySystem-20181010220109.csv\"\n",
        "participants_cancer_file = \"PGPTrait&DiseaseSurvey2012_Cancers-20181010220037.csv\"\n",
        "participants_endocrine_file = 'PGPTrait&DiseaseSurvey2012_Endocrine,Metabolic,Nutritional,AndImmunity-20181010220044.csv'\n",
        "participants_blood_file ='PGPTrait&DiseaseSurvey2012_Blood-20181010220050.csv'\n",
        "participants_nervous_file  = 'PGPTrait&DiseaseSurvey2012_NervousSystem-20181010220056.csv'\n",
        "participants_senses_file = 'PGPTrait&DiseaseSurvey2012_VisionAndHearing-20181010220103.csv'\n",
        "participants_respiratory_file = 'PGPTrait&DiseaseSurvey2012_RespiratorySystem-20181010220114.csv'\n",
        "participants_digestive_file = 'PGPTrait&DiseaseSurvey2012_DigestiveSystem-20181010214607.csv'\n",
        "participants_genitourinary_file = 'PGPTrait&DiseaseSurvey2012_GenitourinarySystems-20181010214612.csv'\n",
        "participants_skin_file = 'PGPTrait&DiseaseSurvey2012_SkinAndSubcutaneousTissue-20181010214618.csv'\n",
        "participants_muscle_file = 'PGPTrait&DiseaseSurvey2012_MusculoskeletalSystemAndConnectiveTissue-20181010214624.csv'\n",
        "participants_congenital_file = 'PGPTrait&DiseaseSurvey2012_CongenitalTraitsAndAnomalies-20181010214629.csv'\n",
        "\n",
        "col_name_participant = \"Participant\"\n",
        "col_name_conditions = \"Have you ever been diagnosed with one of the following conditions?\"\n",
        "sep = ','\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfs5f9DQThFj"
      },
      "outputs": [],
      "source": [
        "# Función para calcular y agrupar la edad\n",
        "def calculate_and_group_age(row):\n",
        "    year_of_birth_val = row['Year of birth']\n",
        "    timestamp_year = row['Timestamp'].year\n",
        "\n",
        "    if pd.isna(year_of_birth_val):\n",
        "        return np.nan\n",
        "\n",
        "    # Si ya es un rango de edad, se devuelve tal cual\n",
        "    if 'years' in str(year_of_birth_val) or '-' in str(year_of_birth_val):\n",
        "        return year_of_birth_val\n",
        "\n",
        "    try:\n",
        "        birth_year = int(year_of_birth_val)\n",
        "        age = timestamp_year - birth_year\n",
        "\n",
        "        if 21 <= age <= 29:\n",
        "            return '21-29 years'\n",
        "        elif 30 <= age <= 39:\n",
        "            return '30-39 years'\n",
        "        elif 40 <= age <= 49:\n",
        "            return '40-49 years'\n",
        "        elif 50 <= age <= 59:\n",
        "            return '50-59 years'\n",
        "        elif 60 <= age <= 69:\n",
        "            return '60-69 years'\n",
        "        elif 70 <= age <= 79:\n",
        "            return '70-79 years'\n",
        "        elif 80 <= age <= 89:\n",
        "            return '80-89 years'\n",
        "        elif 90 <= age <= 99:\n",
        "            return '90-99 years'\n",
        "        elif age < 21:\n",
        "            return '<21 years'\n",
        "        else:\n",
        "            return '100+ years'\n",
        "\n",
        "    except ValueError:\n",
        "        # Si no es un año numérico ni un rango de edad, se devuelve el valor original\n",
        "        return year_of_birth_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Apbyx2TUd_ac"
      },
      "outputs": [],
      "source": [
        "# Clasificar los ancestros según el país de Origen\n",
        "def classify_ancestry_by_country(country):\n",
        "    if pd.isna(country):\n",
        "        return np.nan\n",
        "    country = str(country).strip()\n",
        "    if country in ['United States', 'Canada','United States Minor Outlying Islands']:\n",
        "        return 'North America'\n",
        "    elif country in ['Spain', 'Poland', 'Austria', 'Lithuania', 'Belarus', 'Aland Islands', 'Italy', 'Hungary', 'United Kingdom', 'Germany',\n",
        "                     'Latvia', 'France', 'Russian Federation', 'Czech Republic', 'Ukraine', 'Estonia', 'Netherlands', 'Ireland', 'Denmark', 'Norway',\n",
        "                     'Sweden','Bulgaria','Serbia','Macedonia, The Former Yugoslav Republic Of','Portugal','Iceland','Belgium','Switzerland','Greece',\n",
        "                     'Finland', 'Slovenia','Slovakia','Isle of Man','Georgia','Romania','Croatia']:\n",
        "        return 'Europe'\n",
        "    elif country in ['India','Sri Lanka','Pakistan','Afghanistan']:\n",
        "        return 'South Asia'\n",
        "    elif country in ['Honduras', 'Mexico','Puerto Rico','Trinidad and Tobago']:\n",
        "        return 'Central America'\n",
        "    elif country in ['Brazil','Peru','Colombia','Venezuela','Argentina']:\n",
        "        return 'South America'\n",
        "    elif country in ['Australia', 'New Zealand', 'British Indian Ocean Territory']:\n",
        "        return 'Oceania'\n",
        "    elif country in ['Turkey', 'Armenia','Lebanon','Syrian Arab Republic','Israel']:\n",
        "        return 'Western Asia'\n",
        "    elif country in ['China','Japan']:\n",
        "        return 'East Asia'\n",
        "    elif country in ['Viet Nam', 'Philippines']:\n",
        "        return 'Southeast Asia'\n",
        "    elif country in ['Ethiopia','Morocco','Tanzania, United Republic of']:\n",
        "        return 'Africa'\n",
        "    else:\n",
        "        return country"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipf3Lja5toNc"
      },
      "outputs": [],
      "source": [
        "def convert_height_to_cm(height_str):\n",
        "    if pd.isna(height_str):\n",
        "        return np.nan\n",
        "\n",
        "    # Convertir a string para asegurar el procesamiento\n",
        "    height_str = str(height_str).strip()\n",
        "\n",
        "    # Verificar si el formato es 'X'Y\"'\n",
        "    if \"'\" in height_str and '\"' in height_str:\n",
        "        try:\n",
        "            parts = height_str.split(\"'\")\n",
        "            feet = int(parts[0])\n",
        "            inches_str = parts[1].replace('\"', '').strip()\n",
        "            inches = int(inches_str) if inches_str else 0\n",
        "\n",
        "            total_inches = (feet * 12) + inches\n",
        "            cm = total_inches * 2.54\n",
        "            return round(cm, 2)\n",
        "        except ValueError:\n",
        "            # Si hay un error en la conversión, devolver NaN\n",
        "            return np.nan\n",
        "    else:\n",
        "        # Si no tiene el formato esperado, intentar convertir directamente a numérico\n",
        "        try:\n",
        "            # Intenta convertir a número, si ya está en cm o un formato diferente\n",
        "            return float(height_str)\n",
        "        except ValueError:\n",
        "            # Si no se puede convertir, devuelve el valor original o NaN\n",
        "            return np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfFkwSjSzoF5"
      },
      "outputs": [],
      "source": [
        "def convert_lbs_to_kg(lbs_value):\n",
        "    if pd.isna(lbs_value):\n",
        "        return np.nan\n",
        "    try:\n",
        "        # 1 libra = 0.453592 kilogramos\n",
        "        return float(lbs_value) * 0.453592\n",
        "    except ValueError:\n",
        "        return np.nan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QLuF7iNHk1P"
      },
      "outputs": [],
      "source": [
        "def extractData_from_generalSurvey(data_path, data_file, generalsurvey_file,  col_name_id, selected_columns):\n",
        "    # Carga Datos de pacientes extraidos de CargaDatos.ipynb\n",
        "    df_participants = extractData.load_csv_to_dataframe(data_path, data_file)\n",
        "    participant_list = df_participants[\"participant_id\"].unique().tolist()\n",
        "\n",
        "    # Carga los datos del fichero general de pacientes\n",
        "    df_data = extractData.load_csv_to_dataframe(data_path, generalsurvey_file)\n",
        "\n",
        "    # Convertir 'Timestamp' a formato de fecha y hora\n",
        "    df_data['Timestamp'] = pd.to_datetime(df_data['Timestamp'])\n",
        "\n",
        "    # Filtrar datos por participante en la lista\n",
        "    df_data_filtered = df_data[df_data[col_name_id].isin(participant_list)].copy()\n",
        "\n",
        "    # Ordernar los participantes por fechas descendiente\n",
        "    df_data_sorted = df_data_filtered.sort_values(by=[col_name_id, 'Timestamp'], ascending=[True, False])\n",
        "\n",
        "    # Borra duplicados, manteniendo el más reciente de los participantes\n",
        "    df_data_unique_recent = df_data_sorted.drop_duplicates(subset=[col_name_id], keep='first')\n",
        "\n",
        "    df_data = df_data_unique_recent.loc[:, selected_columns]\n",
        "\n",
        "    # Aplicar la función a la columna 'Year of birth'\n",
        "    df_data['Year of birth'] = df_data.apply(calculate_and_group_age, axis=1)\n",
        "\n",
        "    # Clasificación de ancestros por areas según el país de origen\n",
        "    ancestry_country_columns = [\n",
        "        'Maternal grandmother: Country of origin',\n",
        "        'Maternal grandfather: Country of origin',\n",
        "        'Paternal grandmother: Country of origin',\n",
        "        'Paternal grandfather: Country of origin'\n",
        "    ]\n",
        "\n",
        "    for col in ancestry_country_columns:\n",
        "        df_data[col] = df_data[col].apply(classify_ancestry_by_country)\n",
        "\n",
        "    return df_data, participant_list\n",
        "\n",
        "def extractSummary_dataframe(df_data,participant_list, col_name_id,selected_columns):\n",
        "\n",
        "    print(f\"Número de participantes: {len(df_data[col_name_id])}\")\n",
        "\n",
        "    print(\"\\n--- Análisis de valores únicos por columna ---\")\n",
        "    columns_to_analyze = [col for col in selected_columns if col not in ['Participant', 'Timestamp']]\n",
        "\n",
        "    for column in columns_to_analyze:\n",
        "        print(f\"\\nColumna: {column}\")\n",
        "        print(f\"Número de valores únicos: {df_data[column].nunique()}\")\n",
        "        print(\"Conteo de valores:\")\n",
        "        print(df_data[column].value_counts(dropna=False)) # dropna=False to include NaN counts\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    # Identify participants in participant_list but not in df_data\n",
        "    participants_in_df_data = set(df_data[col_name_participant].unique())\n",
        "    participants_not_in_df = [p for p in participant_list if p not in participants_in_df_data]\n",
        "\n",
        "    print(f\"\\n--- Participantes en la lista inicial pero no en el DataFrame final ({len(participants_not_in_df)}) ---\")\n",
        "    if participants_not_in_df:\n",
        "        for p in participants_not_in_df:\n",
        "            print(p)\n",
        "    else:\n",
        "        print(\"Todos los participantes de la lista inicial están presentes en el DataFrame final.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6inkxni5oLjE"
      },
      "outputs": [],
      "source": [
        "selected_columns = ['Participant', 'Timestamp','Year of birth','Severe disease or rare genetic trait',\n",
        "                    'Sex/Gender', 'Race/ethnicity',\n",
        "                    'Maternal grandmother: Country of origin','Maternal grandfather: Country of origin',\n",
        "                    'Paternal grandmother: Country of origin', 'Paternal grandfather: Country of origin']\n",
        "\n",
        "# Crea el conjunto de datos inicial\n",
        "df_data_cases, participant_list = extractData_from_generalSurvey(data_path,participants_hypertension_file,survey_path + participants_generalsurvey_file,\n",
        "                                                                 col_name_participant,selected_columns)\n",
        "\n",
        "# extractSummary_dataframe(df_data_cases,participant_list,col_name_participant,selected_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9T5Eq1fvRPd"
      },
      "outputs": [],
      "source": [
        "# Crea el conjunto de datos inicial de control\n",
        "df_data_control, participant_control_list = extractData_from_generalSurvey(data_path,participants_control_file,survey_path + participants_generalsurvey_file,\n",
        "                                                                            col_name_participant,selected_columns)\n",
        "\n",
        "# extractSummary_dataframe(df_data_control,participant_control_list,col_name_participant,selected_columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "748c8ccf"
      },
      "outputs": [],
      "source": [
        "def balance_control_group(df_cases, df_control, grouped_cases_distribution):\n",
        "    balanced_control_samples = []\n",
        "\n",
        "    # Iterate through each demographic group in the cases distribution\n",
        "    for (year_of_birth, sex_gender), target_count in grouped_cases_distribution.stack().items():\n",
        "        if target_count == 0: # Skip if no cases in this demographic\n",
        "            continue\n",
        "\n",
        "        # Filter the control group for the current demographic\n",
        "        current_demographic_controls = df_control[\n",
        "            (df_control['Year of birth'] == year_of_birth) &\n",
        "            (df_control['Sex/Gender'] == sex_gender)\n",
        "        ]\n",
        "\n",
        "        num_available_controls = len(current_demographic_controls)\n",
        "\n",
        "        if num_available_controls >= target_count:\n",
        "            # If enough controls, sample randomly\n",
        "            sampled_controls = current_demographic_controls.sample(n=target_count, random_state=42)\n",
        "            balanced_control_samples.append(sampled_controls)\n",
        "        elif num_available_controls > 0:\n",
        "            # If not enough, take all available controls for this demographic\n",
        "            balanced_control_samples.append(current_demographic_controls)\n",
        "        # If num_available_controls is 0, do nothing (no controls for this demographic)\n",
        "\n",
        "    # Concatenate all sampled DataFrames into a single DataFrame\n",
        "    if balanced_control_samples:\n",
        "        balanced_df_control = pd.concat(balanced_control_samples, ignore_index=True)\n",
        "    else:\n",
        "        balanced_df_control = pd.DataFrame(columns=df_control.columns)\n",
        "\n",
        "    return balanced_df_control\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a072043"
      },
      "outputs": [],
      "source": [
        "# Grupo de datos de casos\n",
        "grouped_data_cases = df_data_cases.groupby(['Year of birth','Sex/Gender']).size().unstack(fill_value=0)\n",
        "print(grouped_data_cases)\n",
        "\n",
        "grouped_data_control = df_data_control.groupby(['Year of birth','Sex/Gender']).size().unstack(fill_value=0)\n",
        "print(grouped_data_control)\n",
        "\n",
        "# Extrae del dataset de control un conjunto de datos balanceado\n",
        "balanced_df_control = balance_control_group(df_data_cases, df_data_control, grouped_data_cases)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c6ca70d"
      },
      "outputs": [],
      "source": [
        "grouped_balanced_control = balanced_df_control.groupby(['Year of birth','Sex/Gender']).size().unstack(fill_value=0)\n",
        "\n",
        "participant_control_list = balanced_df_control[\"Participant\"].unique().tolist()\n",
        "extractSummary_dataframe(balanced_df_control,participant_control_list,col_name_participant,selected_columns)\n",
        "\n",
        "print(\"Demographic distribution of the balanced control group:\")\n",
        "print(grouped_balanced_control)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuCSzZR4715M"
      },
      "outputs": [],
      "source": [
        "# Extrae la información del resto de datasets\n",
        "\n",
        "# Fenotipos\n",
        "df_data = extractData.load_csv_to_dataframe(data_path, survey_path + participants_basicPhenoSurvey_file)\n",
        "\n",
        "# Seleccionar las columnas y transformar los datos\n",
        "selected_columns = ['Participant', 'Timestamp','1.1 — Blood Type','1.2 — Height', '1.3 — Weight',\n",
        "                    '2.3 — Left Eye Color - Text Description', '2.4 — Right Eye Color - Text Description',\n",
        "                    '3.1 — What is your natural hair color currently, when without artificial color or dye?',\n",
        "                    '1.4 — Handedness']\n",
        "\n",
        "df_data = df_data.loc[:, selected_columns]\n",
        "\n",
        "# Renombrar columnas para hacerlas más sencillas\n",
        "df_data = df_data.rename(columns={\n",
        "    '1.1 — Blood Type': 'Blood_type',\n",
        "    '1.2 — Height': 'Height',\n",
        "    '1.3 — Weight': 'Weight',\n",
        "    '2.3 — Left Eye Color - Text Description': 'Left_eye_color',\n",
        "    '2.4 — Right Eye Color - Text Description': 'Right_eye_color',\n",
        "    '3.1 — What is your natural hair color currently, when without artificial color or dye?': 'Hair_color',\n",
        "    '1.4 — Handedness': 'Handedness'\n",
        "})\n",
        "\n",
        "# Reemplazar same del color derecho por el valor del ojeo izquerdo\n",
        "df_data.loc[df_data['Right_eye_color'].str.lower() == 'same', 'Right_eye_color'] = df_data['Left_eye_color']\n",
        "\n",
        "# Convertir la altura de pies y pulgadas a cm\n",
        "df_data['Height_cm'] = df_data.apply(lambda row: convert_height_to_cm(row['Height']), axis=1)\n",
        "# Convertir el peso en libras a kg\n",
        "df_data['Weight_kg'] = df_data.apply(lambda row: convert_lbs_to_kg(row['Weight']), axis=1)\n",
        "\n",
        "#Calcula el índice de masa corporal\n",
        "df_data['IMC'] = df_data['Weight_kg'] / ((df_data['Height_cm'] / 100)**2)\n",
        "\n",
        "df_data.loc[:,['Height','Height_cm',\"Weight\",\"Weight_kg\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa23ac70"
      },
      "outputs": [],
      "source": [
        "### Unir los datos de fenotipos básicos al conjunto de datos de general y al conjunto de datos de control\n",
        "\n",
        "# Asegurarse de que 'Timestamp' es de tipo datetime en df_data (ya debería estarlo, pero para seguridad)\n",
        "df_data['Timestamp'] = pd.to_datetime(df_data['Timestamp'])\n",
        "\n",
        "# Ordernar df_data por participante y fecha descendente para quedarnos con el más reciente\n",
        "df_data_sorted = df_data.sort_values(by=['Participant', 'Timestamp'], ascending=[True, False])\n",
        "\n",
        "# Borrar duplicados, manteniendo el más reciente de cada participante\n",
        "df_data_unique_recent = df_data_sorted.drop_duplicates(subset=['Participant'], keep='first')\n",
        "\n",
        "# Unir df_data_cases con df_data_unique_recent\n",
        "# Usamos un 'left merge' para asegurarnos de que todos los participantes de df_data_cases se mantengan\n",
        "# y las columnas no coincidentes de df_data se llenen con NaN.\n",
        "df_data_cases = pd.merge(df_data_cases, df_data_unique_recent, on='Participant', how='left', suffixes=('_survey', '_pheno'))\n",
        "\n",
        "# Opcional: Eliminar la columna 'Timestamp_pheno' si no es necesaria para el análisis posterior\n",
        "df_data_cases = df_data_cases.drop(columns=['Timestamp_pheno'])\n",
        "\n",
        "# Unir balanced_df_control con df_data_unique_recent\n",
        "# Usamos un 'left merge' para asegurarnos de que todos los participantes de balanced_df_control se mantengan\n",
        "# y las columnas no coincidentes de balanced_df_control se llenen con NaN.\n",
        "balanced_df_control = pd.merge(balanced_df_control, df_data_unique_recent, on='Participant', how='left', suffixes=('_survey', '_pheno'))\n",
        "\n",
        "\n",
        "# Opcional: Eliminar la columna 'Timestamp_pheno' si no es necesaria para el análisis posterior\n",
        "balanced_df_control = balanced_df_control.drop(columns=['Timestamp_pheno'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xq0XS1o1YVY8"
      },
      "outputs": [],
      "source": [
        "from os.path import join\n",
        "def join_datasets(df_data_cases, balanced_df_control, folder_name, file_name, col_id, col_name_conditions, sep, summary_column):\n",
        "    # Extract Data from Excel\n",
        "    df_data = extractData.load_csv_to_dataframe(folder_name, file_name)\n",
        "\n",
        "    # Extraer todas las condiciones del fichero\n",
        "    all_cv_conditions = extractData.extract_conditions(df_data,col_name_conditions,sep)\n",
        "\n",
        "    print (f\" Hay {len(all_cv_conditions)} para {summary_column}\")\n",
        "    print(all_cv_conditions)\n",
        "    n = len(all_cv_conditions)\n",
        "\n",
        "    # Crear dataset de participantes\n",
        "    df_data = extractData.create_conditions_df(df_data, all_cv_conditions,col_id,col_name_conditions,sep)\n",
        "\n",
        "    # Añadir una columna que sume todas las condiciones\n",
        "    has_conditions = df_data[all_cv_conditions].sum(axis=1)\n",
        "\n",
        "    # ¿Debería no considerar hemorroides?\n",
        "    # Se elimina la hipertensión\n",
        "    if (summary_column == 'Circulatory_conditions'):\n",
        "      has_conditions = df_data[all_cv_conditions].sum(axis=1) - df_data['Hypertension']\n",
        "\n",
        "    df_data[summary_column] = has_conditions\n",
        "\n",
        "    # Añade estas columnas al conjunto de datos de casos y de control\n",
        "    df_cases = pd.merge(df_data_cases, df_data, on='Participant', how='left', suffixes=('_cases', '_condition'))\n",
        "    df_control = pd.merge(balanced_df_control, df_data, on='Participant', how='left', suffixes=('_control', '_condition'))\n",
        "\n",
        "    return df_cases, df_control, n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL5V2aq4EdUt"
      },
      "outputs": [],
      "source": [
        "def summarize_conditions(df_data):\n",
        "    condition_columns = [col for col in df_data_cases.columns if col.endswith('_conditions')]\n",
        "    columns = ['condition_type', 'has_condition', 'no_condition']\n",
        "    conditions_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "    for col in condition_columns:\n",
        "        has_condition = (df_data[col] > 0).sum() # Count where condition is present (value > 0)\n",
        "        no_condition = (df_data[col] == 0).sum() # Count where condition is not present (value == 0)\n",
        "        # Also account for NaN values as no condition\n",
        "        no_condition += df_data[col].isna().sum()\n",
        "\n",
        "        new_row =pd.DataFrame([{\"condition_type\": col,\"has_condition\": has_condition,\"no_condition\":no_condition}])\n",
        "        conditions_df = pd.concat([conditions_df, new_row], ignore_index=True)\n",
        "\n",
        "    return conditions_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCTFB7UH5Aln"
      },
      "outputs": [],
      "source": [
        "# TODO. Ejecutar las filas anteriores o inicializar los dataframes\n",
        "columns = ['condition_type', 'N']\n",
        "conditions_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "col_name_conditions_one = 'Have you ever been diagnosed with one of the following conditions?'\n",
        "col_name_conditions_any = 'Have you ever been diagnosed with any of the following conditions?'\n",
        "\n",
        "# Unir los datos relacionados con enfermedades cardiovasculares\n",
        "(df_data_cases, balanced_df_control,n_conditions) = join_datasets(df_data_cases, balanced_df_control, data_path + survey_path, participants_circulatory_file,\n",
        "                                                     col_name_participant, col_name_conditions_one, sep, 'Circulatory_conditions')\n",
        "new_row =pd.DataFrame([{\"condition_type\": \"Sistema circulatorio\",\"N\": n_conditions}])\n",
        "conditions_df = pd.concat([conditions_df, new_row], ignore_index=True)\n",
        "\n",
        "# Unir los datos relacionados con cancer\n",
        "(df_data_cases, balanced_df_control, n_conditions) = join_datasets(df_data_cases, balanced_df_control, data_path + survey_path, participants_cancer_file,\n",
        "                                                     col_name_participant, col_name_conditions_one, sep, 'cancer_conditions')\n",
        "new_row =pd.DataFrame([{\"condition_type\": \"Cáncer\",\"N\": n_conditions}])\n",
        "conditions_df = pd.concat([conditions_df, new_row], ignore_index=True)\n",
        "\n",
        "# Unir los datos relacionados con el sistema endocrino\n",
        "(df_data_cases, balanced_df_control,n_conditions) = join_datasets(df_data_cases, balanced_df_control, data_path + survey_path, participants_endocrine_file,\n",
        "                                                     col_name_participant, col_name_conditions_any, sep, 'endocrine_conditions')\n",
        "new_row =pd.DataFrame([{\"condition_type\": \"Sistema endocrino\",\"N\": n_conditions}])\n",
        "conditions_df = pd.concat([conditions_df, new_row], ignore_index=True)\n",
        "\n",
        "# Unir los datos relacionados con enfermedades de la sangre\n",
        "(df_data_cases, balanced_df_control,n_conditions) = join_datasets(df_data_cases, balanced_df_control, data_path + survey_path, participants_blood_file,\n",
        "                                                     col_name_participant, col_name_conditions_any, sep, 'blood_conditions')\n",
        "new_row =pd.DataFrame([{\"condition_type\": \"Sistema sanguineo\",\"N\": n_conditions}])\n",
        "conditions_df = pd.concat([conditions_df, new_row], ignore_index=True)\n",
        "\n",
        "# Unir los datos relacionados con el sistema nervioso\n",
        "(df_data_cases, balanced_df_control,n_conditions) = join_datasets(df_data_cases, balanced_df_control, data_path + survey_path, participants_nervous_file,\n",
        "                                                     col_name_participant, col_name_conditions_one, sep, 'nervous_conditions')\n",
        "new_row =pd.DataFrame([{\"condition_type\": \"Sistema nervioso\",\"N\": n_conditions}])\n",
        "conditions_df = pd.concat([conditions_df, new_row], ignore_index=True)\n",
        "\n",
        "# Unir los datos relacionados con los sentidos de la vista y el oido\n",
        "(df_data_cases, balanced_df_control,n_conditions) = join_datasets(df_data_cases, balanced_df_control, data_path + survey_path, participants_senses_file,\n",
        "                                                     col_name_participant, col_name_conditions_one, sep, 'senses_conditions')\n",
        "new_row =pd.DataFrame([{\"condition_type\": \"Sistema Visual y auditivo\",\"N\": n_conditions}])\n",
        "conditions_df = pd.concat([conditions_df, new_row], ignore_index=True)\n",
        "\n",
        "# Unir los datos relacionados con el sistema respiratorio\n",
        "(df_data_cases, balanced_df_control,n_conditions) = join_datasets(df_data_cases, balanced_df_control, data_path + survey_path, participants_respiratory_file,\n",
        "                                                     col_name_participant, col_name_conditions_any, sep, 'respiratory_conditions')\n",
        "new_row =pd.DataFrame([{\"condition_type\": \"Sistema respiratorio\",\"N\": n_conditions}])\n",
        "conditions_df = pd.concat([conditions_df, new_row], ignore_index=True)\n",
        "\n",
        "# Unir los datos relacionados con el sistema digestivo\n",
        "(df_data_cases, balanced_df_control, n_conditions) = join_datasets(df_data_cases, balanced_df_control, data_path + survey_path, participants_digestive_file,\n",
        "                                                     col_name_participant, col_name_conditions_any, sep, 'digestive_conditions')\n",
        "new_row =pd.DataFrame([{\"condition_type\": \"Sistema digestivo\",\"N\": n_conditions}])\n",
        "conditions_df = pd.concat([conditions_df, new_row], ignore_index=True)\n",
        "\n",
        "# Unir los datos relacionados con el sistema reproductor\n",
        "(df_data_cases, balanced_df_control,n_conditions) = join_datasets(df_data_cases, balanced_df_control, data_path + survey_path, participants_genitourinary_file,\n",
        "                                                     col_name_participant, col_name_conditions_any, sep, 'genitourinary_conditions')\n",
        "new_row =pd.DataFrame([{\"condition_type\": \"Sistema genitourinario\",\"N\": n_conditions}])\n",
        "conditions_df = pd.concat([conditions_df, new_row], ignore_index=True)\n",
        "\n",
        "# Unir los datos relacionados con enferemedades de la piel\n",
        "(df_data_cases, balanced_df_control,n_conditions) = join_datasets(df_data_cases, balanced_df_control, data_path + survey_path, participants_skin_file,\n",
        "                                                     col_name_participant, col_name_conditions_any, sep, 'skin_conditions')\n",
        "new_row =pd.DataFrame([{\"condition_type\": \"Piel\",\"N\": n_conditions}])\n",
        "conditions_df = pd.concat([conditions_df, new_row], ignore_index=True)\n",
        "\n",
        "# Unir los datos relacionados con enferemedades musculo-esqueléticas\n",
        "# Corrected col_name_conditions for participants_muscle_file\n",
        "(df_data_cases, balanced_df_control,n_conditions) = join_datasets(df_data_cases, balanced_df_control, data_path + survey_path, participants_muscle_file,\n",
        "                                                     col_name_participant, col_name_conditions_any, sep, 'muscular_conditions')\n",
        "new_row =pd.DataFrame([{\"condition_type\": \"Sistema musculo esquelético\",\"N\": n_conditions}])\n",
        "conditions_df = pd.concat([conditions_df, new_row], ignore_index=True)\n",
        "\n",
        "# Unir los datos relacionados con rasgos congénitos\n",
        "# Assuming 'one of the following conditions' for congenital traits\n",
        "(df_data_cases, balanced_df_control,n_conditions) = join_datasets(df_data_cases, balanced_df_control, data_path + survey_path, participants_congenital_file,\n",
        "                                                     col_name_participant, col_name_conditions_any, sep, 'congenital_conditions')\n",
        "new_row =pd.DataFrame([{\"condition_type\": \"Rasgos congénitos\",\"N\": n_conditions}])\n",
        "conditions_df = pd.concat([conditions_df, new_row], ignore_index=True)\n",
        "\n",
        "\n",
        "print(f\"Number of rows in df_data_cases: {df_data_cases.shape[0]}\")\n",
        "print(f\"Number of columns in df_data_cases: {df_data_cases.shape[1]}\")\n",
        "\n",
        "print(f\"Number of rows in balanced_df_control: {balanced_df_control.shape[0]}\")\n",
        "print(f\"Number of columns in balanced_df_control: {balanced_df_control.shape[1]}\")\n",
        "\n",
        "print(conditions_df)\n",
        "\n",
        "df_sum_condition_cases = summarize_conditions(df_data_cases)\n",
        "df_sum_condition_control = summarize_conditions(balanced_df_control)\n",
        "\n",
        "print(df_sum_condition_cases)\n",
        "print(df_sum_condition_control)\n",
        "\n",
        "\n",
        "df_participants = pd.concat([df_data_cases, balanced_df_control], axis=0)\n",
        "\n",
        "df_participants.to_csv(data_path +'selected_participants.csv', sep=',', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NJqGP_iTBv1v"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "def search_original_file(participant_list, metadata_df, max_file_size, download):\n",
        "  data_records = []\n",
        "  ## Recorre los participantes de la lista\n",
        "  for participant_id in participant_list:\n",
        "      if download:\n",
        "            participant_dir = os.path.join(data_path + participant_data_path, participant_id)\n",
        "            os.makedirs(participant_dir, exist_ok=True)\n",
        "            print(f\"Ensured directory exists: {participant_dir}\")\n",
        "\n",
        "      print(f\"##### Participant: {participant_id} #####\")\n",
        "\n",
        "      #Busca en el dataset de metadatos los registros que corresponden al participante\n",
        "      metadata_df_participant = metadata_df[metadata_df['participant_id'] == participant_id]\n",
        "\n",
        "      #Recorre cada uno de los registros encontrados y busca la URL original (siempre y cuando el tamaño sea menor de 11 GB)\n",
        "      for index,record in metadata_df_participant.iterrows():\n",
        "          # Comprueba el tamaño del fichero\n",
        "          print(f\"Tamaño del fichero {record['file_size']}\")\n",
        "          if record['file_size_unit']=='GB' and record['file_size_value'] > max_file_size:\n",
        "              record['original_url'] = \"Sin verificar - Tamaño excesivo\"\n",
        "              record['filename'] = ''\n",
        "              data_records.append(record)\n",
        "              continue\n",
        "\n",
        "          print(f\"Download url: {record['download_url']}\")\n",
        "          if pd.isna(record['download_url']):\n",
        "              record['original_url'] = \"Sin archivo para descargar\"\n",
        "              record['filename'] = ''\n",
        "              data_records.append(record)\n",
        "          else:\n",
        "              try:\n",
        "                  final_url, filename, soup = extractData.get_html_parser(record['download_url'])\n",
        "                  print(f\"Final url: {final_url}\")\n",
        "                  print(f\"Filename: {filename}\")\n",
        "\n",
        "                  if not soup:\n",
        "                      download_file = []\n",
        "                  else:\n",
        "                      download_file = extractData.get_list_genetic_data(soup, final_url)\n",
        "\n",
        "                  if len(download_file) == 0:\n",
        "                      record['original_url'] = final_url\n",
        "                      record['filename'] = filename\n",
        "                      data_records.append(record)\n",
        "                      ext = filename.split('.')\n",
        "                      if download and ext[-1].strip().upper() != 'BAM':\n",
        "                          print(f\"Descargando... {final_url}\")\n",
        "                          extractData.get_download_file(final_url,participant_dir,filename)\n",
        "                  else:\n",
        "                      for url_file in download_file:\n",
        "                          print(f\"Descargando... {url_file}\")\n",
        "                          record['original_url'] = url_file\n",
        "                          record['filename'] = ''\n",
        "                          data_records.append(record)\n",
        "                          if download_file and ext[-1].strip().upper() != 'BAM':\n",
        "                            print(f\"Descargando... {url_file}\")\n",
        "                            extractData.get_download_file(url_file,participant_dir,None)\n",
        "\n",
        "\n",
        "              except requests.exceptions.HTTPError as errh:\n",
        "                  print(f\"HTTP Error for {record['participant_id']}: {errh}\")\n",
        "              except requests.exceptions.ConnectionError as errc:\n",
        "                  print(f\"Error Connecting for {record['participant_id']}: {errc}\")\n",
        "              except requests.exceptions.Timeout as errt:\n",
        "                  print(f\"Timeout Error for {record['participant_id']}: {errt}\")\n",
        "              except requests.exceptions.RequestException as err:\n",
        "                  print(f\"An unexpected error occurred for {record['participant_id']}: {err}\")\n",
        "              except Exception as e:\n",
        "                  print(f\"An unexpected error occurred searching URL: {e}\")\n",
        "\n",
        "  return pd.DataFrame(data_records)\n",
        "\n",
        "# Inicialización de variables\n",
        "max_file_size = 8\n",
        "participant_list = df_participants['Participant'].to_list()\n",
        "# Carga el fichero de metadatos generado en CargaDatos\n",
        "metadata_df = extractData.load_csv_to_dataframe(data_path, metadata_file)\n",
        "\n",
        "# ['hu034DB1', 'hu05FD49', 'hu0878AF', 'hu094BE5', 'hu0D1FA1']\n",
        "#participant_list = ['hu05FD49']\n",
        "selected_participants_metadata_df = search_original_file(participant_list, metadata_df, max_file_size,False)\n",
        "\n",
        "print(selected_participants_metadata_df.shape)\n",
        "\n",
        "selected_participants_metadata_df.to_csv(data_path +'metadatos_geneticos_participantes.csv', sep=',', index=False)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}