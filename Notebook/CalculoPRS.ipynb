{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FyHUPeXI_q7"
      },
      "outputs": [],
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Cambiar el directorio de trabajo\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Define the TFM directory path\n",
        "TFM_PATH = '/content/drive/My Drive/TFM'\n",
        "\n",
        "# Change the current working directory to the TFM directory\n",
        "os.chdir(TFM_PATH)\n",
        "print(f\"Current working directory changed to: {os.getcwd()}\")\n",
        "\n",
        "# Add the TFM directory to the Python system path\n",
        "if TFM_PATH not in sys.path:\n",
        "    sys.path.append(TFM_PATH)\n",
        "    print(f\"'{TFM_PATH}' added to Python system path.\")\n",
        "else:\n",
        "    print(f\"'{TFM_PATH}' is already in Python system path.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from scipy.stats import shapiro, normaltest, levene, fligner, bartlett, skew, kurtosis\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.covariance import MinCovDet\n",
        "\n",
        "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
        "from scipy.spatial.distance import squareform\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import extractData"
      ],
      "metadata": {
        "id": "IJjGKVA0qAxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = TFM_PATH + '/data/'\n",
        "participant_data_path = data_path + 'participant_genetic_data/'\n",
        "modelos = data_path + 'Modelos/'\n",
        "std_files = data_path + 'std_files/'\n",
        "\n",
        "download_files = \"download_files.csv\""
      ],
      "metadata": {
        "id": "AoP2sFKOqGFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extraer_rsids_desde_xref(xref):\n",
        "    \"\"\"\n",
        "    xRef suele venir como:\n",
        "        'dbsnp.83:rs526642' o\n",
        "        'dbsnp.100:rs2748067;dbsnp.131:rs76046194'\n",
        "\n",
        "    Devolvemos un conjunto de rsID encontrados.\n",
        "    \"\"\"\n",
        "    rsids = set()\n",
        "    if not xref:\n",
        "        return rsids\n",
        "\n",
        "    # Separar por ';' y luego por ':' y quedarnos con las cadenas que empiezan por 'rs'\n",
        "    for chunk in xref.split(\";\"):\n",
        "        for piece in chunk.split(\":\"):\n",
        "            piece = piece.strip()\n",
        "            if piece.startswith(\"rs\"):\n",
        "                rsids.add(piece)\n",
        "    return rsids\n"
      ],
      "metadata": {
        "id": "tA7bHslL2t7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40aed897"
      },
      "source": [
        "def estandarizar_TSV_files(var_path, out_tsv_path,save_file = False):\n",
        "    \"\"\"\n",
        "    Recorre el fichero de variantes de Complete Genomics y genera\n",
        "    un TSV estandarizado con:\n",
        "\n",
        "        rsID, chr, pos, effect_allele, other_allele,\n",
        "        effect_weight, allele1, allele2, dosage\n",
        "    \"\"\"\n",
        "    # rsID -> lista de alelos observados ['A','G'] (máx. ploidía)\n",
        "    genotipos_data = [] # Usaremos una lista para acumular los datos\n",
        "\n",
        "    with extractData.open_compressed_file(var_path, \"rt\") as f:\n",
        "        # Saltar hasta la cabecera que empieza con '>'\n",
        "        for line in f:\n",
        "            if line.startswith(\">\"):\n",
        "                break\n",
        "\n",
        "        # A partir de aquí vienen las filas de variantes\n",
        "        for line in f:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            if not line or line.startswith(\"#\"):\n",
        "                continue\n",
        "\n",
        "\n",
        "            parts = line.split(\"\\t\")\n",
        "            # El fichero debe tener 16 columnas según tu descripción\n",
        "            #if len(parts) < 16:\n",
        "            #    continue\n",
        "\n",
        "            # Campos según el orden que comentabas\n",
        "            # 0:locus, 1:ploidy, 2:allele, 3:chromosome, 4:begin, 5:end,\n",
        "            # 6:varType, 7:reference, 8:alleleSeq, ..., 13:xRef, 14:alleleFreq, 15:alternativeCalls\n",
        "            var_type = parts[6]\n",
        "            allele_pos = parts[2]\n",
        "            chrom = parts[3]\n",
        "            pos = parts[5]   # cojo el end en lugoar de begin\n",
        "            allele_ref = parts[7]\n",
        "            allele_seq = parts[8]\n",
        "            var_filter = parts[11]\n",
        "            xref = parts[13]\n",
        "\n",
        "            # Solo SNPs cuyo calidad sea buena (varFilter == PASS), ignoramos otras variantes\n",
        "            if var_type.lower() != \"snp\":\n",
        "                continue\n",
        "            else:\n",
        "                if (var_filter.lower() != \"pass\" and var_filter.lower()!=\"vqhigh\"):\n",
        "                    continue\n",
        "            # Alelo desconocido\n",
        "            if allele_seq in (\"\", \".\", \"?\", \"N\"):\n",
        "                continue\n",
        "\n",
        "            rsids = [\n",
        "                r for r in extraer_rsids_desde_xref(xref)\n",
        "                #if r in pgs_dict\n",
        "            ]\n",
        "\n",
        "            if not rsids:\n",
        "                # Ningún rsID de esta fila está en el modelo\n",
        "                continue\n",
        "\n",
        "            # Si hay más de un rsID, nos quedamos con el primero que está en el modelo\n",
        "            rsid = rsids[0]\n",
        "\n",
        "            # En función de si es el primer alelo o el segundo lo pone en allele1 o allele2\n",
        "            if allele_pos == \"1\":\n",
        "                a1_ref = allele_ref\n",
        "                a1 = allele_seq\n",
        "                a2 = a2_ref = '.'\n",
        "            else:\n",
        "                a2_ref = allele_ref\n",
        "                a2 = allele_seq\n",
        "                a1 = a1_ref = '.'\n",
        "\n",
        "\n",
        "            genotipos_data.append({\n",
        "                \"rsID\": rsid,\n",
        "                \"chr\": chrom,\n",
        "                \"pos\": pos,\n",
        "                \"allele1_ref\": a1_ref,\n",
        "                \"allele2_ref\": a2_ref,\n",
        "                \"allele1\": a1,\n",
        "                \"allele2\": a2,\n",
        "                \"dosage\":1\n",
        "            })\n",
        "\n",
        "    # Crear el DataFrame de pandas con los datos acumulados\n",
        "    df_genotipos = pd.DataFrame(genotipos_data)\n",
        "\n",
        "    # Escribir el DataFrame estandarizado a un fichero TSV\n",
        "    if (save_file):\n",
        "        out_path = Path(out_tsv_path)\n",
        "        df_genotipos.to_csv(out_tsv_path, sep=\"\\t\", index=False)\n",
        "\n",
        "    return df_genotipos, out_tsv_path\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc9BHKkHq1v7"
      },
      "source": [
        "def estandarizar_TSV3_files(var_path, out_tsv_path,save_file = False):\n",
        "    \"\"\"\n",
        "    Recorre el fichero de variantes de Complete Genomics y genera\n",
        "    un TSV estandarizado con:\n",
        "\n",
        "        rsID, chr, pos, effect_allele, other_allele,\n",
        "        effect_weight, allele1, allele2, dosage\n",
        "    \"\"\"\n",
        "    # rsID -> lista de alelos observados ['A','G'] (máx. ploidía)\n",
        "    genotipos_data = [] # Usaremos una lista para acumular los datos\n",
        "\n",
        "    with extractData.open_compressed_file(var_path, \"rt\") as f:\n",
        "        # Saltar hasta la cabecera que empieza con '>'\n",
        "        for line in f:\n",
        "            if line.startswith(\">\"):\n",
        "                break\n",
        "\n",
        "        # A partir de aquí vienen las filas de variantes\n",
        "        for line in f:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            if not line or line.startswith(\"#\"):\n",
        "                continue\n",
        "\n",
        "\n",
        "            parts = line.split(\"\\t\")\n",
        "            # El fichero debe tener 16 columnas según tu descripción\n",
        "            #if len(parts) < 16:\n",
        "            #    continue\n",
        "\n",
        "            # Campos según el orden que comentabas\n",
        "            # 0:locus, 1:ploidy, 2:chromosome, 3:begin, 4:end, 5:zigosity\n",
        "            # 6:varType, 7:reference, 8:allele1Seq, 9:allele2Seq, ..., 14:allele1VarQuality,\n",
        "            # 15:allele2VarQuality, 15:alternativeCalls,..., 18:allele1XRef, 19:allele2XRef, ...\n",
        "            var_type = parts[6]\n",
        "            chrom = parts[2]\n",
        "            pos = parts[4]   # cojo el end en lugoar de begin\n",
        "            allele_ref = parts[7]\n",
        "            allele1_seq = parts[8]\n",
        "            allele2_seq = parts[9]\n",
        "            var1_filter = parts[14]\n",
        "            var2_filter = parts[15]\n",
        "            xref1 = parts[18]\n",
        "            xref2 = parts[19]\n",
        "\n",
        "            # Solo SNPs cuyo calidad sea buena (varFilter == PASS), ignoramos otras variantes\n",
        "            if var_type.lower() != \"snp\":\n",
        "                continue\n",
        "            else:\n",
        "                # Allele 1\n",
        "                if var1_filter.lower() == \"vqhigh\":\n",
        "                    if allele1_seq not in (\"\", \".\", \"?\", \"N\"):\n",
        "                        if (allele1_seq != allele_ref):\n",
        "                            rsids = [r for r in extraer_rsids_desde_xref(xref1)]\n",
        "                            if rsids:\n",
        "                                rsid = rsids[0]\n",
        "                                a1_ref = allele_ref\n",
        "                                a1 = allele1_seq\n",
        "                                a2 = a2_ref = '.'\n",
        "                                genotipos_data.append({\n",
        "                                    \"rsID\": rsid,\n",
        "                                    \"chr\": chrom,\n",
        "                                    \"pos\": pos,\n",
        "                                    \"allele1_ref\": a1_ref,\n",
        "                                    \"allele2_ref\": a2_ref,\n",
        "                                    \"allele1\": a1,\n",
        "                                    \"allele2\": a2,\n",
        "                                    \"dosage\":1\n",
        "                                })\n",
        "\n",
        "                if var2_filter.lower() == \"vqhigh\":\n",
        "                    if allele2_seq not in (\"\", \".\", \"?\", \"N\"):\n",
        "                        if (allele2_seq != allele_ref):\n",
        "                            rsids = [r for r in extraer_rsids_desde_xref(xref2)]\n",
        "                            if rsids:\n",
        "                                rsid = rsids[0]\n",
        "                                a2_ref = allele_ref\n",
        "                                a2 = allele2_seq\n",
        "                                a1 = a1_ref = '.'\n",
        "                                genotipos_data.append({\n",
        "                                    \"rsID\": rsid,\n",
        "                                    \"chr\": chrom,\n",
        "                                    \"pos\": pos,\n",
        "                                    \"allele1_ref\": a1_ref,\n",
        "                                    \"allele2_ref\": a2_ref,\n",
        "                                    \"allele1\": a1,\n",
        "                                    \"allele2\": a2,\n",
        "                                    \"dosage\":1\n",
        "                                })\n",
        "\n",
        "    # Crear el DataFrame de pandas con los datos acumulados\n",
        "    df_genotipos = pd.DataFrame(genotipos_data)\n",
        "\n",
        "    # Escribir el DataFrame estandarizado a un fichero TSV\n",
        "    if (save_file):\n",
        "        out_path = Path(out_tsv_path)\n",
        "        df_genotipos.to_csv(out_tsv_path, sep=\"\\t\", index=False)\n",
        "\n",
        "    return df_genotipos, out_tsv_path\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ac_value(info_string):\n",
        "    \"\"\"\n",
        "    Busca 'AC' en una cadena de información y devuelve su valor. Si no se encuentra 'AC', devuelve 1.\n",
        "    Ejemplo de cadena: 'AB=0.294118;ABP=15.5282;AC=2;ADP=41'\n",
        "    \"\"\"\n",
        "    for item in info_string.split(';'):\n",
        "        if item.startswith('AC='):\n",
        "            try:\n",
        "                # Extrae el valor después de 'AC='\n",
        "                return int(item.split('=')[1])\n",
        "            except ValueError:\n",
        "                # En caso de que el valor de AC no sea un entero válido\n",
        "                return 1 # O manejar el error de otra manera\n",
        "    return 1\n",
        "\n"
      ],
      "metadata": {
        "id": "ch82jh1vzeJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dosage_value(info_string):\n",
        "    \"\"\"\n",
        "    Busca 'HET'; 'HOM' en una cadena de información y devuelve su valor. Si no se encuentra ninguna cadena, devuelve 0.\n",
        "    Ejemplo de cadena: 'AB=0.294118;ABP=15.5282;AC=2;ADP=41'\n",
        "    \"\"\"\n",
        "    dosage = 0\n",
        "    het = 0\n",
        "    hom = 0\n",
        "    for item in info_string.split(';'):\n",
        "        if item.startswith('HET='):\n",
        "            try:\n",
        "                het =  int(item.split('=')[1])\n",
        "            except ValueError:\n",
        "                het = 0\n",
        "        if item.startswith('HOM='):\n",
        "            try:\n",
        "                hom =  int(item.split('=')[1])\n",
        "            except ValueError:\n",
        "                hom = 0\n",
        "    if het == 1 and hom == 0:\n",
        "        dosage = 1\n",
        "    elif het == 0 and hom == 1:\n",
        "        dosage = 2\n",
        "    else:\n",
        "        dosage = 0\n",
        "\n",
        "    return dosage\n"
      ],
      "metadata": {
        "id": "O6UGHK8c8NnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_genotype(gt: str, ref: str, alt: str, info:str):\n",
        "    \"\"\"\n",
        "    Convierte un campo GT (ej. '0/1', '1|1', './.') a (allele1, allele2)\n",
        "    usando REF y ALT.\n",
        "\n",
        "    Devuelve (allele1, allele2) donde cada uno es una base o '.' si missing.\n",
        "    \"\"\"\n",
        "\n",
        "    if gt in (\".\", \"./.\", \".|.\"):\n",
        "        return \".\", \".\",\".\",\".\"\n",
        "\n",
        "    # ALT puede tener múltiples alelos: 'A,G'\n",
        "    alt_alleles = alt.split(\",\") if alt not in (\".\", \"\") else []\n",
        "\n",
        "    # Separar por / o | (no nos importa la fase para PRS)\n",
        "    tokens = re.split(r\"[\\/|]\", gt)\n",
        "    alleles = []\n",
        "    alleles_ref =[]\n",
        "\n",
        "    t = tokens[0]\n",
        "    if t in (\"\", \".\"):\n",
        "        alleles.append(\".\")\n",
        "        alleles.append(\".\")\n",
        "        alleles_ref.append(\".\")\n",
        "        alleles_ref.append(\".\")\n",
        "\n",
        "    try:\n",
        "      idx = int(t)\n",
        "    except ValueError:\n",
        "        alleles.append(\".\")\n",
        "        alleles.append(\".\")\n",
        "        alleles_ref.append(\".\")\n",
        "        alleles_ref.append(\".\")\n",
        "\n",
        "    if idx == 0:\n",
        "        alleles_ref.append(ref)\n",
        "        alleles_ref.append(\".\")\n",
        "        alleles.append(alt_alleles[idx])\n",
        "        alleles.append(\".\")\n",
        "    elif idx == 1:\n",
        "        alleles_ref.append(\".\")\n",
        "        alleles_ref.append(ref)\n",
        "        alleles.append(\".\")\n",
        "        alleles.append(alt_alleles[idx-1])\n",
        "    else:\n",
        "      alleles.append(\".\")\n",
        "      alleles.append(\".\")\n",
        "      alleles_ref.append(\".\")\n",
        "      alleles_ref.append(\".\")\n",
        "\n",
        "    count = get_ac_value(info)\n",
        "\n",
        "    return alleles_ref[0], alleles_ref[1],alleles[0], alleles[1],count\n"
      ],
      "metadata": {
        "id": "cWAkWHcvmFDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estandarizar_VCF_files(var_path:str, out_vcf_path:str,\n",
        "    add_chr_prefix: bool = False,save_file:bool=False,):\n",
        "    \"\"\"\n",
        "    Convierte un VCF a un TSV con formato:\n",
        "        rsID    chr    pos    allele1    allele2\n",
        "\n",
        "    - vcf_path: ruta del .vcf o .vcf.gz\n",
        "    - out_path: ruta del .tsv de salida\n",
        "    - sample_name: nombre de la muestra (columna VCF).\n",
        "        - Si es None, se usa la primera muestra que aparezca tras FORMAT.\n",
        "    - add_chr_prefix: si True, convierte '1' -> 'chr1', etc.\n",
        "    \"\"\"\n",
        "\n",
        "    # rsID -> lista de alelos observados ['A','G'] (máx. ploidía)\n",
        "    genotipos_data = [] # Usaremos una lista para acumular los datos\n",
        "\n",
        "    with extractData.open_compressed_file(var_path, \"rt\") as f:\n",
        "        header_seen = False\n",
        "        sample_idx = None\n",
        "\n",
        "        # Recorre todas las líneas del fichero\n",
        "        for line in f:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "\n",
        "            # Ignora los metadatos del VCF\n",
        "            if line.startswith(\"##\"):\n",
        "                continue\n",
        "\n",
        "            # Cabecera de las columnas\n",
        "            if line.startswith(\"#CHROM\"):\n",
        "                header_seen = True\n",
        "                cols = line.lstrip(\"#\").split(\"\\t\")\n",
        "\n",
        "            # Las primeras 9 columnas son fijas en VCF\n",
        "            # CHROM POS ID REF ALT QUAL FILTER INFO FORMAT [SAMPLES...]\n",
        "            # Analizando datos cabecera\n",
        "            if len(cols) < 10:\n",
        "                print(\n",
        "                    \"La cabecera del VCF no tiene columnas de muestras.\",\n",
        "                    file=sys.stderr,\n",
        "                )\n",
        "                break;\n",
        "\n",
        "            format_idx = 8  # índice de FORMAT\n",
        "            sample_idx = 9\n",
        "\n",
        "            # Si aún no hemos visto la cabecera, algo va mal\n",
        "            if not header_seen:\n",
        "                continue\n",
        "\n",
        "            # Procesar variante\n",
        "            cols = line.split(\"\\t\")\n",
        "            if len(cols) < sample_idx + 1:\n",
        "                # No hay columna para la muestra elegida\n",
        "                print(f\"No hay columna de muestra: {line}\")\n",
        "                continue\n",
        "\n",
        "            chrom = cols[0]\n",
        "            pos = cols[1]\n",
        "            rsid = cols[2]\n",
        "            ref = cols[3]\n",
        "            alt = cols[4]\n",
        "            filter = cols[6]\n",
        "            info = cols[7]\n",
        "            fmt = cols[8]\n",
        "            fix_col = cols[sample_idx].strip()\n",
        "            try:\n",
        "              sample_field = cols[sample_idx +1].strip()\n",
        "            except IndexError:\n",
        "              sample_field = fix_col\n",
        "\n",
        "            # Si no hay ID, podemos dejar '.' o construir algo como CHR:POS\n",
        "            if rsid == \"\":\n",
        "                # Aquí dejamos '.', pero podrías hacer:\n",
        "                rsid = \".\"\n",
        "            elif rsid.startswith(\"rs\"):\n",
        "                rsid = rsid\n",
        "            else:\n",
        "                rsid = \".\"\n",
        "\n",
        "            # Cuando la longitud del alelo de referencia es mayor que 1, pasa a la siguiente linea\n",
        "            if len(ref) > 1:\n",
        "              continue\n",
        "\n",
        "            # Comprueba la calidad. Si no es PASS pasa a la siguiente linea\n",
        "            if filter.lower() != \"pass\":\n",
        "                continue\n",
        "\n",
        "            dosage = get_dosage_value(info)\n",
        "            if (dosage > 0):\n",
        "                a1_ref, a2_ref, a1, a2, dosage = ref, \".\",alt,\".\",dosage\n",
        "            else:\n",
        "                # Buscar el índice de GT en FORMAT\n",
        "                fmt_keys = fmt.split(\":\")\n",
        "                if \"GT\" not in fmt_keys:\n",
        "                    # Sin genotipo no podemos construir alelos\n",
        "                    allele1, allele2 = \".\", \".\"\n",
        "                else:\n",
        "                    print(line)\n",
        "                    print(fix_col)\n",
        "                    print(sample_field)\n",
        "                    gt_index = fmt_keys.index(\"GT\")\n",
        "                    if (sample_field != \".\" and sample_field != \"\"):\n",
        "                        sample_values = sample_field.split(\":\")\n",
        "                        if gt_index >= len(sample_values):\n",
        "                            a1_ref, a2_ref, a1, a2, dosage = \".\", \".\",\".\",\".\",0\n",
        "                        else:\n",
        "                            gt = sample_values[gt_index]\n",
        "                            a1_ref, a2_ref, a1, a2, dosage = parse_genotype(gt, ref, alt, info)\n",
        "\n",
        "            # cromosoma de salida\n",
        "            if add_chr_prefix and not chrom.startswith(\"chr\"):\n",
        "                chr_out = f\"chr{chrom}\"\n",
        "            else:\n",
        "                chr_out = chrom\n",
        "\n",
        "            genotipos_data.append({\n",
        "                \"rsID\": rsid,\n",
        "                \"chr\": chrom,\n",
        "                \"pos\": pos,\n",
        "                \"allele1_ref\": a1_ref,\n",
        "                \"allele2_ref\": a2_ref,\n",
        "                \"allele1\": a1,\n",
        "                \"allele2\": a2,\n",
        "                \"dosage\":dosage\n",
        "            })\n",
        "\n",
        "    # Crear el DataFrame de pandas con los datos acumulados\n",
        "    df_genotipos = pd.DataFrame(genotipos_data)\n",
        "\n",
        "    # Escribir el DataFrame estandarizado a un fichero TSV\n",
        "    if (save_file):\n",
        "        out_path = Path(out_vcf_path)\n",
        "        df_genotipos.to_csv(out_vcf_path, sep=\"\\t\", index=False)\n",
        "\n",
        "    return df_genotipos, out_vcf_path\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z6lO6TeZYZ9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def leer_modelo_pgs(pgs_path, add_chr_prefix):\n",
        "    \"\"\"\n",
        "    Lee el fichero de scoring del PGS Catalog y devuelve un diccionario:\n",
        "\n",
        "        rsID -> {\n",
        "            \"chr\": str,\n",
        "            \"pos\": int,\n",
        "            \"effect_allele\": str,\n",
        "            \"other_allele\": str,\n",
        "            \"weight\": float\n",
        "        }\n",
        "    \"\"\"\n",
        "    pgs = []\n",
        "    header_cols = []\n",
        "    col_indices = {} # To store mapping from desired_name -> index\n",
        "\n",
        "    with extractData.open_compressed_file(pgs_path, \"rt\") as f:\n",
        "        # Saltar los metadatos hasta la línea de cabecera\n",
        "        for line in f:\n",
        "            if line.startswith(\"rsID\") or line.startswith(\"chr_name\"): # Assuming this is the actual header line\n",
        "                header_cols = [col.strip() for col in line.strip().split(\"\\t\")]\n",
        "                print(f\"Nombres de cabeceras --> {header_cols}\")\n",
        "                break\n",
        "\n",
        "        # Check if header columns were found\n",
        "        if not header_cols:\n",
        "            print(f\"Warning: Header line starting with 'rsID' not found in {pgs_path}\", file=sys.stderr)\n",
        "            return pd.DataFrame(pgs) # Return empty DataFrame if no header\n",
        "\n",
        "        # Define mappings for desired column names to actual header names\n",
        "        # and their original fixed indices as fallback\n",
        "        desired_to_actual_cols = {\n",
        "            \"rsID\": [\"hm_rsID\", \"rsID\"],\n",
        "            \"effect_allele\": [\"effect_allele\", \"effect_A\", \"allele_A\"],\n",
        "            \"other_allele\": [\"other_allele\", \"other_A\", \"allele_B\", \"hm_inferOtherAllele\"],\n",
        "            \"weight\": [\"weight\", \"beta\", \"score_w\", \"effect_weight\"],\n",
        "            \"chr\": [\"hm_chr\", \"chromosome\", \"CHR\", \"chr_name\"], # Original code implies index 9\n",
        "            \"pos\": [\"hm_pos\", \"position\", \"POS\", \"chr_position\"] # Original code implies index 10\n",
        "        }\n",
        "\n",
        "        # Try to find the index for each desired column\n",
        "        for desired_name, possible_actual_names in desired_to_actual_cols.items():\n",
        "            found_idx = -1\n",
        "            for actual_name in possible_actual_names:\n",
        "                if actual_name in header_cols:\n",
        "                    found_idx = header_cols.index(actual_name)\n",
        "                    break\n",
        "\n",
        "            # If not found by name, use the original positional index as fallback\n",
        "            if found_idx == -1:\n",
        "                if desired_name == \"rsID\": found_idx = 8\n",
        "                elif desired_name == \"effect_allele\": found_idx = 3\n",
        "                elif desired_name == \"other_allele\": found_idx = 4\n",
        "                elif desired_name == \"weight\": found_idx = 5\n",
        "                elif desired_name == \"chr\": found_idx = 9 # Based on original code\n",
        "                elif desired_name == \"pos\": found_idx = 10 # Based on original code\n",
        "\n",
        "                # If we still haven't found an index, it's a critical error for required columns\n",
        "                if found_idx == -1:\n",
        "                    print(f\"Error: Could not determine index for column '{desired_name}' in file {pgs_path}\", file=sys.stderr)\n",
        "                    return pd.DataFrame(pgs) # Exit if essential column is missing\n",
        "\n",
        "            col_indices[desired_name] = found_idx\n",
        "\n",
        "        print(f\"Col índices --> {col_indices}\")\n",
        "\n",
        "        # A partir de aquí vienen las filas de variantes\n",
        "        for line in f:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split(\"\\t\")\n",
        "\n",
        "            # Ensure the row has enough columns\n",
        "            max_idx = -1\n",
        "            if col_indices: # Ensure col_indices is not empty before calling max\n",
        "                max_idx = max(col_indices.values())\n",
        "\n",
        "            if len(parts) < max_idx + 1:\n",
        "                continue\n",
        "\n",
        "            # Access data using the determined indices\n",
        "            rsid = parts[col_indices[\"rsID\"]].strip()\n",
        "            if rsid == \"\":\n",
        "                rsid = \".\"\n",
        "\n",
        "            chr_name = parts[col_indices[\"chr\"]].strip()\n",
        "\n",
        "            try:\n",
        "                pos = int(parts[col_indices[\"pos\"]])\n",
        "            except (ValueError, IndexError):\n",
        "                continue\n",
        "\n",
        "            effect_allele = parts[col_indices[\"effect_allele\"]].strip()\n",
        "            other_allele = parts[col_indices[\"other_allele\"]].strip()\n",
        "\n",
        "            # Validate and convert weight\n",
        "            try:\n",
        "                weight = float(parts[col_indices[\"weight\"]])\n",
        "            except (ValueError, IndexError):\n",
        "                continue\n",
        "\n",
        "\n",
        "            # cromosoma de salida\n",
        "            if add_chr_prefix and not chr_name.startswith(\"chr\"):\n",
        "                chr_out = f\"chr{chr_name}\"\n",
        "            else:\n",
        "                chr_out = chr_name\n",
        "\n",
        "            pgs.append({\n",
        "                \"rsID\": rsid,\n",
        "                \"chr\": chr_out,\n",
        "                \"pos\": pos,\n",
        "                \"effect_allele\": effect_allele,\n",
        "                \"other_allele\": other_allele,\n",
        "                \"weight\": weight\n",
        "            })\n",
        "\n",
        "    # Crear el DataFrame de pandas con los datos acumulados\n",
        "    df_pgs_modelo = pd.DataFrame(pgs)\n",
        "\n",
        "    return df_pgs_modelo"
      ],
      "metadata": {
        "id": "pBL9vOBke3o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------\n",
        "# FUNCIÓN PARA CALCULAR EL PRS\n",
        "# -------------------------------------------------------------\n",
        "def calculate_prs(df, df_modelo):\n",
        "    # If df_modelo is empty, return 0 PRS, as no scoring can be done\n",
        "    if df_modelo.empty:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "    # Combina el modelo y los datos del participante por cromosoma y posición del cromosoma\n",
        "    df_combinado_chr_pos = pd.merge(df, df_modelo, on=['chr', 'pos'], how='inner', suffixes=('_est', '_pgs'))\n",
        "\n",
        "    # Calcula la columna 'score' condicionalmente\n",
        "    df_combinado_chr_pos['score'] = np.where(\n",
        "        (df_combinado_chr_pos['effect_allele'] == df_combinado_chr_pos['allele1']) |\n",
        "        (df_combinado_chr_pos['effect_allele'] == df_combinado_chr_pos['allele2']),\n",
        "        df_combinado_chr_pos['weight'] * df_combinado_chr_pos['dosage'],\n",
        "        0 # Si la condición no se cumple, el score es 0\n",
        "    )\n",
        "\n",
        "    prs_total = df_combinado_chr_pos['score'].sum()\n",
        "\n",
        "    return prs_total\n",
        "\n"
      ],
      "metadata": {
        "id": "TCUcw92dljLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 1. Estándarizar ficheros de datos\n",
        "\n",
        "1.   Leer y cargar el fichero downloads_files.csv que contiene el listado de los ficheros a analizar y el tipo de datos.\n",
        "2.   Recorrer el fichero y según el tipo de fichero ir a una función u otra para estandarizar (TSV, TSV3, VCF)\n",
        "3. Para cada participante. Generar un fichero con sus datos estandarizados.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mzk7tb8yFAWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Procesando el fichero de variantes del individuo\n",
        "\n",
        "# Abre el fichero Excel que contiene el listado de ficheros de partacipantes a estandarizar\n",
        "files_to_std_df = extractData.load_csv_to_dataframe(data_path,download_files)\n",
        "\n",
        "# Recorre uno a uno todos los elementos del dataset\n",
        "print(f\"##### ESTANDARIZACIÓN DE FICHEROS #####\")\n",
        "\n",
        "save_file = True\n",
        "for index, row in files_to_std_df.iterrows():\n",
        "    id_participant = row['Participant_ID']\n",
        "    file_name = row['File_name']\n",
        "    file_type = row['File_Type']\n",
        "    var_file = participant_data_path + id_participant + \"/\" + file_name\n",
        "    out_std_file = std_files +  id_participant + \".tsv\"\n",
        "\n",
        "    if index>=0:\n",
        "        # Estandarizar fichero en función del tipo de archivo\n",
        "        print(f\"### {index}. PARTICIPANTE: {id_participant} {file_type} ###\")\n",
        "        if file_type == \"TSV\":\n",
        "            variants_df, file_path = estandarizar_TSV_files(var_file, out_std_file, save_file)\n",
        "        elif file_type == \"VCF\":\n",
        "            variants_df, file_path = estandarizar_VCF_files(var_file, out_std_file, True, save_file)\n",
        "        elif file_type == \"TSV3\":\n",
        "            variants_df, file_path = estandarizar_TSV3_files(var_file, out_std_file, save_file)\n",
        "        else:\n",
        "            print(f\"[{id_participant}] Tipo de fichero desconocido: {file_type}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PWrNYuOCpw9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 2. Calcula el PRS para todos los participantes y todos los modelos\n",
        "\n",
        "1. Recorre los ficheros estandarizdos de cada uno de los participatnes\n",
        "\n",
        "2. Para cada participante\n",
        "\n",
        "- Recorre todos los modelos\n",
        "\n",
        "3. Para cada pareja participante - modelo calcula el PRS\n",
        "\n",
        "\n",
        "Finalmente crea un dataframe donde las columnas son los modelos y las filas son los participantes."
      ],
      "metadata": {
        "id": "O0iKXYtac1J-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Crear una lista de nombres de ficheros en el directorio std_files\n",
        "std_files_list = []\n",
        "if os.path.exists(std_files):\n",
        "    for filename in os.listdir(std_files):\n",
        "        if os.path.isfile(os.path.join(std_files, filename)):\n",
        "            std_files_list.append(filename)\n",
        "print(f\"Ficheros en {std_files}:\\n{std_files_list[:5]}...\") # Muestra los primeros 5 por brevedad\n",
        "print(f\"Total de ficheros estandarizados: {len(std_files_list)}\\n\")\n",
        "\n",
        "# 2. Recorrer la carpeta Modelos y guardar nombres de carpeta y archivo\n",
        "modelos_files_list = []\n",
        "if os.path.exists(modelos):\n",
        "    for root, dirs, files in os.walk(modelos):\n",
        "        # Extraer el nombre de la carpeta actual (último componente de la ruta)\n",
        "        current_folder_name = os.path.basename(root)\n",
        "        for file in files:\n",
        "            modelos_files_list.append((current_folder_name, file))\n",
        "print(f\"Modelos encontrados (carpeta, archivo):\\n{modelos_files_list[:5]}...\") # Muestra los primeros 5 por brevedad\n",
        "print(f\"Total de modelos encontrados: {len(modelos_files_list)}\")\n",
        "\n",
        "# 1. Extract participant IDs from std_files_list\n",
        "participant_ids = [filename.replace('.tsv', '') for filename in std_files_list]\n",
        "\n",
        "# 2. Extract unique model IDs from modelos_files_list\n",
        "model_ids = sorted(list(set([model_info[0] for model_info in modelos_files_list])))\n",
        "\n",
        "# 3. Create an empty Pandas DataFrame named results_df\n",
        "results_df = pd.DataFrame(index=participant_ids, columns=model_ids)\n",
        "\n",
        "print(\"results_df initialized with shape:\", results_df.shape)\n",
        "print(\"First 5 participant IDs (index):\", results_df.index[:5].tolist())\n",
        "print(\"First 5 model IDs (columns):\", results_df.columns[:5].tolist())\n",
        "\n",
        "# Iterate through each standardized participant file\n",
        "for participant_filename in std_files_list:\n",
        "    # Extract participant_id\n",
        "    participant_id = participant_filename.replace('.tsv', '')\n",
        "\n",
        "    # Construct full path to participant's standardized file\n",
        "    current_participant_file = std_files + participant_filename\n",
        "\n",
        "    # Load participant's data\n",
        "    df_estandarizado = pd.read_csv(current_participant_file, sep='\\t')\n",
        "\n",
        "    # Iterate through each model file\n",
        "    for model_id, model_filename in modelos_files_list:\n",
        "        print(f\"Processing participant {participant_id} with model {model_id}\")\n",
        "\n",
        "        # Construct full path to the model file\n",
        "        current_pgs_path = modelos + model_id + \"/\" + model_filename\n",
        "\n",
        "        # Load model data\n",
        "        modelo_pgs = leer_modelo_pgs(current_pgs_path, True)\n",
        "\n",
        "        # Calculate PRS\n",
        "        prs_value = calculate_prs(df_estandarizado, modelo_pgs)\n",
        "\n",
        "        # Store the calculated PRS value in results_df\n",
        "        results_df.loc[participant_id, model_id] = prs_value\n",
        "\n",
        "print(\"PRS calculation complete. Displaying the first few rows of results_df:\")\n",
        "print(results_df.head())\n",
        "\n",
        "results_df.to_csv(data_path + \"/prs_calculados.tsv\", sep=\"\\t\", index=True)"
      ],
      "metadata": {
        "id": "QGEVJiF0dq0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis estadístico descriptivo\n",
        "\n",
        "* Estandarización de los modelos\n",
        "* Identificar correlaciones entre los modelos\n",
        "* Estudiar la distribución de los diferentes modelos\n",
        "* homocedasticidad\n",
        "* Outlayers\n"
      ],
      "metadata": {
        "id": "ptD0N4y0zn0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### Estandarización de modelos a través de z-score #####\n",
        "\n",
        "# Carga modelos\n",
        "file_modelo_prs = data_path + \"/prs_calculados.tsv\"\n",
        "file_model_prs_zscore = data_path + \"/prs_calculados_zscore.tsv\"\n",
        "ID_COL = \"Unnamed: 0\"\n",
        "\n",
        "df_prs = pd.read_csv(file_modelo_prs, sep=\"\\t\")\n",
        "\n",
        "# Renombra la columna del modelo (Identificador)\n",
        "df_prs = df_prs.rename(columns={df_prs.columns[0]: 'Participant_ID'})\n",
        "\n",
        "# IDs de participantes a excluir\n",
        "excluded_participants = ['huA06676', 'huDCF4FA']\n",
        "\n",
        "# Filtrar el DataFrame para excluir a los participantes especificados\n",
        "df_prs = df_prs[~df_prs['Participant_ID'].isin(excluded_participants)]\n",
        "\n",
        "models = [c for c in df_prs.columns if c != 'Participant_ID']\n",
        "X = df_prs[models].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "if X.isna().any().any():\n",
        "    raise ValueError(\"Existen valores NaN en los PRS. Revisa el fichero.\")\n",
        "\n",
        "# Estandarización z-score\n",
        "Xz = (X - X.mean(axis=0)) / X.std(axis=0, ddof=1)\n",
        "\n",
        "# Reconstruye el dataset final\n",
        "df_prs_z = pd.concat([df_prs[['Participant_ID']], Xz], axis=1)\n",
        "df_prs_z.to_csv(file_model_prs_zscore, sep=\"\\t\", index=False)"
      ],
      "metadata": {
        "id": "TRVG4KAuznRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Estandarización de modelos a través de z-score #####\n",
        "\n",
        "# Carga modelos\n",
        "file_modelo_prs = data_path + \"/prs_calculados.tsv\"\n",
        "file_model_prs_zscore = data_path + \"/prs_calculados_zscore.tsv\"\n",
        "ID_COL = \"Participant_ID\"\n",
        "\n",
        "df_prs = pd.read_csv(file_modelo_prs, sep=\"\\t\")\n",
        "\n",
        "# Renombra la columna del modelo (Identificador)\n",
        "df_prs = df_prs.rename(columns={df_prs.columns[0]: 'Participant_ID'})\n",
        "models = [c for c in df_prs.columns if c != 'Participant_ID']\n",
        "X = df_prs[models].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "if X.isna().any().any():\n",
        "    raise ValueError(\"Existen valores NaN en los PRS. Revisa el fichero.\")\n",
        "\n",
        "# Estandarización z-score\n",
        "Xz = (X - X.mean(axis=0)) / X.std(axis=0, ddof=1)\n",
        "\n",
        "# Reconstruye el dataset final\n",
        "df_prs_z = pd.concat([df_prs[['Participant_ID']], Xz], axis=1)\n",
        "df_prs_z.to_csv(file_model_prs_zscore, sep=\"\\t\", index=False)\n",
        "\n",
        "##### Correlación entre modelos #####\n",
        "CORR_METHOD = \"pearson\"\n",
        "CLUSTER_CORR_THRESHOLD = 0.90   # |r| >= 0.90 -> mismo clúster\n",
        "\n",
        "# Figuras\n",
        "HEATMAP_MAX_MODELS = 49        # si fueran muchos, podría recortarse\n",
        "FIG_DPI = 160\n",
        "\n",
        "ID_COL = 'Participant_ID'\n",
        "\n",
        "models = [c for c in df_prs_z.columns if c != ID_COL]\n",
        "Xz = df_prs_z[models].copy()\n",
        "\n",
        "corr = Xz.corr(method=CORR_METHOD)\n",
        "corr.to_csv(data_path + \"/correlaciones_\" + CORR_METHOD + \".csv\")\n",
        "\n",
        "# Histograma de correlaciones\n",
        "# Excluir la diagonal\n",
        "corr_matrix_without_diag = corr.copy()\n",
        "np.fill_diagonal(corr_matrix_without_diag.values, np.nan)\n",
        "corr_vals = corr_matrix_without_diag.stack().to_numpy()\n",
        "plt.figure()\n",
        "plt.hist(corr_vals, bins=40)\n",
        "plt.title(\"Distribución de correlaciones Pearson entre modelos\")\n",
        "plt.xlabel(\"r\")\n",
        "plt.ylabel(\"frecuencia\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Mapa de calor de correlaciones\n",
        "plt.figure(figsize=(12, 10)) # Ajusta el tamaño de la figura según necesidad\n",
        "plt.imshow(corr.values, cmap='coolwarm', aspect='auto')\n",
        "plt.colorbar(label='Pearson r')\n",
        "plt.xticks(range(len(models)), models, rotation=90, fontsize=8)\n",
        "plt.yticks(range(len(models)), models, fontsize=8)\n",
        "plt.title('Mapa de calor de correlaciones Pearson entre modelos')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Distancia = 1 - |r|\n",
        "dist = 1 - corr.abs()\n",
        "\n",
        "# Convertir a forma condensada (requerido por scipy)\n",
        "dist_condensed = squareform(dist.values, checks=False)\n",
        "\n",
        "# 4) Clustering jerárquico\n",
        "Z = linkage(dist_condensed, method=\"average\")\n",
        "\n",
        "# Umbral de distancia correspondiente a |r| >= threshold\n",
        "distance_threshold = 1 - CLUSTER_CORR_THRESHOLD\n",
        "\n",
        "clusters = fcluster(Z, t=distance_threshold, criterion=\"distance\")\n",
        "\n",
        "cluster_df = pd.DataFrame({\n",
        "    \"model\": models,\n",
        "    \"cluster\": clusters\n",
        "}).sort_values(\"cluster\")\n",
        "\n",
        "cluster_df.to_csv(data_path + \"/clusters_modelos_prs.csv\", index=False)\n",
        "print(\"OK: clusters_modelos_prs.csv\")\n",
        "\n",
        "# =========================\n",
        "# 5) Modelo representativo por clúster\n",
        "# =========================\n",
        "representatives = []\n",
        "\n",
        "for cl in sorted(cluster_df[\"cluster\"].unique()):\n",
        "    cl_models = cluster_df.loc[cluster_df[\"cluster\"] == cl, \"model\"].tolist()\n",
        "\n",
        "    if len(cl_models) == 1:\n",
        "        representatives.append((cl, cl_models[0], 1, 1.0))\n",
        "        continue\n",
        "\n",
        "    # Correlación media absoluta dentro del clúster\n",
        "    subcorr = corr.loc[cl_models, cl_models].abs()\n",
        "    mean_corr = subcorr.mean(axis=1)\n",
        "\n",
        "    best_model = mean_corr.idxmax()\n",
        "    representatives.append(\n",
        "        (cl, best_model, len(cl_models), mean_corr[best_model])\n",
        "    )\n",
        "\n",
        "rep_df = pd.DataFrame(\n",
        "    representatives,\n",
        "    columns=[\"cluster\", \"representative_model\", \"cluster_size\", \"mean_abs_correlation\"]\n",
        ").sort_values(\"cluster\")\n",
        "\n",
        "rep_df.to_csv(data_path + \"/modelos_representativos_por_cluster.csv\", index=False)\n",
        "print(\"OK: modelos_representativos_por_cluster.csv\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 6) PRS estandarizados solo con modelos representativos\n",
        "# =========================\n",
        "rep_models = rep_df[\"representative_model\"].tolist()\n",
        "\n",
        "# Incluir el ID_COL (Participant_ID) junto con los modelos representativos\n",
        "Xz_rep = df_prs_z[[ID_COL] + rep_models].copy()\n",
        "Xz_rep.to_csv(data_path + \"/prs_zscore_modelos_representativos.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(\"OK: prs_zscore_modelos_representativos.tsv\")\n",
        "print(f\"Reducción: {len(models)} → {len(rep_models)} modelos\")\n",
        "\n",
        "\n",
        "# 5.1 Heatmap de correlación (modelos originales o representativos)\n",
        "# (Aquí lo hago con representativos para que sea legible)\n",
        "corr_rep = Xz_rep[rep_models].corr(method=\"pearson\") # Excluir ID_COL para la correlación\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(corr_rep.values, aspect=\"auto\")\n",
        "plt.colorbar(label=\"Pearson r\")\n",
        "plt.xticks(range(len(rep_models)), rep_models, rotation=90, fontsize=7)\n",
        "plt.yticks(range(len(rep_models)), rep_models, fontsize=7)\n",
        "plt.title(\"Heatmap de correlación (PRS z-score) - Modelos representativos\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5.2 Dendrograma (modelos originales, más informativo)\n",
        "plt.figure(figsize=(12, 6))\n",
        "dendrogram(Z, labels=models, leaf_rotation=90, color_threshold=distance_threshold)\n",
        "plt.title(f\"Dendrograma jerárquico de modelos PRS (dist = 1 - |r|), corte |r|≥{CLUSTER_CORR_THRESHOLD}\")\n",
        "plt.ylabel(\"Distancia (1 - |r|)\")\n",
        "plt.tight_layout()\n",
        "plt.show() # Display the dendrogram"
      ],
      "metadata": {
        "id": "9SOL7T1MhPq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8482b842-1e9a-41f2-ba26-1755106e6a17"
      },
      "source": [
        "# Identificar modelos excluidos\n",
        "excluded_models = list(set(models) - set(rep_models))\n",
        "\n",
        "print(\"##### Modelos Excluidos #####\")\n",
        "if excluded_models:\n",
        "    print(\"Los siguientes modelos fueron excluidos por ser altamente correlacionados (Pearson |r| >= 0.90) con otros modelos y, por lo tanto, no se consideraron representativos de sus clústeres:\")\n",
        "    for model in sorted(excluded_models):\n",
        "        print(f\"- {model}\")\n",
        "    print(f\"\\nNúmero total de modelos excluidos: {len(excluded_models)}\")\n",
        "else:\n",
        "    print(\"No se excluyó ningún modelo. Todos los modelos fueron considerados representativos.\")\n",
        "\n",
        "print(\"\\n##### Explicación de la Exclusión #####\")\n",
        "print(\"Durante el proceso de clustering jerárquico, los modelos se agrupan en clústeres basados en su similitud (baja distancia, que corresponde a alta correlación Pearson). Para cada clúster, se selecciona un único 'modelo representativo' (medoid), que es el modelo con la mayor correlación media absoluta con los otros miembros de su clúster. Los demás modelos en ese clúster se consideran redundantes y se excluyen para reducir la dimensionalidad y evitar la multicolinealidad en análisis posteriores. Esto asegura que solo tengamos un conjunto de modelos relativamente independientes que representen la diversidad de los PRS calculados.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b8817f1"
      },
      "source": [
        "# Visualización de las distribuciones de los modelos (histogramas)\n",
        "file_model_prs_zscore = data_path + \"/prs_calculados_zscore.tsv\"\n",
        "df_prs_z = pd.read_csv(file_model_prs_zscore, sep=\"\\t\")\n",
        "\n",
        "df_models =  pd.read_csv(data_path + \"/modelos_representativos_por_cluster.csv\", sep=\",\")\n",
        "rep_df = df_models.sort_values(\"cluster\")\n",
        "rep_models = rep_df[\"representative_model\"].tolist()\n",
        "\n",
        "\n",
        "# Filtrar df_prs_z para incluir solo los modelos representativos\n",
        "models_to_plot = [c for c in df_prs_z.columns if c in rep_models]\n",
        "\n",
        "# Definir el número de filas y columnas para el grid\n",
        "n_cols = 5\n",
        "n_rows = (len(models_to_plot) + n_cols - 1) // n_cols # Calcula el número de filas necesario\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 3), sharex=False, sharey=False)\n",
        "axes = axes.flatten() # Aplanar para facilitar la iteración\n",
        "\n",
        "for i, model_name in enumerate(models_to_plot):\n",
        "    ax = axes[i]\n",
        "\n",
        "    # Plot histogram\n",
        "    sns.histplot(df_prs_z[model_name], bins=30, kde=True, ax=ax, edgecolor='black', color='skyblue', stat='density')\n",
        "\n",
        "    ax.set_title(f'{model_name}', fontsize=10)\n",
        "    ax.set_xlabel('Valor Z-Score', fontsize=8)\n",
        "    ax.set_ylabel('Densidad', fontsize=8)\n",
        "    ax.tick_params(axis='x', labelsize=7)\n",
        "    ax.tick_params(axis='y', labelsize=7)\n",
        "    ax.grid(axis='y', alpha=0.75)\n",
        "\n",
        "# Ocultar los subplots vacíos si los hay\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.suptitle('Distribución Z-Score de los Modelos PRS con Línea de Densidad (Modelos Representativos)', fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.97]) # Ajustar layout para suptitle\n",
        "plt.savefig(f\"{data_path}/hist_distribuciones_grid_representativos.png\", dpi=300)\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "print(\"Histogramas de las distribuciones de los modelos representativos guardados en: \" + data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ID_COL = 'Participant_ID'\n",
        "\n",
        "# Usar solo los modelos representativos para el análisis\n",
        "models = rep_models\n",
        "Xz = df_prs_z[models].copy()\n",
        "\n",
        "# Seguridad: asegurar numéricos y sin NaNs\n",
        "Xz = Xz.apply(pd.to_numeric, errors=\"coerce\")\n",
        "if Xz.isna().any().any():\n",
        "    raise ValueError(\"Hay NaNs tras convertir a numérico. Revisa separadores/valores.\")\n",
        "\n",
        "n, p = Xz.shape\n",
        "print(f\"Participantes: {n}, Modelos: {p}, Missing: {Xz.isna().sum().sum()}\")\n",
        "\n",
        "##### Descripción por modelo #####\n",
        "desc_stats = Xz.describe().T\n",
        "# Asimetria. Grado de la desvicación frente a la normal en sentido horizontal\n",
        "desc_stats[\"skew\"] = Xz.apply(skew)\n",
        "# Desviación de las colas\n",
        "desc_stats[\"kurtosis_excess\"] = Xz.apply(lambda s: kurtosis(s, fisher=True))\n",
        "\n",
        "#### Normalidad de los modelos ######\n",
        "norm_rows = []\n",
        "for col in models:\n",
        "    vals = Xz[col].to_numpy()\n",
        "\n",
        "    W, p_sh = shapiro(vals)             # Shapiro–Wilk (bien para n~123)\n",
        "\n",
        "    norm_rows.append([col, W, p_sh])\n",
        "\n",
        "norm_stats = pd.DataFrame(norm_rows, columns=[\"model\", \"shapiro_W\", \"shapiro_p\"])\n",
        "norm_stats = norm_stats.set_index('model')\n",
        "\n",
        "# Corrección por múltiples contrastes (FDR)\n",
        "rej, p_adj, *_ = multipletests(norm_stats[\"shapiro_p\"], alpha=0.05, method=\"fdr_bh\")\n",
        "norm_stats[\"shapiro_p_fdr\"] = p_adj\n",
        "norm_stats[\"shapiro_reject_fdr\"] = rej\n",
        "\n",
        "# Combinar descriptivos y normalidad\n",
        "combined_stats = desc_stats.merge(norm_stats, left_index=True, right_index=True, how='left')\n",
        "combined_stats.to_csv(data_path + \"/descriptivos_y_normalidad_por_modelo.csv\")\n",
        "\n",
        "# Modelos que no cumplen la distribución normal\n",
        "non_normal_models = combined_stats[combined_stats[\"shapiro_reject_fdr\"] == True]\n",
        "\n",
        "if not non_normal_models.empty:\n",
        "    print(\"Modelos que no cumplen la distribución normal (Shapiro-Wilk con FDR < 0.05):\")\n",
        "    display(non_normal_models[[\"shapiro_W\", \"shapiro_p\", \"shapiro_p_fdr\"]])\n",
        "else:\n",
        "    print(\"Todos los modelos representativos cumplen con la distribución normal (Shapiro-Wilk con FDR < 0.05).\")\n",
        "\n",
        "# Boxplots de los modelos representativos en una única gráfica\n",
        "\n",
        "# Filtrar df_prs_z para incluir solo los modelos representativos\n",
        "# y luego preparar los datos para graficar todos los modelos en un mismo boxplot\n",
        "models_data = df_prs_z[rep_models]\n",
        "\n",
        "# Melt el DataFrame para tener un formato largo, adecuado para sns.boxplot con múltiples variables\n",
        "models_melted = models_data.melt(var_name='Modelo PRS', value_name='Valor Z-Score')\n",
        "\n",
        "plt.figure(figsize=(15, 7)) # Ajusta el tamaño de la figura para que sea legible\n",
        "sns.boxplot(x='Modelo PRS', y='Valor Z-Score', data=models_melted, hue='Modelo PRS', legend=False, palette='viridis')\n",
        "plt.title('Boxplots de todos los Modelos PRS (Z-Score) - Modelos Representativos', fontsize=16)\n",
        "plt.xlabel('Modelo PRS', fontsize=10)\n",
        "plt.ylabel('Valor Z-Score', fontsize=10)\n",
        "plt.xticks(rotation=90, fontsize=8) # Rota las etiquetas del eje X para evitar solapamiento\n",
        "plt.yticks(fontsize=8)\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.tight_layout() # Ajusta el layout para asegurar que todo el contenido se muestre\n",
        "plt.savefig(f\"{data_path}/boxplots_todos_modelos_representativos.png\", dpi=300)\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "print(\"Boxplots de todos los modelos representativos guardados en: \" + data_path)\n",
        "\n",
        "##### Homocedasticidad de los modelos #####\n",
        "# Suposición de que la varianza de los errores es constante\n",
        "groups = [Xz[c].to_numpy() for c in models]\n",
        "\n",
        "lev_stat, lev_p = levene(*groups, center=\"median\")\n",
        "fl_stat, fl_p = fligner(*groups)\n",
        "ba_stat, ba_p = bartlett(*groups)\n",
        "\n",
        "homo = pd.DataFrame(\n",
        "    [{\n",
        "        \"test\": \"Levene_median\", \"stat\": lev_stat, \"p\": lev_p\n",
        "    },{\n",
        "        \"test\": \"Fligner\", \"stat\": fl_stat, \"p\": fl_p\n",
        "    },{\n",
        "        \"test\": \"Bartlett\", \"stat\": ba_stat, \"p\": ba_p\n",
        "    }]\n",
        ")\n",
        "homo.to_csv(data_path + \"/homocedasticidad_entre_modelos.csv\", index=True)\n",
        "print(homo)\n",
        "\n",
        "\n",
        "##### Outliers / leverage / influyentes (participantes)  ######\n",
        "\n",
        "# 5.1 Outliers multivariantes: Mahalanobis robusta\n",
        "mcd = MinCovDet().fit(Xz)\n",
        "md2 = mcd.mahalanobis(Xz)  # distancia^2\n",
        "df_prs_z[\"md2_robust\"] = md2\n",
        "\n",
        "# 5.2 Leverage en espacio reducido (PCA al 95% varianza)\n",
        "pca = PCA(n_components=0.95, svd_solver=\"full\")\n",
        "scores = pca.fit_transform(Xz.to_numpy())  # n x k\n",
        "k = scores.shape[1]\n",
        "\n",
        "XtX_inv = np.linalg.inv(scores.T @ scores)\n",
        "h = np.einsum(\"ij,jk,ik->i\", scores, XtX_inv, scores)  # diag(H)\n",
        "df_prs_z[\"leverage_pca\"] = h\n",
        "\n",
        "# Umbral típico: 2k/n\n",
        "lev_thr = 2 * k / n\n",
        "df_prs_z[\"high_leverage\"] = df_prs_z[\"leverage_pca\"] > lev_thr\n",
        "\n",
        "# 5.3 Score de influencia (heurístico) combinando distancia y leverage\n",
        "df_prs_z[\"influence_score\"] = df_prs_z[\"md2_robust\"] * (df_prs_z[\"leverage_pca\"] / (1 - df_prs_z[\"leverage_pca\"] + 1e-9))\n",
        "\n",
        "out_part = df_prs_z[[ID_COL, \"md2_robust\", \"leverage_pca\", \"high_leverage\", \"influence_score\"]].sort_values(\n",
        "    \"influence_score\", ascending=False\n",
        ")\n",
        "out_part.to_csv(data_path + \"/participantes_outliers_leverage_influencia.csv\", index=False)\n",
        "print(\"OK: participantes_outliers_leverage_influencia.csv\")\n",
        "display(out_part.head())"
      ],
      "metadata": {
        "id": "QCsAqdQAZb9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc4e723a"
      },
      "source": [
        "file_auc_path = data_path + 'AUC.xlsx'\n",
        "df_auc = pd.read_excel(file_auc_path)\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "file_auc_path = data_path + 'AUC.xlsx'\n",
        "\n",
        "# Check if the file exists before attempting to load\n",
        "if os.path.exists(file_auc_path):\n",
        "    df_auc = pd.read_excel(file_auc_path)\n",
        "\n",
        "    print(f\"'AUC.XLSX' loaded into df_auc with shape: {df_auc.shape}\")\n",
        "    print(\"First 5 rows of df_auc:\")\n",
        "    display(df_auc.head())\n",
        "else:\n",
        "    print(f\"Error: The file '{file_auc_path}' was not found. Please ensure 'AUC.XLSX' is uploaded to the '{data_path}' directory.\")\n",
        "    df_auc = pd.DataFrame() # Initialize an empty DataFrame to avoid errors in subsequent steps\n",
        "\n",
        "if df_auc.empty:\n",
        "    print(\"Cannot calculate descriptive statistics: df_auc is empty. Please upload the 'AUC.XLSX' file first.\")\n",
        "else:\n",
        "    # Assuming the AUC values are in a column named 'AUC'\n",
        "    # If the column name is different, please adjust 'AUC' accordingly\n",
        "    auc_column = 'AUC'\n",
        "\n",
        "    if auc_column not in df_auc.columns:\n",
        "        print(f\"Error: Column '{auc_column}' not found in df_auc. Available columns are: {df_auc.columns.tolist()}\")\n",
        "    else:\n",
        "        # Calculate descriptive statistics\n",
        "        min_auc = df_auc[auc_column].min()\n",
        "        max_auc = df_auc[auc_column].max()\n",
        "        q1_auc = df_auc[auc_column].quantile(0.25)\n",
        "        q3_auc = df_auc[auc_column].quantile(0.75)\n",
        "        mean_auc = df_auc[auc_column].mean()\n",
        "        median_auc = df_auc[auc_column].median()\n",
        "\n",
        "        # Create a DataFrame for descriptive statistics\n",
        "        descriptive_stats_df = pd.DataFrame({\n",
        "            'Metric': ['Minimum', '1st Quartile', 'Median', 'Mean', '3rd Quartile', 'Maximum'],\n",
        "            'Value': [min_auc, q1_auc, median_auc, mean_auc, q3_auc, max_auc]\n",
        "        })\n",
        "\n",
        "        print(\"\\nDescriptive Statistics for AUC values:\")\n",
        "        display(descriptive_stats_df)\n",
        "\n",
        "        # Generate horizontal boxplot\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        sns.boxplot(x=df_auc[auc_column], orient='h', color='skyblue')\n",
        "        plt.title('Boxplot de los valores de AUC')\n",
        "        plt.xlabel('AUC Value')\n",
        "        plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}