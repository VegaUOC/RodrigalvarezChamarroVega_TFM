{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Generación de modelos ##\n"
      ],
      "metadata": {
        "id": "yl0Mn1ce5Ts1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FyHUPeXI_q7"
      },
      "outputs": [],
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Cambiar el directorio de trabajo\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Define the TFM directory path\n",
        "TFM_PATH = '/content/drive/My Drive/TFM'\n",
        "\n",
        "# Change the current working directory to the TFM directory\n",
        "os.chdir(TFM_PATH)\n",
        "print(f\"Current working directory changed to: {os.getcwd()}\")\n",
        "\n",
        "# Add the TFM directory to the Python system path\n",
        "if TFM_PATH not in sys.path:\n",
        "    sys.path.append(TFM_PATH)\n",
        "    print(f\"'{TFM_PATH}' added to Python system path.\")\n",
        "else:\n",
        "    print(f\"'{TFM_PATH}' is already in Python system path.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ml_pipeline_htn.py\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any, Optional, Tuple, List\n",
        "\n",
        "from sklearn.base import clone\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_predict\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, accuracy_score, f1_score, precision_score, recall_score,\n",
        "    confusion_matrix, make_scorer\n",
        ")\n",
        "from sklearn.base import clone\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# XGBoost (opcional)\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAS_XGB = True\n",
        "except Exception:\n",
        "    HAS_XGB = False\n",
        "\n",
        "# Statsmodels para p-values en logística (opcional)\n",
        "try:\n",
        "    import statsmodels.api as sm\n",
        "    HAS_SM = True\n",
        "except Exception:\n",
        "    HAS_SM = False\n",
        "\n",
        "# SHAP (opcional, para explicabilidad “si es posible”)\n",
        "try:\n",
        "    import shap\n",
        "    HAS_SHAP = True\n",
        "except Exception:\n",
        "    HAS_SHAP = False\n"
      ],
      "metadata": {
        "id": "Xofdzh0K5e6o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables\n",
        "TARGET = \"Hypertension\"                  # <- cambia si tu target tiene otro nombre\n",
        "\n",
        "source_path = TFM_PATH + '/sources/'\n",
        "SEP = \"\\t\"\n",
        "\n",
        "dataset_covariables = \"selected_participants_imputed_reduced_standardized.tsv\"\n",
        "dataset_covariables_analysis = \"selected_participants_imputed_analysis_standardized.tsv\"\n",
        "dataset_modelos = \"prs_zscore_modelos_representativos_target.tsv\"\n",
        "dataset_cov_mod = \"selected_participants_modelos_reduced_standardized.tsv\"\n",
        "dataset_cov_mod_analysis = \"selected_participants_modelos_analysis_standardized.tsv\""
      ],
      "metadata": {
        "id": "UbKvjojTp6K7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Utilidades de carga / limpieza\n",
        "# =========================================================\n",
        "def _maybe_convert_comma_decimal(series: pd.Series, min_numeric_ratio: float = 0.98) -> pd.Series:\n",
        "    \"\"\"Convierte strings tipo '0,123' -> float 0.123 si casi todo es numérico.\"\"\"\n",
        "    s = series.astype(str).str.replace(\",\", \".\", regex=False)\n",
        "    num = pd.to_numeric(s, errors=\"coerce\")\n",
        "    if num.notna().mean() >= min_numeric_ratio:\n",
        "        return num\n",
        "    return series\n",
        "\n",
        "\n",
        "def load_dataset_tsv(\n",
        "    path: str,\n",
        "    target_candidates: Tuple[str, ...] = (\"Hypertension\", \"target\", \"HTN\"),\n",
        "    drop_cols: Tuple[str, ...] = (\"Participant\", \"Participant_ID\", \"participant_id\"),\n",
        ") -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    df = pd.read_csv(path, sep=\"\\t\")\n",
        "\n",
        "    # target\n",
        "    target_col = None\n",
        "    for c in target_candidates:\n",
        "        if c in df.columns:\n",
        "            target_col = c\n",
        "            break\n",
        "    if target_col is None:\n",
        "        raise ValueError(f\"No se encontró target en {path}. Probé: {target_candidates}\")\n",
        "\n",
        "    # drop ids\n",
        "    for c in drop_cols:\n",
        "        if c in df.columns:\n",
        "            df = df.drop(columns=[c])\n",
        "\n",
        "    y = df[target_col].astype(int)\n",
        "    X = df.drop(columns=[target_col])\n",
        "\n",
        "    # convertir coma decimal en columnas object si aplica\n",
        "    Xc = X.copy()\n",
        "    for col in Xc.columns:\n",
        "        if Xc[col].dtype == \"object\":\n",
        "            Xc[col] = _maybe_convert_comma_decimal(Xc[col])\n",
        "\n",
        "    return Xc, y\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Preprocesado: numéricas + categóricas\n",
        "# =========================================================\n",
        "def make_preprocessor(\n",
        "    X: pd.DataFrame,\n",
        "    scale_numeric: bool = True,\n",
        "    impute_numeric: str = \"median\",\n",
        "    impute_categorical: str = \"most_frequent\",\n",
        "    onehot_min_frequency: Optional[float] = None,  # None => normal onehot\n",
        ") -> ColumnTransformer:\n",
        "\n",
        "    numeric_features = X.select_dtypes(include=[\"number\", \"bool\"]).columns.tolist()\n",
        "    categorical_features = [c for c in X.columns if c not in numeric_features]\n",
        "\n",
        "    num_steps = [(\"imputer\", SimpleImputer(strategy=impute_numeric))]\n",
        "    if scale_numeric:\n",
        "        num_steps.append((\"scaler\", StandardScaler()))\n",
        "\n",
        "    if onehot_min_frequency is None:\n",
        "        ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "    else:\n",
        "        # útil si hubiera categorías raras\n",
        "        ohe = OneHotEncoder(handle_unknown=\"ignore\", min_frequency=onehot_min_frequency)\n",
        "\n",
        "    cat_pipe = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=impute_categorical)),\n",
        "        (\"onehot\", ohe),\n",
        "    ])\n",
        "\n",
        "    preprocess = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", Pipeline(num_steps), numeric_features),\n",
        "            (\"cat\", cat_pipe, categorical_features),\n",
        "        ],\n",
        "        remainder=\"drop\",\n",
        "        verbose_feature_names_out=True\n",
        "    )\n",
        "    return preprocess\n"
      ],
      "metadata": {
        "id": "JOGdBjAD6QWG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class CVResult:\n",
        "    folds: pd.DataFrame\n",
        "    summary: pd.Series\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Métricas CV\n",
        "# =========================================================\n",
        "def evaluate_with_cv(\n",
        "    base_model,\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.Series,\n",
        "    preprocess: ColumnTransformer,\n",
        "    n_splits: int = 5,\n",
        "    random_state: int = 42,\n",
        "    threshold: float = 0.5,\n",
        ") -> CVResult:\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "    TP = TN = FP = FN = 0\n",
        "\n",
        "    for fold, (tr, te) in enumerate(skf.split(X, y), start=1):\n",
        "        model = clone(base_model)\n",
        "        pipe = Pipeline([(\"preprocess\", preprocess), (\"model\", model)])\n",
        "        pipe.fit(X.iloc[tr], y.iloc[tr])\n",
        "\n",
        "        # Probabilidades\n",
        "        if hasattr(pipe.named_steps[\"model\"], \"predict_proba\"):\n",
        "            proba = pipe.predict_proba(X.iloc[te])[:, 1]\n",
        "        else:\n",
        "            # por compatibilidad (p.ej., SVC sin probability=True)\n",
        "            # aquí asumimos que se configuró probability=True si se quiere AUC\n",
        "            raise ValueError(\"El modelo no tiene predict_proba. Configura probability=True si aplica.\")\n",
        "\n",
        "        pred = (proba >= threshold).astype(int)\n",
        "        y_te = y.iloc[te]\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(y_te, pred, labels=[0, 1]).ravel()\n",
        "        TN += tn; FP += fp; FN += fn; TP += tp\n",
        "\n",
        "        rows.append({\n",
        "            \"fold\": fold,\n",
        "            \"AUC\": roc_auc_score(y_te, proba),\n",
        "            \"accuracy\": accuracy_score(y_te, pred),\n",
        "            \"f1\": f1_score(y_te, pred, zero_division=0),  # f-measure\n",
        "            \"precision\": precision_score(y_te, pred, zero_division=0),\n",
        "            \"recall\": recall_score(y_te, pred, zero_division=0),\n",
        "            \"TN\": tn, \"FP\": fp, \"FN\": fn, \"TP\": tp\n",
        "        })\n",
        "\n",
        "    folds = pd.DataFrame(rows)\n",
        "\n",
        "    summary = pd.Series({\n",
        "        \"AUC_mean\": folds[\"AUC\"].mean(),\n",
        "        \"AUC_std\": folds[\"AUC\"].std(ddof=1),\n",
        "        \"accuracy_mean\": folds[\"accuracy\"].mean(),\n",
        "        \"accuracy_std\": folds[\"accuracy\"].std(ddof=1),\n",
        "        \"f1_mean\": folds[\"f1\"].mean(),\n",
        "        \"f1_std\": folds[\"f1\"].std(ddof=1),\n",
        "        \"precision_mean\": folds[\"precision\"].mean(),\n",
        "        \"precision_std\": folds[\"precision\"].std(ddof=1),\n",
        "        \"recall_mean\": folds[\"recall\"].mean(),\n",
        "        \"recall_std\": folds[\"recall\"].std(ddof=1),\n",
        "        \"TN_total\": TN, \"FP_total\": FP, \"FN_total\": FN, \"TP_total\": TP\n",
        "    })\n",
        "\n",
        "    return CVResult(folds=folds, summary=summary)\n"
      ],
      "metadata": {
        "id": "Etiz72qBToDa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Importancia / pesos / p-values\n",
        "# =========================================================\n",
        "def get_feature_names(preprocess: ColumnTransformer) -> np.ndarray:\n",
        "    return preprocess.get_feature_names_out()\n",
        "\n",
        "def linear_weights_from_fitted_pipeline(fitted_pipe: Pipeline) -> pd.DataFrame:\n",
        "    preprocess = fitted_pipe.named_steps[\"preprocess\"]\n",
        "    model = fitted_pipe.named_steps[\"model\"]\n",
        "    names = get_feature_names(preprocess)\n",
        "    coefs = model.coef_.ravel()\n",
        "    df = pd.DataFrame({\"feature\": names, \"coef\": coefs})\n",
        "    df[\"abs_coef\"] = df[\"coef\"].abs()\n",
        "    df = df.sort_values(\"abs_coef\", ascending=False).reset_index(drop=True)\n",
        "    # Odds ratio interpretable si no hay estandarización, pero aún útil como ranking\n",
        "    df[\"odds_ratio\"] = np.exp(df[\"coef\"])\n",
        "    return df\n",
        "\n",
        "def tree_native_importance_from_fitted_pipeline(fitted_pipe: Pipeline) -> Optional[pd.DataFrame]:\n",
        "    preprocess = fitted_pipe.named_steps[\"preprocess\"]\n",
        "    model = fitted_pipe.named_steps[\"model\"]\n",
        "    if not hasattr(model, \"feature_importances_\"):\n",
        "        return None\n",
        "    names = get_feature_names(preprocess)\n",
        "    imp = model.feature_importances_\n",
        "    df = pd.DataFrame({\"feature\": names, \"importance\": imp})\n",
        "    df[\"abs_importance\"] = df[\"importance\"].abs()\n",
        "    df = df.sort_values(\"abs_importance\", ascending=False).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def permutation_importance_from_fitted_pipeline(\n",
        "    fitted_pipe: Pipeline,\n",
        "    X_val: pd.DataFrame,\n",
        "    y_val: pd.Series,\n",
        "    n_repeats: int = 50,\n",
        "    random_state: int = 42,\n",
        "    scoring: str = \"roc_auc\"\n",
        ") -> pd.DataFrame:\n",
        "    # Use original feature names for permutation importance, as it perturbs original columns\n",
        "    names = X_val.columns.tolist()\n",
        "\n",
        "    r = permutation_importance(\n",
        "        fitted_pipe, X_val, y_val,\n",
        "        scoring=scoring,\n",
        "        n_repeats=n_repeats,\n",
        "        random_state=random_state,\n",
        "        n_jobs=1\n",
        "    )\n",
        "    df = pd.DataFrame({\n",
        "        \"feature\": names,\n",
        "        \"perm_importance_mean\": r.importances_mean,\n",
        "        \"perm_importance_std\": r.importances_std\n",
        "    }).sort_values(\"perm_importance_mean\", ascending=False).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def logistic_pvalues_statsmodels(\n",
        "    preprocess: ColumnTransformer,\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.Series,\n",
        "    fit_intercept: bool = True,\n",
        ") -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    P-values para logística usando statsmodels.\n",
        "    Nota: esto es para logística NO regularizada (o regularización no estándar).\n",
        "    En L1 (LASSO) no hay p-values clásicos directos.\n",
        "    \"\"\"\n",
        "    if not HAS_SM:\n",
        "        return None\n",
        "\n",
        "    # Clone and fit the preprocessor to ensure correct feature names after transform\n",
        "    fitted_preprocess = clone(preprocess)\n",
        "    Xt = fitted_preprocess.fit_transform(X, y)\n",
        "    if hasattr(Xt, \"toarray\"):\n",
        "        Xt = Xt.toarray()\n",
        "\n",
        "    # Get feature names from the fitted preprocessor\n",
        "    feature_names_after_transform = list(fitted_preprocess.get_feature_names_out())\n",
        "\n",
        "    # --- Check for and remove constant features (zero variance) ---\n",
        "    std_devs = np.std(Xt, axis=0)\n",
        "    constant_features_indices = np.where(std_devs < 1e-9)[0] # Using a small threshold for floating point precision\n",
        "\n",
        "    if len(constant_features_indices) > 0:\n",
        "        removed_features_names = [feature_names_after_transform[i] for i in constant_features_indices]\n",
        "        print(f\"Advertencia: Se encontraron y eliminaron {len(removed_features_names)} características constantes o casi constantes (STD < 1e-9) antes de calcular p-values con Statsmodels: {', '.join(removed_features_names)}\")\n",
        "        Xt = np.delete(Xt, constant_features_indices, axis=1)\n",
        "        # Update feature names list to reflect removed columns\n",
        "        remaining_feature_names = [name for i, name in enumerate(feature_names_after_transform) if i not in constant_features_indices]\n",
        "    else:\n",
        "        remaining_feature_names = feature_names_after_transform\n",
        "\n",
        "    if fit_intercept:\n",
        "        try:\n",
        "            Xt = sm.add_constant(Xt, has_constant=\"add\")\n",
        "            feat = [\"const\"] + remaining_feature_names\n",
        "        except ValueError as e:\n",
        "            print(f\"Advertencia: No se pudo añadir una constante al modelo Statsmodels: {e}. Retornando None para p-values.\")\n",
        "            return None\n",
        "    else:\n",
        "        feat = remaining_feature_names\n",
        "\n",
        "    try:\n",
        "        # Check if Xt is empty after removing features\n",
        "        if Xt.shape[1] == 0:\n",
        "            print(\"Advertencia: No quedan características después de la eliminación de constantes. No se puede ajustar el modelo Statsmodels. Retornando None para p-values.\")\n",
        "            return None\n",
        "\n",
        "        model = sm.Logit(y.values, Xt)\n",
        "        res = model.fit(disp=False)\n",
        "    except np.linalg.LinAlgError as e:\n",
        "        print(f\"Advertencia: LinAlgError al calcular p-values con Statsmodels. La matriz de diseño es singular, lo que indica problemas de multicolinealidad. Detalles: {e}. Retornando None para p-values.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Advertencia: Error inesperado al calcular p-values con Statsmodels: {e}. Retornando None para p-values.\")\n",
        "        return None\n",
        "\n",
        "    # Ensure feat matches the number of parameters in res.params\n",
        "    if len(feat) != len(res.params):\n",
        "        print(f\"Advertencia: El número de nombres de características ({len(feat)}) no coincide con el número de parámetros del modelo ({len(res.params)}). Esto puede indicar un problema. Se intenta emparejar los nombres de forma heurística, pero podría ser incorrecto.\")\n",
        "        # Fallback to generic names if mismatch, to prevent further errors\n",
        "        feat = [f\"param_{i}\" for i in range(len(res.params))]\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"feature\": feat,\n",
        "        \"coef\": res.params,\n",
        "        \"std_err\": res.bse,\n",
        "        \"z\": res.tvalues,\n",
        "        \"p_value\": res.pvalues\n",
        "    })\n",
        "    out[\"abs_coef\"] = np.abs(out[\"coef\"])\n",
        "    out = out.sort_values(\"p_value\", ascending=True).reset_index(drop=True)\n",
        "    return out\n",
        "\n",
        "\n",
        "def try_shap_kernel_explainer(\n",
        "    fitted_pipe: Pipeline,\n",
        "    X_background: pd.DataFrame,\n",
        "    X_explain: pd.DataFrame,\n",
        "    nsamples: int = 200\n",
        ") -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    SHAP KernelExplainer (modelo-tabular general), costoso pero con N=122 suele ser viable.\n",
        "    Devuelve importancia media absoluta por feature.\n",
        "    \"\"\"\n",
        "    if not HAS_SHAP:\n",
        "        return None\n",
        "\n",
        "    # SHAP necesita función que devuelva probas\n",
        "    def f(x_np):\n",
        "        x_df = pd.DataFrame(x_np, columns=X_background.columns)\n",
        "        return fitted_pipe.predict_proba(x_df)[:, 1]\n",
        "\n",
        "    # Convertimos a numpy\n",
        "    bg = X_background.values\n",
        "    ex = X_explain.values\n",
        "\n",
        "    explainer = shap.KernelExplainer(f, bg)\n",
        "    shap_values = explainer.shap_values(ex, nsamples=nsamples)  # (n_samples, n_features)\n",
        "\n",
        "    mean_abs = np.mean(np.abs(shap_values), axis=0)\n",
        "    df = pd.DataFrame({\"feature\": X_background.columns, \"mean_abs_shap\": mean_abs})\n",
        "    df = df.sort_values(\"mean_abs_shap\", ascending=False).reset_index(drop=True)\n",
        "    return df"
      ],
      "metadata": {
        "id": "daETMMSb6x57"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Modelos (factory) parametrizables\n",
        "# =========================================================\n",
        "def build_models(config: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    config: dict {model_name: {params...}}\n",
        "    Devuelve instancias sklearn ya configuradas.\n",
        "    \"\"\"\n",
        "    models = {}\n",
        "\n",
        "    # 1) Logistic\n",
        "    if \"logreg\" in config:\n",
        "        p = config[\"logreg\"]\n",
        "        models[\"logreg\"] = LogisticRegression(**p)\n",
        "    if \"logreg_d1a\" in config:\n",
        "        p = config[\"logreg_d1a\"]\n",
        "        models[\"logreg_d1a\"] = LogisticRegression(**p)\n",
        "    if \"logreg_l3\" in config:\n",
        "        p = config[\"logreg_l3\"]\n",
        "        models[\"logreg_l3\"] = LogisticRegression(**p)\n",
        "\n",
        "    # 1.2) Logistic (baseline / L2 / L1)\n",
        "    if \"logreg_l4\" in config:\n",
        "        p = config[\"logreg_l4\"]\n",
        "        models[\"logreg_l4\"] = LogisticRegression(**p)\n",
        "\n",
        "    # 2) Logistic L1 (si quieres explícito)\n",
        "    if \"logreg_l1\" in config:\n",
        "        p = config[\"logreg_l1\"]\n",
        "        models[\"logreg_l1\"] = LogisticRegression(**p)\n",
        "\n",
        "    # 3) Random Forest\n",
        "    if \"rf\" in config:\n",
        "        p = config[\"rf\"]\n",
        "        models[\"rf\"] = RandomForestClassifier(**p)\n",
        "\n",
        "    if \"rf_d1a\" in config:\n",
        "        p = config[\"rf_d1a\"]\n",
        "        models[\"rf_d1a\"] = RandomForestClassifier(**p)\n",
        "\n",
        "    if \"rf_d3\" in config:\n",
        "        p = config[\"rf_d3\"]\n",
        "        models[\"rf_d3\"] = RandomForestClassifier(**p)\n",
        "\n",
        "    # 4) XGBoost\n",
        "    if \"xgb\" in config:\n",
        "        if not HAS_XGB:\n",
        "            raise RuntimeError(\"xgboost no está instalado. Instala con: pip install xgboost\")\n",
        "        p = config[\"xgb\"]\n",
        "        models[\"xgb\"] = XGBClassifier(**p)\n",
        "\n",
        "    if \"xgb_d1a\" in config:\n",
        "        if not HAS_XGB:\n",
        "            raise RuntimeError(\"xgboost no está instalado. Instala con: pip install xgboost\")\n",
        "        p = config[\"xgb_d1a\"]\n",
        "        models[\"xgb_d1a\"] = XGBClassifier(**p)\n",
        "\n",
        "    if \"xgb_d2\" in config:\n",
        "        if not HAS_XGB:\n",
        "            raise RuntimeError(\"xgboost no está instalado. Instala con: pip install xgboost\")\n",
        "        p = config[\"xgb_d2\"]\n",
        "        models[\"xgb_d2\"] = XGBClassifier(**p)\n",
        "\n",
        "    if \"xgb_d3\" in config:\n",
        "        if not HAS_XGB:\n",
        "            raise RuntimeError(\"xgboost no está instalado. Instala con: pip install xgboost\")\n",
        "        p = config[\"xgb_d3\"]\n",
        "        models[\"xgb_d3\"] = XGBClassifier(**p)\n",
        "\n",
        "    # 5) SVM (probability=True para AUC)\n",
        "    if \"svm_rbf\" in config:\n",
        "        p = config[\"svm_rbf\"]\n",
        "        models[\"svm_rbf\"] = SVC(**p)\n",
        "\n",
        "    # 6) MLP\n",
        "    if \"mlp\" in config:\n",
        "        p = config[\"mlp\"]\n",
        "        models[\"mlp\"] = MLPClassifier(**p)\n",
        "\n",
        "    if \"mlp_d2\" in config:\n",
        "        p = config[\"mlp_d2\"]\n",
        "        models[\"mlp_d2\"] = MLPClassifier(**p)\n",
        "\n",
        "    if \"mlp_d3a\" in config:\n",
        "        p = config[\"mlp_d2\"]\n",
        "        models[\"mlp_d3a\"] = MLPClassifier(**p)\n",
        "\n",
        "    # 7) Bootstrap Decision Forest (bagging de árboles)\n",
        "    if \"bootstrap_df\" in config:\n",
        "        p = config[\"bootstrap_df\"]\n",
        "        base_tree_params = p.pop(\"base_tree_params\", {})\n",
        "        base_tree = DecisionTreeClassifier(**base_tree_params)\n",
        "        models[\"bootstrap_df\"] = BaggingClassifier(estimator=base_tree, **p)\n",
        "\n",
        "    return models\n"
      ],
      "metadata": {
        "id": "l5gef4R_64Ge"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Ejecución por dataset + reporte\n",
        "# =========================================================\n",
        "@dataclass\n",
        "class DatasetSpec:\n",
        "    name: str\n",
        "    path: str\n",
        "    scale_numeric: bool\n",
        "    models_to_run: List[str]\n",
        "\n",
        "def run_experiment(\n",
        "    spec: DatasetSpec,\n",
        "    model_config: Dict[str, Dict[str, Any]],\n",
        "    n_splits: int = 5,\n",
        "    random_state: int = 42,\n",
        "    threshold: float = 0.5,\n",
        "    permutation_repeats: int = 50,\n",
        "    top_k: int = 20,\n",
        ") -> Dict[str, Any]:\n",
        "    X, y = load_dataset_tsv(spec.path)\n",
        "\n",
        "    preprocess = make_preprocessor(\n",
        "        X,\n",
        "        scale_numeric=spec.scale_numeric\n",
        "    )\n",
        "\n",
        "    models = build_models(model_config)\n",
        "\n",
        "    results = {\n",
        "        \"dataset\": spec.name,\n",
        "        \"n\": int(len(y)),\n",
        "        \"pos\": int((y == 1).sum()),\n",
        "        \"neg\": int((y == 0).sum()),\n",
        "        \"models\": {}\n",
        "    }\n",
        "\n",
        "    # CV + importancias\n",
        "    for mname in spec.models_to_run:\n",
        "        print()\n",
        "        print(\"=======================================================================================\")\n",
        "        print (f\"     Conjunto de datos[{spec.name}] - Modelo de a ejecutar [{mname}]\")\n",
        "        print(\"=======================================================================================\")\n",
        "        print()\n",
        "        if mname not in models:\n",
        "            raise ValueError(f\"Modelo '{mname}' no está en model_config.\")\n",
        "\n",
        "        base_model = models[mname]\n",
        "        cv_res = evaluate_with_cv(\n",
        "            base_model, X, y, preprocess,\n",
        "            n_splits=n_splits,\n",
        "            random_state=random_state,\n",
        "            threshold=threshold\n",
        "        )\n",
        "\n",
        "        # Ajuste final en TODO el dataset para extraer pesos / importancias\n",
        "        fitted_pipe = Pipeline([(\"preprocess\", preprocess), (\"model\", clone(base_model))])\n",
        "        fitted_pipe.fit(X, y)\n",
        "\n",
        "        # Importancias / pesos:\n",
        "        weights = None\n",
        "        native_imp = None\n",
        "        perm_imp = None\n",
        "        pvals = None\n",
        "        shap_imp_raw = None\n",
        "\n",
        "        # lineales\n",
        "        if hasattr(fitted_pipe.named_steps[\"model\"], \"coef_\"):\n",
        "            weights = linear_weights_from_fitted_pipeline(fitted_pipe).head(top_k)\n",
        "\n",
        "            # p-values solo si (a) statsmodels disponible y (b) logística NO regularizada (o la tratas como tal)\n",
        "            # Si el usuario define penalty=\"none\" o penalty=\"l2\", y quiere p-values, intentamos.\n",
        "            if isinstance(fitted_pipe.named_steps[\"model\"], LogisticRegression):\n",
        "                pen = getattr(fitted_pipe.named_steps[\"model\"], \"penalty\", None)\n",
        "                if HAS_SM and (pen in (None, \"none\") or pen == \"l2\"):\n",
        "                    # p-values con statsmodels (no es “el mismo” solver/regularización exacta, pero es estándar para inferencia)\n",
        "                    fit_intercept = getattr(fitted_pipe.named_steps[\"model\"], \"fit_intercept\", True)\n",
        "                    pvals = logistic_pvalues_statsmodels(preprocess, X, y, fit_intercept=fit_intercept)\n",
        "                    if pvals is not None:\n",
        "                        pvals = pvals.head(top_k)\n",
        "\n",
        "        # árboles / ensembles con feature_importances_\n",
        "        native_imp = tree_native_importance_from_fitted_pipeline(fitted_pipe)\n",
        "        if native_imp is not None:\n",
        "            native_imp = native_imp.head(top_k)\n",
        "\n",
        "        # permutation importance (robusto para cualquier modelo)\n",
        "        # Usamos un pequeño “pseudo-val” (todo el dataset) para ranking general.\n",
        "        perm_imp = permutation_importance_from_fitted_pipeline(\n",
        "            fitted_pipe, X, y,\n",
        "            n_repeats=permutation_repeats,\n",
        "            random_state=random_state\n",
        "        ).head(top_k)\n",
        "\n",
        "        # SHAP KernelExplainer opcional (coste; útil en NN/SVM y modelos sin importancias interpretables)\n",
        "        # Nota: esto da SHAP sobre columnas ORIGINALES de X (antes de onehot), es intencional para interpretabilidad “humana”.\n",
        "        if HAS_SHAP and mname in (\"mlp\", \"mlp_d2\", \"mlp_d3a\", \"svm_rbf\"):\n",
        "            # background pequeño\n",
        "            bg = X.sample(min(30, len(X)), random_state=random_state)\n",
        "            ex = X.sample(min(30, len(X)), random_state=random_state)\n",
        "            try:\n",
        "                shap_imp_raw = try_shap_kernel_explainer(fitted_pipe, bg, ex, nsamples=200)\n",
        "                if shap_imp_raw is not None:\n",
        "                    shap_imp_raw = shap_imp_raw.head(top_k)\n",
        "            except Exception:\n",
        "                shap_imp_raw = None\n",
        "\n",
        "        results[\"models\"][mname] = {\n",
        "            \"cv_folds\": cv_res.folds,\n",
        "            \"cv_summary\": cv_res.summary,\n",
        "            \"top_weights_or_coefs\": weights,\n",
        "            \"top_pvalues\": pvals,\n",
        "            \"top_native_importance\": native_imp,\n",
        "            \"top_permutation_importance\": perm_imp,\n",
        "            \"top_shap_kernel_importance_original_X\": shap_imp_raw,\n",
        "        }\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "HIL1L6BD7Pqm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# CONFIGURACIÓN PARA TUS 3 DATASETS\n",
        "# =========================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # ---- Paths de tus 3 conjuntos ----\n",
        "    DATASETS = [\n",
        "        DatasetSpec(\n",
        "            name=\"D1_clinico_demografico\",\n",
        "            path= source_path + dataset_covariables,\n",
        "            scale_numeric=True,\n",
        "            models_to_run=[\"logreg\", \"rf\", \"xgb\",\"mlp\"]\n",
        "        ),\n",
        "        DatasetSpec(\n",
        "            name=\"D1_clinico_demografico_analisis\",\n",
        "            path= source_path + dataset_covariables_analysis,\n",
        "            scale_numeric=True,\n",
        "            models_to_run= [\"logreg_d1a\", \"rf_d1a\", \"xgb_d1a\", \"mlp_d2\"]\n",
        "        ),\n",
        "        DatasetSpec(\n",
        "           name=\"D2_prs_34modelos\",\n",
        "           path=source_path + dataset_modelos,\n",
        "           scale_numeric=True,\n",
        "           models_to_run= [\"logreg_l1\", \"svm_rbf\", \"xgb_d2\", \"mlp_d2\"]\n",
        "       ),\n",
        "       DatasetSpec(\n",
        "           name=\"D3_combinado_clinico_prs\",\n",
        "           path=source_path + dataset_cov_mod,\n",
        "           scale_numeric=True,\n",
        "           models_to_run=[\"logreg_l3\", \"rf_d3\", \"xgb_d3\", \"mlp_d2\"]\n",
        "       ),\n",
        "       DatasetSpec(\n",
        "           name=\"D3_combinado_clinico_prs_analisis\",\n",
        "           path=source_path + dataset_cov_mod_analysis,\n",
        "           scale_numeric=True,\n",
        "           models_to_run=[\"logreg_l4\", \"rf\", \"xgb\", \"mlp_d3a\"]\n",
        "       ),\n",
        "    ]\n",
        "\n",
        "    # ---- Modelos parametrizables (ajusta lo que quieras) ----\n",
        "    # Nota: SVC debe llevar probability=True para AUC.\n",
        "    MODEL_CONFIG = {\n",
        "        # Logística baseline (sin regularización “fuerte”)\n",
        "        \"logreg\": dict(                 # clinico-demográfico reducido\n",
        "            penalty=\"l2\",\n",
        "            C=0.01,                     # [0.01, 0.03, 0.1, 0.3, 1, 3, 10]   # Default 1.0\n",
        "            solver=\"liblinear\",\n",
        "            max_iter=5000,\n",
        "            fit_intercept=True,       # [True, False]\n",
        "            random_state=42\n",
        "        ),\n",
        "        \"logreg_d1a\": dict(                 # clinico-demográfico bibliografía\n",
        "            penalty=\"elasticnet\",\n",
        "            C=0.3,                     # [0.01, 0.03, 0.1, 0.3, 1, 3, 10]   # Default 1.0\n",
        "            l1_ratio = 0.5,\n",
        "            solver=\"saga\",\n",
        "            max_iter=5000,\n",
        "            fit_intercept=True,       # [True, False]\n",
        "            random_state=42\n",
        "        ),\n",
        "        # Logística L1 (selección automática) – muy útil en PRS\n",
        "        \"logreg_l1\": dict(            # prs\n",
        "            penalty=\"l1\",\n",
        "            C=1,\n",
        "            solver=\"saga\",\n",
        "            max_iter=5000,\n",
        "            fit_intercept=True,\n",
        "            tol=1e-4,\n",
        "            random_state=42\n",
        "        ),\n",
        "        # Logística baseline (sin regularización “fuerte”)\n",
        "        \"logreg_l3\": dict(\n",
        "            penalty=\"elasticnet\",       # combinado clinico + prs\n",
        "            l1_ratio = 0.1,\n",
        "            C=0.01,                     # [0.01, 0.03, 0.1, 0.3, 1, 3, 10]   # Default 1.0\n",
        "            solver=\"saga\",\n",
        "            max_iter=5000,\n",
        "            fit_intercept=True,       # [True, False]\n",
        "            random_state=42\n",
        "        ),\n",
        "        # Logística baseline (sin regularización “fuerte”)\n",
        "        \"logreg_l4\": dict(\n",
        "            penalty=\"elasticnet\",     # combinado clinico + prs  bibliografía\n",
        "            l1_ratio = 0.3,\n",
        "            C=1,                     # [0.01, 0.03, 0.1, 0.3, 1, 3, 10]   # Default 1.0\n",
        "            solver=\"saga\",\n",
        "            max_iter=5000,\n",
        "            fit_intercept=True,       # [True, False]\n",
        "            random_state=42\n",
        "        ),\n",
        "\n",
        "        \"rf\": dict(                 # Clinico demográfico reducido\n",
        "            n_estimators=300,\n",
        "            max_depth=5,\n",
        "            min_samples_leaf=5,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "        \"rf_d1a\": dict(                  # clinico-demográfico bibliografía\n",
        "            n_estimators=800,\n",
        "            max_depth=3,\n",
        "            min_samples_leaf=2,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "        \"rf_d3\": dict(                  # clinico-demográfico + modelos\n",
        "            n_estimators=500,\n",
        "            max_depth=4,\n",
        "            min_samples_leaf=5,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "\n",
        "        \"xgb\": dict(              # clinico-demográfico reducido\n",
        "            n_estimators=500,     #[200, 500, 1200]\n",
        "            learning_rate=0.1,     #[0.01, 0.05, 0.1]\n",
        "            max_depth=4,            # [2, 3, 4]\n",
        "            subsample=0.8,          #  [0.6, 0.8, 1.0]\n",
        "            colsample_bytree=0.8,     #[0.5, 0.8, 1.0]\n",
        "            reg_lambda=10,           # [1, 5, 10, 20\n",
        "            reg_alpha = 1,       #[0, 0.5, 1, 3]\n",
        "            min_child_weight=10,          # [1, 5, 10, 20]\n",
        "            objective=\"binary:logistic\",\n",
        "            eval_metric=\"auc\",\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            tree_method=\"hist\",\n",
        "            verbosity=0\n",
        "        ),\n",
        "        \"xgb_d1a\": dict(            # clinico-demográfico bibliografía\n",
        "            n_estimators=800,     #[200, 500, 1200]\n",
        "            learning_rate=0.1,     #[0.01, 0.05, 0.1]\n",
        "            max_depth=4,            # [2, 3, 4]\n",
        "            subsample=0.9,          #  [0.6, 0.8, 1.0]\n",
        "            colsample_bytree=0.9,     #[0.5, 0.8, 1.0]\n",
        "            reg_lambda=10,           # [1, 5, 10, 20\n",
        "            reg_alpha = 0.5,       #[0, 0.5, 1, 3]\n",
        "            min_child_weight=10,          # [1, 5, 10, 20]\n",
        "            objective=\"binary:logistic\",\n",
        "            eval_metric=\"auc\",\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            tree_method=\"hist\",\n",
        "            verbosity=0\n",
        "        ),\n",
        "\n",
        "        \"xgb_d2\": dict(           # prs\n",
        "            n_estimators=500,     #[200, 500, 1200]\n",
        "            learning_rate=0.1,     #[0.01, 0.05, 0.1]\n",
        "            max_depth=4,            # [2, 3, 4]\n",
        "            subsample=0.8,          #  [0.6, 0.8, 1.0]\n",
        "            colsample_bytree=0.8,     #[0.5, 0.8, 1.0]\n",
        "            reg_lambda=20,           # [1, 5, 10, 20\n",
        "            reg_alpha = 3,       #[0, 0.5, 1, 3]\n",
        "            min_child_weight=10,          # [1, 5, 10, 20]\n",
        "            objective=\"binary:logistic\",\n",
        "            eval_metric=\"auc\",\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            tree_method=\"hist\",\n",
        "            verbosity=0\n",
        "        ),\n",
        "\n",
        "        \"xgb_d3\": dict(            # clinico-demográfico bibliografía\n",
        "            n_estimators=300,     #[200, 500, 1200]\n",
        "            learning_rate=0.05,     #[0.01, 0.05, 0.1]\n",
        "            max_depth=3,            # [2, 3, 4]\n",
        "            subsample=0.8,          #  [0.6, 0.8, 1.0]\n",
        "            colsample_bytree=0.9,     #[0.5, 0.8, 1.0]\n",
        "            reg_lambda=1,           # [1, 5, 10, 20\n",
        "            reg_alpha = 1,       #[0, 0.5, 1, 3]\n",
        "            min_child_weight=10,          # [1, 5, 10, 20]\n",
        "            objective=\"binary:logistic\",\n",
        "            eval_metric=\"auc\",\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            tree_method=\"hist\",\n",
        "            verbosity=0\n",
        "        ),\n",
        "\n",
        "        \"svm_rbf\": dict(        # prs\n",
        "            C=1,                # C: [0.1, 1, 10, 100]\n",
        "            gamma=0.01,        # [\"scale\", 0.01, 0.1, 1]\n",
        "            kernel=\"rbf\",\n",
        "            probability=True,\n",
        "            random_state=42\n",
        "        ),\n",
        "\n",
        "        \"mlp\": dict(            # clinico-demográfico reducido + bibliografia\n",
        "            hidden_layer_sizes=(64,32),      # [(16,), (32,), (32, 16), (64, 32)],\n",
        "            activation=\"relu\",\n",
        "            solver=\"adam\",\n",
        "            alpha=1e-3,             # [1e-5, 1e-4, 1e-3, 1e-2]\n",
        "            batch_size=32,          # [8, 16, 32]\n",
        "            learning_rate=\"adaptive\",\n",
        "            learning_rate_init=1e-3,    # [1e-4, 1e-3, 5e-3]\n",
        "            max_iter=2000,\n",
        "            early_stopping=True,\n",
        "            validation_fraction=0.2,\n",
        "            n_iter_no_change=30,\n",
        "            random_state=42\n",
        "        ),\n",
        "\n",
        "        \"mlp_d2\": dict(           # prs\n",
        "            hidden_layer_sizes=(64,32),      # [(16,), (32,), (32, 16), (64, 32)],\n",
        "            activation=\"relu\",\n",
        "            solver=\"adam\",\n",
        "            alpha=1e-4,             # [1e-5, 1e-4, 1e-3, 1e-2]\n",
        "            batch_size=32,          # [8, 16, 32]\n",
        "            learning_rate=\"adaptive\",\n",
        "            learning_rate_init=1e-3,    # [1e-4, 1e-3, 5e-3]\n",
        "            max_iter=2000,\n",
        "            early_stopping=True,\n",
        "            validation_fraction=0.2,\n",
        "            n_iter_no_change=30,\n",
        "            random_state=42\n",
        "        ),\n",
        "\n",
        "        \"mlp_d3a\": dict(           # prs\n",
        "            hidden_layer_sizes=(32,16),      # [(16,), (32,), (32, 16), (64, 32)],\n",
        "            activation=\"relu\",\n",
        "            solver=\"adam\",\n",
        "            alpha=1e-5,             # [1e-5, 1e-4, 1e-3, 1e-2]\n",
        "            batch_size=16,          # [8, 16, 32]\n",
        "            learning_rate=\"adaptive\",\n",
        "            learning_rate_init=1e-4,    # [1e-4, 1e-3, 5e-3]\n",
        "            max_iter=2000,\n",
        "            early_stopping=True,\n",
        "            validation_fraction=0.2,\n",
        "            n_iter_no_change=30,\n",
        "            random_state=42\n",
        "        ),\n",
        "    }\n",
        "\n",
        "\n",
        "    # ---- Ejecución ----\n",
        "    all_results = []\n",
        "    for ds in DATASETS:\n",
        "        res = run_experiment(\n",
        "            spec=ds,\n",
        "            model_config=MODEL_CONFIG,\n",
        "            n_splits=5,\n",
        "            random_state=42,\n",
        "            threshold=0.5,\n",
        "            permutation_repeats=50,\n",
        "            top_k=20\n",
        "        )\n",
        "        all_results.append(res)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*90)\n",
        "        print(f\"DATASET: {res['dataset']} | N={res['n']} | pos={res['pos']} | neg={res['neg']}\")\n",
        "        print(\"=\"*90)\n",
        "\n",
        "        for mname, out in res[\"models\"].items():\n",
        "            print(f\"\\n--- Modelo: {mname} ---\")\n",
        "            print(\"\\nCV summary:\")\n",
        "            print(out[\"cv_summary\"].round(4).to_string())\n",
        "\n",
        "            print(\"\\nCV folds:\")\n",
        "            print(out[\"cv_folds\"].round(4).to_string(index=False))\n",
        "\n",
        "            if out[\"top_weights_or_coefs\"] is not None:\n",
        "                print(\"\\nTop coeficientes (peso/odds-ratio):\")\n",
        "                print(out[\"top_weights_or_coefs\"].round(6).to_string(index=False))\n",
        "\n",
        "            if out[\"top_pvalues\"] is not None:\n",
        "                print(\"\\nTop p-values (statsmodels; válido sobre todo para logística no-L1):\")\n",
        "                print(out[\"top_pvalues\"].round(6).to_string(index=False))\n",
        "\n",
        "            if out[\"top_native_importance\"] is not None:\n",
        "                print(\"\\nTop importancia nativa (feature_importances_):\")\n",
        "                print(out[\"top_native_importance\"].round(6).to_string(index=False))\n",
        "\n",
        "            if out[\"top_permutation_importance\"] is not None:\n",
        "                print(\"\\nTop permutation importance (AUC):\")\n",
        "                print(out[\"top_permutation_importance\"].round(6).to_string(index=False))\n",
        "\n",
        "            if out[\"top_shap_kernel_importance_original_X\"] is not None:\n",
        "                print(\"\\nTop SHAP (KernelExplainer) sobre variables originales (útil en MLP/SVM):\")\n",
        "                print(out[\"top_shap_kernel_importance_original_X\"].round(6).to_string(index=False))\n",
        "\n",
        "    # (Opcional) Guardar todo a disco de forma simple:\n",
        "    # - summaries a un TSV por dataset/modelo\n",
        "    mod_res_path = TFM_PATH + \"/modelos_results/\"\n",
        "    for res in all_results:\n",
        "        for mname, out in res[\"models\"].items():\n",
        "            out[\"cv_folds\"].to_csv(mod_res_path+ f\"{res['dataset']}__{mname}__cv_folds.tsv\", sep=\"\\t\", index=False)\n",
        "            out[\"cv_summary\"].to_frame(\"value\").to_csv(mod_res_path+ f\"{res['dataset']}__{mname}__cv_summary.tsv\", sep=\"\\t\")\n",
        "            if out[\"top_weights_or_coefs\"] is not None:\n",
        "                out[\"top_weights_or_coefs\"].to_csv(mod_res_path + f\"{res['dataset']}__{mname}__top_coefs.tsv\", sep=\"\\t\", index=False)\n",
        "            if out[\"top_pvalues\"] is not None:\n",
        "                out[\"top_pvalues\"].to_csv(mod_res_path + f\"{res['dataset']}__{mname}__top_pvalues.tsv\", sep=\"\\t\", index=False)\n",
        "            if out[\"top_native_importance\"] is not None:\n",
        "                out[\"top_native_importance\"].to_csv(mod_res_path + f\"{res['dataset']}__{mname}__top_native_importance.tsv\", sep=\"\\t\", index=False)\n",
        "            if out[\"top_permutation_importance\"] is not None:\n",
        "                out[\"top_permutation_importance\"].to_csv(mod_res_path + f\"{res['dataset']}__{mname}__top_perm_importance.tsv\", sep=\"\\t\", index=False)\n",
        "            if out[\"top_shap_kernel_importance_original_X\"] is not None:\n",
        "                out[\"top_shap_kernel_importance_original_X\"].to_csv(mod_res_path + f\"{res['dataset']}__{mname}__top_shap_kernel.tsv\", sep=\"\\t\", index=False)"
      ],
      "metadata": {
        "id": "1b76_Ksi7VSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c20ed8ee"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "summary_data = []\n",
        "\n",
        "for res in all_results:\n",
        "    dataset_name = res['dataset']\n",
        "    for mname, model_results in res['models'].items():\n",
        "        summary_series = model_results['cv_summary']\n",
        "        summary_dict = summary_series.to_dict()\n",
        "        summary_dict['Dataset_Model'] = f\"{dataset_name} - {mname}\"\n",
        "        summary_data.append(summary_dict)\n",
        "\n",
        "# Create a DataFrame from the collected summaries\n",
        "full_cv_summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "# Set 'Dataset_Model' as the index and sort it\n",
        "full_cv_summary_df = full_cv_summary_df.set_index('Dataset_Model')\n",
        "full_cv_summary_df = full_cv_summary_df.round(4)\n",
        "\n",
        "# Display the DataFrame\n",
        "display(full_cv_summary_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FPUPgVCgaOrV"
      }
    }
  ]
}