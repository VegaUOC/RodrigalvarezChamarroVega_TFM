{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FyHUPeXI_q7"
      },
      "outputs": [],
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Cambiar el directorio de trabajo\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Define the TFM directory path\n",
        "TFM_PATH = '/content/drive/My Drive/TFM'\n",
        "\n",
        "# Change the current working directory to the TFM directory\n",
        "os.chdir(TFM_PATH)\n",
        "print(f\"Current working directory changed to: {os.getcwd()}\")\n",
        "\n",
        "# Add the TFM directory to the Python system path\n",
        "if TFM_PATH not in sys.path:\n",
        "    sys.path.append(TFM_PATH)\n",
        "    print(f\"'{TFM_PATH}' added to Python system path.\")\n",
        "else:\n",
        "    print(f\"'{TFM_PATH}' is already in Python system path.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga de librerias\n",
        "import extractData\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "n2KNbObyp16E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables\n",
        "data_path = TFM_PATH + '/data/'\n",
        "\n",
        "participants_file = \"selected_participants.csv\""
      ],
      "metadata": {
        "id": "UbKvjojTp6K7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "participants_df = extractData.load_csv_to_dataframe(data_path,participants_file)\n"
      ],
      "metadata": {
        "id": "rgnyRA6vp8VD"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Análisis de datos\n",
        "\n",
        "# Mostrar como se distribuyen los datos por grupos de edad y sexo\n",
        "def summary_histogram_base(df_data):\n",
        "  age_order = ['21-29 years', '30-39 years', '40-49 years', '50-59 years', '60-69 years', '70-79 years', '80-89 years', '90-99 years']\n",
        "  sex_order = ['Male', 'Female', 'No response']\n",
        "\n",
        "  df_data['Year of birth'] = pd.Categorical(df_data['Year of birth'], categories=age_order, ordered=True)\n",
        "  df_data['Sex/Gender'] = pd.Categorical(df_data['Sex/Gender'], categories=sex_order, ordered=True)\n",
        "\n",
        "  grouped_data_cases = df_data.groupby(['Year of birth','Sex/Gender']).size().unstack(fill_value=0)\n",
        "  print(grouped_data_cases)\n",
        "\n",
        "  # Genera un histograma\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.countplot(data=df_data, x='Year of birth', hue='Sex/Gender', palette='viridis')\n",
        "  plt.title('Distribución de grupos de edad por género para participantes con hipertensión')\n",
        "  plt.xlabel('Grupo de edad')\n",
        "  plt.ylabel('# participantes')\n",
        "  plt.legend(title='Sexo/Genero')\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "participants_cases_df = participants_df[participants_df['Hypertension'] == 1]\n",
        "summary_histogram_base(participants_cases_df)\n",
        "participants_control_df = participants_df[participants_df['Hypertension'] == 0]\n",
        "summary_histogram_base (participants_control_df)\n"
      ],
      "metadata": {
        "id": "8er7Vda1c7XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a07cf301"
      },
      "source": [
        "# Muestra como se distribuyen los participantes por ancestros\n",
        "\n",
        "def summary_histogram_ancestral_origin(df_data):\n",
        "  selected_columns = [\n",
        "      'Paternal grandmother: Country of origin',\n",
        "      'Paternal grandfather: Country of origin',\n",
        "      'Maternal grandmother: Country of origin',\n",
        "      'Maternal grandfather: Country of origin'\n",
        "  ]\n",
        "\n",
        "  # 1. Crea un nuevo dataframe con los ancestros\n",
        "  ancestral_origins_df =df_data[selected_columns].copy()\n",
        "\n",
        "  # 2. Renombra columnas\n",
        "  ancestral_origins_df.rename(columns={\n",
        "      'Paternal grandmother: Country of origin': 'Paternal grandmother',\n",
        "      'Paternal grandfather: Country of origin': 'Paternal grandfather',\n",
        "      'Maternal grandmother: Country of origin': 'Maternal grandmother',\n",
        "      'Maternal grandfather: Country of origin': 'Maternal grandfather'\n",
        "  }, inplace=True)\n",
        "\n",
        "  # 3. Transforma el dataset para poder dibujar el histograma\n",
        "  ancestral_origins_melted_df = ancestral_origins_df.melt(\n",
        "      var_name='Origin Type',\n",
        "      value_name='Country of Origin'\n",
        "  )\n",
        "\n",
        "  # 4. Drop any rows where 'Country of Origin' is missing (NaN)\n",
        "  #hypertensive_ancestral_origins_melted_df.dropna(subset=['Country of Origin'], inplace=True)\n",
        "  type_order = ['Paternal grandmother', 'Paternal grandfather', 'Maternal grandmother', 'Maternal grandfather']\n",
        "  country_order = ['North America', 'Central America', 'South America',\n",
        "                   'Europe', 'South Asia', 'Western Asia', 'East Asia',\n",
        "                   'Oceania', 'Africa']\n",
        "\n",
        "\n",
        "  ancestral_origins_melted_df['Origin Type'] = pd.Categorical(ancestral_origins_melted_df['Origin Type'], categories=type_order, ordered=True)\n",
        "  ancestral_origins_melted_df['Country of Origin'] = pd.Categorical(ancestral_origins_melted_df['Country of Origin'], categories=country_order, ordered=True)\n",
        "\n",
        "  # Crea el histograma\n",
        "  plt.figure(figsize=(15, 8))\n",
        "  sns.countplot(data=ancestral_origins_melted_df, x='Country of Origin', hue='Origin Type', palette='viridis')\n",
        "  plt.title('Distribución de ancestros por pacientes')\n",
        "  plt.xlabel('País de origen')\n",
        "  plt.ylabel('N9mero de participantes')\n",
        "  plt.legend(title='Ancestría')\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "summary_histogram_ancestral_origin(participants_cases_df)\n",
        "summary_histogram_ancestral_origin(participants_control_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_ethnicity_distribution(df_data, column, title):\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.countplot(data=df_data, x=column, hue='Hypertension_label', palette='viridis')\n",
        "  plt.title(title)\n",
        "  plt.xlabel(column)\n",
        "  plt.ylabel('Número de Participantes')\n",
        "  plt.legend(title='Hipertensión')\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "# Create a temporary column with descriptive labels for Hypertension\n",
        "plot_df = participants_df.copy()\n",
        "plot_df['Hypertension_label'] = plot_df['Hypertension'].map({0: 'Control', 1: 'Casos'})\n",
        "\n",
        "plot_ethnicity_distribution(plot_df, 'Race/ethnicity', 'Distribución por etnia (Casos vs Controles)')"
      ],
      "metadata": {
        "id": "twm1vKg532vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26a89fb2"
      },
      "source": [
        "# Create a temporary column with descriptive labels for Hypertension\n",
        "plot_df = participants_df.copy()\n",
        "plot_df['Hypertension_label'] = plot_df['Hypertension'].map({0: 'Control', 1: 'Casos'})\n",
        "\n",
        "plot_ethnicity_distribution(plot_df, 'Blood_type','Distribución por tipo sanguíneo (Casos vs Controles)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d7ff17b"
      },
      "source": [
        "def plot_frec_histogram(df_data, column, title):\n",
        "  # Sort the 'Weight' column values in ascending order for consistency\n",
        "  sorted_data = df_data[column].sort_values().reset_index(drop=True)\n",
        "\n",
        "  # Create a histogram for the sorted 'Weight' column with 10 bins\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.histplot(sorted_data, kde=True, bins=10, color='lightblue')\n",
        "\n",
        "  # Set the title and labels\n",
        "  plt.title(title)\n",
        "  plt.xlabel(column)\n",
        "  plt.ylabel('Frecuencia')\n",
        "\n",
        "  # Display the plot\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# Call the function for participants_cases_df\n",
        "plot_frec_histogram(participants_cases_df, 'Height_cm', 'Distribución de altura para los casos' )\n",
        "\n",
        "# Call the function for participants_control_df\n",
        "plot_frec_histogram(participants_control_df, 'Height_cm','Distribución de altura para controles')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c63d66d"
      },
      "source": [
        "\n",
        "# Call the function for participants_cases_df\n",
        "plot_frec_histogram(participants_cases_df, 'Weight_kg','Distribución de peso para participantes con hipertensión')\n",
        "\n",
        "# Call the function for participants_control_df\n",
        "plot_frec_histogram(participants_control_df, 'Weight_kg', 'Distribución de peso para participantes de control')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_frequency_table(df_data, column_name, title):\n",
        "  print(f\"\\n{title}:\")\n",
        "  # Get value counts, include NaN values, and sort them by frequency in descending order\n",
        "  frequency_table = df_data[column_name].value_counts(dropna=False).sort_values(ascending=False)\n",
        "  display(frequency_table.to_frame(name='Number of Participants'))\n",
        "\n",
        "# Handedness tables\n",
        "display_frequency_table(participants_cases_df, 'Handedness', 'Frecuencia de Lateralidad para Participantes con Hipertensión')\n",
        "display_frequency_table(participants_control_df, 'Handedness', 'Frecuencia de Lateralidad para Participantes de Control')\n",
        "\n",
        "# Right eye color tables\n",
        "display_frequency_table(participants_cases_df, 'Right_eye_color', 'Frecuencia de Color de Ojo Derecho para Participantes con Hipertensión')\n",
        "display_frequency_table(participants_control_df, 'Right_eye_color', 'Frecuencia de Color de Ojo Derecho para Participantes de Control')\n",
        "\n",
        "# Left eye color tables\n",
        "display_frequency_table(participants_cases_df, 'Left_eye_color', 'Frecuencia de Color de Ojo Izquierdo para Participantes con Hipertensión')\n",
        "display_frequency_table(participants_control_df, 'Left_eye_color', 'Frecuencia de Color de Ojo Izquierdo para Participantes de Control')\n",
        "\n",
        "# Hair color tables\n",
        "display_frequency_table(participants_cases_df, 'Hair_color', 'Frecuencia de Color de Pelo para Participantes con Hipertensión')\n",
        "display_frequency_table(participants_control_df, 'Hair_color', 'Frecuencia de Color de Pelo para Participantes de Control')"
      ],
      "metadata": {
        "id": "NgIMuwsl81Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis Univariante"
      ],
      "metadata": {
        "id": "x4QRbbVbTHYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test de chi2 o Fiscjer para variables binarias\n",
        "\n",
        "def chi2_or_fisher(df, x, y):\n",
        "    \"\"\" Aplica chi2 o Fisher según corresponda \"\"\"\n",
        "    tbl = pd.crosstab(df[y], df[x])\n",
        "    if (tbl.values < 5).sum() > 0:\n",
        "        test = \"Fisher\"\n",
        "        # usar Fisher exact (solo válido 2x2)\n",
        "        if tbl.shape == (2,2):\n",
        "            _, p = stats.fisher_exact(tbl)\n",
        "        else:\n",
        "            p = np.nan  # fisher no generalizado en scipy\n",
        "    else:\n",
        "        test = \"Chi2\"\n",
        "        _, p, _, _ = stats.chi2_contingency(tbl)\n",
        "    return test, p\n",
        "\n",
        "# Regresión logística univariante\n",
        "def logistic_univariate(df, y, x):\n",
        "    \"\"\" Ajusta una regresión logística univariante y devuelve OR, IC y p-value \"\"\"\n",
        "    try:\n",
        "        # Elimina valores nulos\n",
        "        df2 = df[[y, x]].dropna()\n",
        "        formula = f\"{y} ~ {x}\"\n",
        "        # Aplica la regrisión logística\n",
        "        model = smf.logit(formula, df2).fit(disp=False)\n",
        "        coef = model.params[x]\n",
        "        se = model.bse[x]\n",
        "        # Extrae odd-ratio\n",
        "        OR = np.exp(coef)\n",
        "        # Calcula intervalos de confianza\n",
        "        CI_low = np.exp(coef - 1.96*se)\n",
        "        CI_high = np.exp(coef + 1.96*se)\n",
        "        # Extrae el p-valur\n",
        "        p = model.pvalues[x]\n",
        "        return OR, CI_low, CI_high, p\n",
        "    except:\n",
        "        return np.nan, np.nan, np.nan, np.nan\n",
        "\n",
        "# t-test o Mann-Whitney para variables continuas\n",
        "def mannwhitney_or_ttest(df, x, y):\n",
        "    \"\"\" Usa t-test o Mann–Whitney dependiendo de normalidad \"\"\"\n",
        "    # Eliminoa valores nulos\n",
        "    d0 = df[df[y]==0][x].dropna()\n",
        "    d1 = df[df[y]==1][x].dropna()\n",
        "\n",
        "    # prueba simple de normalidad\n",
        "    p_norm0 = stats.shapiro(d0)[1] if len(d0)>=3 else 0\n",
        "    p_norm1 = stats.shapiro(d1)[1] if len(d1)>=3 else 0\n",
        "\n",
        "    if p_norm0>0.05 and p_norm1>0.05:\n",
        "        # t-test\n",
        "        test = \"t-test\"\n",
        "        p = stats.ttest_ind(d0, d1, equal_var=False)[1]\n",
        "    else:\n",
        "        # Mann-Whitney\n",
        "        test = \"Mann-Whitney\"\n",
        "        p = stats.mannwhitneyu(d0, d1, alternative=\"two-sided\")[1]\n",
        "    return test, p\n"
      ],
      "metadata": {
        "id": "vtS8OrseUulo"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nombre de la variable dependiente\n",
        "target = \"Hypertension\"\n",
        "cols_to_ignore = [\"Timestamp_survey\",\"Height\",\"Weight\",\"Left_eye_color\",\"Right_eye_color\",\n",
        "                  \"Hair_color\",\"Handedness\"]\n",
        "excluded_cols = []\n",
        "\n",
        "# Limpieza de filas\n",
        "# Columna Sex/Gender. Eliminar todas las filas que no tienen sexo identificado\n",
        "participants_df = participants_df[participants_df['Sex/Gender'].notna()]\n",
        "participants_df = participants_df[participants_df['Sex/Gender'] != 'No response']\n",
        "\n",
        "# ============================================\n",
        "# Detección de tipo de variable a analizar\n",
        "# ============================================\n",
        "\n",
        "results = []\n",
        "\n",
        "# Recorre una a una todas las variables para identificar aquellas que tiene que analizar\n",
        "for col in participants_df.columns:\n",
        "    # Si es la variable objetivo no la tiene en cuenta\n",
        "    if (col == target or col in cols_to_ignore):\n",
        "        continue\n",
        "\n",
        "    series = participants_df[col]\n",
        "    n_unique = series.nunique(dropna=True)\n",
        "\n",
        "    # Skip IDs\n",
        "    if \"id\" in col.lower() or \"participant\" in col.lower():\n",
        "        continue\n",
        "\n",
        "    # IDENTIFICACIÓN DE TIPO\n",
        "    if n_unique == 1:\n",
        "      excluded_cols.append(col)\n",
        "      continue\n",
        "    elif n_unique == 2:\n",
        "        var_type = \"binary\"\n",
        "    elif series.dtype == \"object\":\n",
        "        var_type = \"categorical\"\n",
        "    elif n_unique <= 10:\n",
        "        var_type = \"ordinal/category\"\n",
        "    else:\n",
        "        var_type = \"continuous\"\n",
        "\n",
        "    # Columna a analizar\n",
        "    print(f\"**** Columna a analizar ****** ----->  {col}\")\n",
        "    # ============================================\n",
        "    # Aplicar el test según el tipo de variable\n",
        "    # ============================================\n",
        "    # Variables binarias\n",
        "    if var_type == \"binary\":\n",
        "        test, p_test = chi2_or_fisher(participants_df, col, target)\n",
        "        OR, CI_low, CI_high, p_log = logistic_univariate(participants_df, target, col)\n",
        "        results.append([col, var_type, test, p_test, OR, CI_low, CI_high, p_log])\n",
        "    # Variables categóricas\n",
        "    elif var_type == \"categorical\":\n",
        "        test = \"Chi2,LRT\"\n",
        "        try:\n",
        "            tbl = pd.crosstab(participants_df[target], participants_df[col])\n",
        "            _, p_test, _, _ = stats.chi2_contingency(tbl)\n",
        "        except:\n",
        "            test = \"Chi2, Error\"\n",
        "            p_test = np.nan\n",
        "\n",
        "        # Divide la columna en tantas columnas como categorias tenga\n",
        "        dummies = pd.get_dummies(participants_df[col], prefix=col, drop_first=True)\n",
        "        df_tmp = pd.concat([participants_df[[target]], dummies], axis=1).dropna()\n",
        "\n",
        "        # LRT global\n",
        "        try:\n",
        "            full_formula = f\"{target} ~ \" + \" + \".join(dummies.columns)\n",
        "            null_formula = f\"{target} ~ 1\"\n",
        "            model_full = smf.logit(full_formula, df_tmp).fit(disp=False)\n",
        "            model_null = smf.logit(null_formula, df_tmp).fit(disp=False)\n",
        "            LRT_p = stats.chi2.sf(2*(model_full.llf - model_null.llf),\n",
        "                                   df_tmp.shape[1]-1)\n",
        "        except:\n",
        "            LRT_p = np.nan\n",
        "\n",
        "        results.append([col, var_type, test, p_test, np.nan, np.nan, np.nan, LRT_p])\n",
        "    elif var_type == \"continuous\":\n",
        "        test, p_test = mannwhitney_or_ttest(participants_df, col, target)\n",
        "        OR, CI_low, CI_high, p_log = logistic_univariate(participants_df, target, col)\n",
        "        results.append([col, var_type, test, p_test, OR, CI_low, CI_high, p_log])\n",
        "\n",
        "    else:  # ordinal / pocos niveles\n",
        "        # tratar como ordinal: codificación automática\n",
        "        test = \"Regresión logística\"\n",
        "        try:\n",
        "            series_ord = pd.factorize(series)[0]\n",
        "            df2 = participants_df.copy()\n",
        "            df2[col] = series_ord\n",
        "            p_test = logistic_univariate(df2, target, col)[3]\n",
        "            OR, CI_low, CI_high, p_log = logistic_univariate(df2, target, col)\n",
        "        except:\n",
        "            test = \"Regresión logística, Error\"\n",
        "            p_test, OR, CI_low, CI_high, p_log = (np.nan,)*5\n",
        "\n",
        "        results.append([col, var_type, test, p_test, OR, CI_low, CI_high, p_log])\n",
        "\n",
        "\n",
        "columns = [\"Variable\", \"Type\", \"test\", \"p_test\", \"OR\", \"CI_low\", \"CI_high\", \"p_logistic\"]\n",
        "results_df = pd.DataFrame(results, columns=columns)\n",
        "\n",
        "# Orden por p_logistic\n",
        "results_df = results_df.sort_values(\"p_logistic\")\n",
        "\n",
        "# Guardar\n",
        "results_df.to_csv(data_path + \"univariate_results.csv\", index=False)\n",
        "\n",
        "print(excluded_cols)\n",
        "results_df\n",
        "\n"
      ],
      "metadata": {
        "id": "Z0wWxj2ATGGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis Multivariante"
      ],
      "metadata": {
        "id": "HSoZ6VQxIyHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable dependiente\n",
        "target = \"Hypertension\"\n",
        "\n",
        "# Variables “forzadas” (clínicas)\n",
        "forced_vars = [\n",
        "    \"Year of birth\",      # categórica ordinal\n",
        "    \"Sex/Gender\"          # categórica binaria (M/F)\n",
        "]\n",
        "\n",
        "candidate_vars = [\n",
        "    \"IMC\",\n",
        "    \"endocrine_conditions\",\n",
        "    \"Circulatory_conditions\",\n",
        "    \"blood_conditions\",\n",
        "    \"High cholesterol (hypercholesterolemia)\",\n",
        "    \"type 2\",\n",
        "    \"Diabetes mellitus\"\n",
        "]\n",
        "\n",
        "all_vars = forced_vars + candidate_vars\n",
        "\n",
        "# Subconjunto de datos con las columnas de interés\n",
        "model_df = participants_df[[target] + all_vars].copy()\n",
        "\n",
        "# Casos completos\n",
        "model_df = model_df.dropna()\n",
        "\n",
        "print(\"N casos completos:\", model_df.shape[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L3smrEdIxcu",
        "outputId": "ebe15da9-85b7-40ba-a2da-192c6fc799fe"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N casos completos: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo logístico multivariante con statsmodels (interpretación detallada)"
      ],
      "metadata": {
        "id": "oibTg0x2KJh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# CODIFICACIÓN DE CATEGÓRICAS\n",
        "# Usamos fórmula de statsmodels que se encarga de las dummies\n",
        "# ====================================\n",
        "\n",
        "# Para evitar problemas con espacios y caracteres, usamos Q(\"col\") en la fórmula\n",
        "formula = (\n",
        "    'Hypertension ~ '\n",
        "    'C(Q(\"Year of birth\")) + '\n",
        "    'C(Q(\"Sex/Gender\")) + '\n",
        "    'IMC + '\n",
        "    'Q(\"endocrine_conditions\") + '\n",
        "    'Q(\"Circulatory_conditions\") + '\n",
        "    'Q(\"blood_conditions\") + '\n",
        "    'Q(\"High cholesterol (hypercholesterolemia)\") + '\n",
        "    'Q(\"type 2\") + '\n",
        "    'Q(\"Diabetes mellitus\")'\n",
        ")\n",
        "\n",
        "logit_model = smf.logit(formula=formula, data=model_df).fit()\n",
        "print(logit_model.summary())\n",
        "\n",
        "# ==========================\n",
        "# OBTENER OR E INTERVALOS\n",
        "# ==========================\n",
        "\n",
        "params = logit_model.params\n",
        "conf = logit_model.conf_int()\n",
        "or_table = pd.DataFrame({\n",
        "    \"coef\": params,\n",
        "    \"OR\": np.exp(params),\n",
        "    \"CI_low\": np.exp(conf[0]),\n",
        "    \"CI_high\": np.exp(conf[1]),\n",
        "    \"p_value\": logit_model.pvalues\n",
        "})\n",
        "print(\"\\nOdds Ratios (modelo multivariante):\\n\")\n",
        "print(or_table)\n"
      ],
      "metadata": {
        "id": "qpn-QFZGKLpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo penalizado (L1, tipo LASSO) con sklearn"
      ],
      "metadata": {
        "id": "FPpuD2inK70d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# ====================================\n",
        "# SEPARAR X, y\n",
        "# ====================================\n",
        "\n",
        "y = model_df[target].values\n",
        "\n",
        "# Definimos tipos de variables (ajusta según tu caso)\n",
        "cat_features = [\"Year of birth\", \"Sex/Gender\"]\n",
        "num_features = [\"IMC\", \"endocrine_conditions\", \"Circulatory_conditions\",\n",
        "                \"blood_conditions\", \"High cholesterol (hypercholesterolemia)\", \"type 2\", \"Diabetes mellitus\"]\n",
        "\n",
        "X = model_df[cat_features + num_features]\n",
        "\n",
        "# ====================================\n",
        "# TRANSFORMADORES: dummies + escalado\n",
        "# ====================================\n",
        "\n",
        "\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(drop=\"first\"), cat_features),\n",
        "        (\"num\", StandardScaler(), num_features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ====================================\n",
        "# MODELO LOGÍSTICO PENALIZADO L1\n",
        "# ====================================\n",
        "\n",
        "log_reg_l1 = LogisticRegression(\n",
        "    penalty=\"l1\",\n",
        "    solver=\"liblinear\",\n",
        "    max_iter=1000\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"model\", log_reg_l1)\n",
        "])\n",
        "\n",
        "# ====================================\n",
        "# VALIDACIÓN CRUZADA (AUC)\n",
        "# ====================================\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "auc_scores = cross_val_score(\n",
        "    pipeline, X, y,\n",
        "    cv=cv,\n",
        "    scoring=\"roc_auc\"\n",
        ")\n",
        "\n",
        "print(\"AUC media (5-fold CV):\", auc_scores.mean())\n",
        "print(\"Desviación estándar AUC:\", auc_scores.std())\n",
        "\n",
        "# ====================================\n",
        "# AJUSTE FINAL EN TODO EL CONJUNTO\n",
        "# ====================================\n",
        "\n",
        "pipeline.fit(X, y)\n",
        "\n",
        "# Puedes extraer coeficientes del modelo final:\n",
        "# Primero transformamos X para ver cuántas columnas hay\n",
        "\n",
        "X_trans = pipeline.named_steps[\"preprocess\"].transform(X)\n",
        "coef = pipeline.named_steps[\"model\"].coef_[0]\n",
        "\n",
        "print(\"Número de features tras one-hot y escalado:\", X_trans.shape[1])\n",
        "print(\"Coeficientes (modelo L1):\")\n",
        "print(coef)"
      ],
      "metadata": {
        "id": "iODnHHgzLDZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4. Métricas adicionales: ROC y calibración (ejemplo sencillo)"
      ],
      "metadata": {
        "id": "KAIVzHrfL0Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Probabilidades predichas (modelo penalizado final)\n",
        "y_pred_proba = pipeline.predict_proba(X)[:, 1]\n",
        "\n",
        "# AUC\n",
        "auc = roc_auc_score(y, y_pred_proba)\n",
        "print(\"AUC (modelo final):\", auc)\n",
        "\n",
        "# Curva ROC\n",
        "fpr, tpr, _ = roc_curve(y, y_pred_proba)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "plt.xlabel(\"1 - Especificidad\")\n",
        "plt.ylabel(\"Sensibilidad\")\n",
        "plt.title(\"Curva ROC - Modelo multivariante\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2Hn_LrB2L2AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis de componentes principales"
      ],
      "metadata": {
        "id": "2odiMn7ZMc5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionamos solo columnas numéricas\n",
        "df_num = participants_df.select_dtypes(include=['int64', 'float64']).copy()\n",
        "\n",
        "# Eliminamos ID si está presente\n",
        "cols_to_drop = [c for c in df_num.columns if 'id' in c.lower() or 'participant' in c.lower()]\n",
        "df_num = df_num.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "# Eliminar columnas constantes\n",
        "df_num = df_num.loc[:, df_num.nunique() > 1]\n",
        "\n",
        "cols_to_ignore = [\"Timestamp_survey\",\"Height\",\"Weight\",\"Left_eye_color\",\"Right_eye_color\",\n",
        "                  \"Hair_color\",\"Handedness\"]\n",
        "\n",
        "df_num = df_num.drop(columns=cols_to_ignore, errors='ignore')\n",
        "\n",
        "# 1.Imputar los valores nulos del peso y la altura con los valores medios\n",
        "mean_weight_kg = participants_df['Weight_kg'].mean()\n",
        "mean_height_cm = participants_df['Height_cm'].mean()\n",
        "\n",
        "df_num['Weight_kg'] = df_num['Weight_kg'].fillna(mean_weight_kg)\n",
        "df_num['Height_cm'] = df_num['Height_cm'].fillna(mean_height_cm)\n",
        "\n",
        "calculated_imc =mean_weight_kg / (mean_height_cm / 100) ** 2\n",
        "\n",
        "df_num['IMC'] = df_num['IMC'].fillna(calculated_imc)\n",
        "\n",
        "# Imputa el resto de nan con 0\n",
        "df_num = df_num.fillna(0)\n",
        "\n",
        "# Estandarizar variables\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df_num)\n",
        "\n",
        "# Calcula PCA\n",
        "pca = PCA(n_components=20)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Varianza explicada\n",
        "explained = pca.explained_variance_ratio_\n",
        "\n",
        "for i, var in enumerate(explained, start=1):\n",
        "    print(f\"PC{i}: {var:.4f} varianza explicada\")\n",
        "\n",
        "print(f\"Varianza acumulada: {np.cumsum(explained)}\")\n",
        "\n",
        "y = df_num[\"Hypertension\"].loc[df_num.index]  # mismo índice\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap='coolwarm', alpha=0.7)\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"PCA coloreado por hipertensión\")\n",
        "plt.colorbar(label=\"Hypertension\")\n",
        "plt.show()\n",
        "\n",
        "# Variables que contribuyen más a cada componente\n",
        "loadings = pd.DataFrame(\n",
        "    pca.components_.T,\n",
        "    index=df_num.columns,\n",
        "    columns=[f\"PC{i}\" for i in range(1, pca.n_components_ + 1)]\n",
        ")\n",
        "\n",
        "loadings.head()\n",
        "print(loadings[\"PC1\"].sort_values(ascending=False).head(10))\n",
        "print(loadings[\"PC2\"].sort_values(ascending=False).head(10))\n",
        "print(loadings[\"PC3\"].sort_values(ascending=False).head(10))\n"
      ],
      "metadata": {
        "id": "osT4sUP4Mesb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "250d4232"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def biplot(score, coeff, labels=None, y=None):\n",
        "    xs = score[:,0] # first principal component scores\n",
        "    ys = score[:,1] # second principal component scores\n",
        "    n = 10 #coeff.shape[0] # number of variables\n",
        "\n",
        "    # Plot the scores\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    if y is not None:\n",
        "        # Ensure y is a numpy array or Series for coloring\n",
        "        if isinstance(y, pd.Series):\n",
        "            y_values = y.values\n",
        "        else:\n",
        "            y_values = y\n",
        "        plt.scatter(xs, ys, c=y_values, cmap='viridis', alpha=0.7)\n",
        "        plt.colorbar(label='Hypertension')\n",
        "    else:\n",
        "        plt.scatter(xs, ys, alpha=0.7)\n",
        "\n",
        "    # Plot the loadings\n",
        "    for i in range(n):\n",
        "        # Scale arrows to make them visible and proportional\n",
        "        # Use max(abs(xs)) and max(abs(ys)) for better scaling if data is centered around 0\n",
        "        scale_x = max(abs(xs)) * 0.7 if len(xs) > 0 else 1.0\n",
        "        scale_y = max(abs(ys)) * 0.7 if len(ys) > 0 else 1.0\n",
        "\n",
        "        plt.arrow(0, 0, coeff[i,0]*scale_x, coeff[i,1]*scale_y, color='r', alpha=0.7, head_width=0.05, head_length=0.05)\n",
        "        if labels is not None:\n",
        "            # Adjust text position for better readability\n",
        "            plt.text(coeff[i,0]*scale_x*1.1, coeff[i,1]*scale_y*1.1, labels[i], color='g', ha='center', va='center')\n",
        "\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.title(\"Biplot de PCA\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "# --- Re-execute data preparation and PCA from cell uijA-vmLSPc3 ---\n",
        "# The pca and X_pca objects are assumed to be from the latest execution in uijA-vmLSPc3\n",
        "# where pca was fitted on 'df_num' (167 columns, including 'Hypertension').\n",
        "\n",
        "# The 'df_num' in the kernel state is the correct source for these feature names.\n",
        "# 'feature_names' must have the same number of elements as 'coeff.shape[0]' (which is 167).\n",
        "feature_names = df_num.columns.tolist()\n",
        "\n",
        "# The 'y' variable for coloring should be 'Hypertension' from df_num\n",
        "y_biplot = df_num['Hypertension']\n",
        "\n",
        "# Call the biplot function with the corrected feature_names\n",
        "biplot(X_pca, pca.components_.T, labels=feature_names, y=y_biplot)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionamos solo columnas numéricas\n",
        "df_num = participants_df.select_dtypes(include=['int64', 'float64']).copy()\n",
        "\n",
        "# Eliminamos ID si está presente\n",
        "cols_to_drop = [c for c in df_num.columns if 'id' in c.lower() or 'participant' in c.lower()]\n",
        "df_num = df_num.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "\n",
        "# Eliminar columnas constantes\n",
        "df_num = df_num.loc[:, df_num.nunique() > 1]\n",
        "\n",
        "cols_to_ignore = [\"Timestamp_survey\",\"Height\",\"Weight\",\"Left_eye_color\",\"Right_eye_color\",\n",
        "                  \"Hair_color\",\"Handedness\"]\n",
        "\n",
        "df_num = df_num.drop(columns=cols_to_ignore, errors='ignore')\n",
        "\n",
        "# Identify columns ending with '_conditions'\n",
        "columns_to_drop_conditions = [col for col in df_num.columns if col.endswith('_conditions')]\n",
        "\n",
        "# Drop these columns from df_num\n",
        "df_num = df_num.drop(columns=columns_to_drop_conditions, errors='ignore')\n",
        "\n",
        "# 1.Imputar los valores nulos del peso y la altura con los valores medios\n",
        "mean_weight_kg = participants_df['Weight_kg'].mean()\n",
        "mean_height_cm = participants_df['Height_cm'].mean()\n",
        "\n",
        "df_num['Weight_kg'] = df_num['Weight_kg'].fillna(mean_weight_kg)\n",
        "df_num['Height_cm'] = df_num['Height_cm'].fillna(mean_height_cm)\n",
        "\n",
        "calculated_imc =mean_weight_kg / (mean_height_cm / 100) ** 2\n",
        "\n",
        "df_num['IMC'] = participants_df['IMC'].fillna(calculated_imc)\n",
        "\n",
        "# Imputa el resto de nan con 0\n",
        "participants_df = participants_df.fillna(0)\n",
        "\n",
        "# Estandarizar variables\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df_num)\n",
        "\n",
        "# Calcula PCA\n",
        "pca = PCA(n_components=20)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Varianza explicada\n",
        "explained = pca.explained_variance_ratio_\n",
        "\n",
        "for i, var in enumerate(explained, start=1):\n",
        "    print(f\"PC{i}: {var:.4f} varianza explicada\")\n",
        "\n",
        "print(f\"Varianza acumulada: {np.cumsum(explained)}\")\n",
        "\n",
        "y = df_num[\"Hypertension\"].loc[df_num.index]  # mismo índice\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap='coolwarm', alpha=0.7)\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"PCA coloreado por hipertensión\")\n",
        "plt.colorbar(label=\"Hypertension\")\n",
        "plt.show()\n",
        "\n",
        "# Variables que contribuyen más a cada componente\n",
        "loadings = pd.DataFrame(\n",
        "    pca.components_.T,\n",
        "    index=df_num.columns,\n",
        "    columns=[f\"PC{i}\" for i in range(1, pca.n_components_ + 1)]\n",
        ")\n",
        "\n",
        "# Call the biplot function\n",
        "# Ensure `df_selected.columns` excludes 'Hypertension' for feature names for loadings\n",
        "feature_names = df_num.drop(columns=['Hypertension']).columns\n",
        "biplot(X_pca, pca.components_.T, labels=feature_names, y=df_num['Hypertension'])\n",
        "\n",
        "loadings.head()\n",
        "print(loadings[\"PC1\"].sort_values(ascending=False).head(10))\n",
        "print(loadings[\"PC2\"].sort_values(ascending=False).head(10))\n",
        "print(loadings[\"PC3\"].sort_values(ascending=False).head(10))\n"
      ],
      "metadata": {
        "id": "uijA-vmLSPc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list with the base columns\n",
        "desired_cols = ['Height_cm', 'Weight_kg','Hypertension']\n",
        "\n",
        "# Add all columns ending with '_conditions' from participants_df\n",
        "for col in participants_df.columns:\n",
        "    if col.endswith('_conditions'):\n",
        "        desired_cols.append(col)\n",
        "\n",
        "# Create the new DataFrame\n",
        "df_selected = participants_df[desired_cols].copy()\n",
        "\n",
        "# 1.Imputar los valores nulos del peso y la altura con los valores medios\n",
        "mean_weight_kg = participants_df['Weight_kg'].mean()\n",
        "mean_height_cm = participants_df['Height_cm'].mean()\n",
        "\n",
        "df_selected['Weight_kg'] = df_selected['Weight_kg'].fillna(mean_weight_kg)\n",
        "df_selected['Height_cm'] = df_selected['Height_cm'].fillna(mean_height_cm)\n",
        "\n",
        "calculated_imc =mean_weight_kg / (mean_height_cm / 100) ** 2\n",
        "\n",
        "df_selected['IMC'] = participants_df['IMC'].fillna(calculated_imc)\n",
        "\n",
        "# Imputa el resto de nan con 0\n",
        "df_selected = df_selected.fillna(0)\n",
        "\n",
        "# Estandarizar variables\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df_selected)\n",
        "\n",
        "# Calcula PCA\n",
        "pca = PCA(n_components=10)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Varianza explicada\n",
        "explained = pca.explained_variance_ratio_\n",
        "\n",
        "for i, var in enumerate(explained, start=1):\n",
        "    print(f\"PC{i}: {var:.4f} varianza explicada\")\n",
        "\n",
        "print(f\"Varianza acumulada: {np.cumsum(explained)}\")\n",
        "\n",
        "y = df_selected[\"Hypertension\"].loc[df_num.index]  # mismo índice\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap='coolwarm', alpha=0.7)\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"PCA coloreado por hipertensión\")\n",
        "plt.colorbar(label=\"Hypertension\")\n",
        "plt.show()\n",
        "\n",
        "# Variables que contribuyen más a cada componente\n",
        "loadings = pd.DataFrame(\n",
        "    pca.components_.T,\n",
        "    index=df_selected.columns,\n",
        "    columns=[f\"PC{i}\" for i in range(1, pca.n_components_ + 1)]\n",
        ")\n",
        "\n",
        "# Call the biplot function\n",
        "# Ensure `df_selected.columns` excludes 'Hypertension' for feature names for loadings\n",
        "feature_names = df_selected.drop(columns=['Hypertension']).columns\n",
        "biplot(X_pca, pca.components_.T, labels=feature_names, y=df_selected['Hypertension'])\n",
        "\n",
        "loadings.head()\n",
        "print(loadings[\"PC1\"].sort_values(ascending=False).head(10))\n",
        "print(loadings[\"PC2\"].sort_values(ascending=False).head(10))\n",
        "print(loadings[\"PC3\"].sort_values(ascending=False).head(10))\n"
      ],
      "metadata": {
        "id": "1sDlcN0rSUAt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}