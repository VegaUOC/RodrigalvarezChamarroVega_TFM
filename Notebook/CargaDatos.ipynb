{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FyHUPeXI_q7"
      },
      "outputs": [],
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Cambiar el directorio de trabajo\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Define the TFM directory path\n",
        "TFM_PATH = '/content/drive/My Drive/TFM'\n",
        "\n",
        "# Change the current working directory to the TFM directory\n",
        "os.chdir(TFM_PATH)\n",
        "print(f\"Current working directory changed to: {os.getcwd()}\")\n",
        "\n",
        "# Add the TFM directory to the Python system path\n",
        "if TFM_PATH not in sys.path:\n",
        "    sys.path.append(TFM_PATH)\n",
        "    print(f\"'{TFM_PATH}' added to Python system path.\")\n",
        "else:\n",
        "    print(f\"'{TFM_PATH}' is already in Python system path.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56604b3a"
      },
      "outputs": [],
      "source": [
        "# Preparación e instalación de librerias\n",
        "\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IndXusAcBJbb"
      },
      "outputs": [],
      "source": [
        "# Carga librerias\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import subprocess\n",
        "import re\n",
        "import extractData\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qA7QSYGsaqRL"
      },
      "outputs": [],
      "source": [
        "# Borra columnas de un dataset\n",
        "def delete_columns_df(df, cols_to_delete):\n",
        "  df = df.drop(columns=cols_to_delete, errors='ignore')\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YgUNbr-J31h"
      },
      "outputs": [],
      "source": [
        "# Obtiene las URLs de descarga desde la página de perfiles de participantes\n",
        "# Función muy adaptada a esta pagina\n",
        "def get_url_download_from_profile(participant_id, soup, web_base,data_list=[]):\n",
        "    # download_urls = []\n",
        "    columns = ['participant_id', 'date', 'data_type', 'download_url']\n",
        "    download_data_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "    # 1. Busca el encabezado 'Uploaded data'\n",
        "    uploaded_data_header = soup.find('h3', string='Uploaded data ')\n",
        "\n",
        "    if uploaded_data_header:\n",
        "        # 2. Despues del encabezado <h3>, busca <div> con la clase profile-data\n",
        "        profile_data_div = uploaded_data_header.find_next_sibling('div', class_='profile-data')\n",
        "\n",
        "        if profile_data_div:\n",
        "            # 3. Busca la tabla dentro del perfil.\n",
        "            table = profile_data_div.find('table')\n",
        "\n",
        "            if table:\n",
        "                # 4. Identifica el índice de la columna download\n",
        "                download_col_idx = -1\n",
        "                date_col_idx = -1\n",
        "                data_type_col_idx = -1\n",
        "                try:\n",
        "                    headers = [th.get_text(strip=True) for th in table.find('thead').find_all('th')]\n",
        "                    for i, header in enumerate(headers):\n",
        "                        if header == 'Download':\n",
        "                            download_col_idx = i\n",
        "                        elif header == 'Date':\n",
        "                            date_col_idx = i\n",
        "                        elif header == 'Data type':\n",
        "                            data_type_col_idx = i\n",
        "                except Exception as e:\n",
        "                    print(f\"Error occurred while parsing table headers: {e}\")\n",
        "\n",
        "                if download_col_idx != -1:\n",
        "                    # 5. Itera por cada una de las columas para coger la url de descarga\n",
        "                    for row in table.find_all('tr'):    #find('tbody').\n",
        "                        full_url = ''\n",
        "                        # 6. Para cada fila, accede a la celda (<td>) que corresponde a la columna 'Download'\n",
        "                        cells = row.find_all('td')\n",
        "                        if len(cells) > download_col_idx:\n",
        "                            download_cell = cells[download_col_idx]\n",
        "                            # 7. Dentro de esta celda,extrae el atributo href del <a> tag.\n",
        "                            download_link_tag = download_cell.find('a')\n",
        "                            if download_link_tag and download_link_tag.has_attr('href'):\n",
        "                                relative_href = download_link_tag['href']\n",
        "                                # 8. Si la URL es relativa la convierte en full\n",
        "                                if relative_href.startswith('/'):\n",
        "                                    full_url = web_base.rstrip('/') + relative_href\n",
        "                                else:\n",
        "                                    full_url = relative_href\n",
        "                            # Rellena el dataframe cuando la lista de tipos está vacia\n",
        "                            # Cuando la lista no está vacia y el tipo de datos se encuentra en la lista\n",
        "                            if (not data_list) or (len(data_list) > 0 and cells[data_type_col_idx].text.strip() in data_list):\n",
        "                                new_row_data = {\n",
        "                                  'participant_id': participant_id,\n",
        "                                  'date': cells[date_col_idx].text.strip(),\n",
        "                                  'data_type': cells[data_type_col_idx].text.strip(),\n",
        "                                  'download_url': full_url\n",
        "                                }\n",
        "                                new_row_df = pd.DataFrame([new_row_data])\n",
        "                                download_data_df = pd.concat([download_data_df, new_row_df], ignore_index=True)\n",
        "\n",
        "                else:\n",
        "                    print(f\"'Download' column not found in table for {participant_id}.\")\n",
        "            else:\n",
        "                print(f\"No table found in 'profile-data' div for {participant_id}.\")\n",
        "        else:\n",
        "            print(f\"'profile-data' div not found after 'Uploaded data' header for {participant_id}.\")\n",
        "    else:\n",
        "        print(f\"'Uploaded data' header not found for {participant_id}.\")\n",
        "    return download_data_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYis8tT4tQuf"
      },
      "outputs": [],
      "source": [
        "# Dada una lista de participantes, descarga todos sus datos y los almacena en\n",
        "# el directorio indicado\n",
        "def get_data_participants(participant_list,web_base,web_profile,genetic_path,data_type_list=[],download = False):\n",
        "\n",
        "    columns = ['participant_id', 'date', 'data_type', 'download_url']\n",
        "    info_participants_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "    for participant_id in participant_list:\n",
        "        print(f\"\\nProcessing participant: {participant_id}\")\n",
        "\n",
        "        if download:\n",
        "            participant_dir = os.path.join(genetic_path, participant_id)\n",
        "            os.makedirs(participant_dir, exist_ok=True)\n",
        "            print(f\"Ensured directory exists: {participant_dir}\")\n",
        "\n",
        "        profile_url = web_profile + participant_id\n",
        "        print(f\"Fetching profile from: {profile_url}\")\n",
        "\n",
        "        try:\n",
        "            final_url, filename, soup = extractData.get_html_parser(profile_url)\n",
        "\n",
        "            # Si no devuelve ningún contenido entonces continua con el siguiente perfil\n",
        "            # Esto significa que la página del participante no se ha encontrado\n",
        "            if not soup:\n",
        "                print(f\"No soup found for {participant_id}.\")\n",
        "                continue\n",
        "            # Busca las páginas de descarga bajo el perfil del paciente y obtiene las urls\n",
        "            download_df = get_url_download_from_profile(participant_id, soup, web_base,data_type_list)\n",
        "            # Concatenar resultados\n",
        "            info_participants_df = pd.concat([info_participants_df, download_df], ignore_index=True)\n",
        "            download_urls = download_df['download_url'].tolist()\n",
        "\n",
        "            if (not download) | (not download_urls):\n",
        "                print(f\"No download links found for {participant_id}.\")\n",
        "            else:\n",
        "                print(f\"Found {len(download_urls)} download links for {participant_id}.\")\n",
        "                # Execute each wget command using the extracted URLs\n",
        "                for url in download_urls:\n",
        "                    if url:\n",
        "                        #Obtiene la url real de la página\n",
        "                        # orig_url = get_real_url(url)\n",
        "\n",
        "                        # Parsea la página\n",
        "                        orig_url, filename, soup = extractData.get_html_parser(url)\n",
        "\n",
        "                        #Obtiene la lista de ficheros a descargar\n",
        "                        if not soup:\n",
        "                          download_file = []\n",
        "                        else:\n",
        "                          download_file = extractData.get_list_genetic_data(soup, orig_url)\n",
        "\n",
        "                        if len(download_file) == 0:\n",
        "                            # Si no ha encontrado fichero a descargar, trata de descargar la url original\n",
        "                            print(f\"Descargando... {orig_url}\")\n",
        "                            extractData.get_download_file(orig_url,participant_dir,filename)\n",
        "                        else:\n",
        "                            # Descarga uno a uno todos los datos genéticos\n",
        "                            for url_file in download_file:\n",
        "                              print(f\"Descargando... {url_file}\")\n",
        "                              extractData.get_download_file(url_file,participant_dir,None)\n",
        "\n",
        "        except requests.exceptions.HTTPError as errh:\n",
        "            print(f\"HTTP Error for {participant_id}: {errh}\")\n",
        "        except requests.exceptions.ConnectionError as errc:\n",
        "            print(f\"Error Connecting for {participant_id}: {errc}\")\n",
        "        except requests.exceptions.Timeout as errt:\n",
        "            print(f\"Timeout Error for {participant_id}: {errt}\")\n",
        "        except requests.exceptions.RequestException as err:\n",
        "            print(f\"An unexpected error occurred for {participant_id}: {err}\")\n",
        "\n",
        "    return info_participants_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28CGChJZHzf7"
      },
      "outputs": [],
      "source": [
        "# Define variables\n",
        "# Define el directorio y el nombre del archivo en variables\n",
        "folder_drive = '/content/drive/MyDrive/TFM/data/'\n",
        "survey_data =\"surveys/\"\n",
        "genetic_path = folder_drive + 'genetic_data/'\n",
        "file_circulatoryDisease= 'PGPTrait&DiseaseSurvey2012_CirculatorySystem-20181010220109.csv'\n",
        "file_general = 'PGPParticipantSurvey-20181010220019.csv'\n",
        "file_basic_phenotypes = 'PGPBasicPhenotypesSurvey2015-20181010214636.csv'\n",
        "cols_to_delete =['Do not touch!']\n",
        "col_name_genetic = 'Have you uploaded genetic data to your PGP participant profile?'\n",
        "col_name_healthRecord = 'Have you uploaded health record data using our Google Health or Microsoft Healthvault interfaces?'\n",
        "column_name = 'Have you ever been diagnosed with one of the following conditions?'\n",
        "pgp_web_base = 'https://my.pgp-hms.org'\n",
        "pgp_web_profile = pgp_web_base + '/profile/'\n",
        "\n",
        "genome_list = ['biometric data - CSV or similar','23andMe','\tComplete Genomics','Counsyl','DeCode','\tFamily Tree DNA','Gencove',\n",
        "                  'Illumina','Knome','Navigenics','\tPathway genomics','Veritas Genetics','AncestryDNA','Ancestry DNA','Ancestry','MyHeritage',\n",
        "               'Geno 2.0 Ancestry Report']\n",
        "complete_genome_list =['Complete Genomics','Veritas Genetics',\n",
        "                       'Nebula','Sequencing.com','Full Genomes','whole genome','Nebula Genomics VCF file',\n",
        "                       'Dante Labs full genome VCF Indel','genetic data - HiFi Reads','dante labs 2021 bloodkit whole genome','\tnebula genomics bam file',\n",
        "                       'nebula genomics data vcf','VCF from Dante Labs vs GRCh37 (gz)','genetic data - Dante Labs','from health nucleus',\n",
        "                       'Helix','Dante Labs WGS','Helix Exome','Dante Labs VCF File','.vcf.gz','\tgenetic data - FGC Chromium 60x','genetic data - CeGaT',\n",
        "                       'genetic data - FullDNA','genetic data-Full Genomes 60x chromium','Genetic data -- Full Genomes',\n",
        "                       'Zip of 4x full genome analysis from Full Genome Corp','\tFull Genomes Corporation, Inc. (Novogene)']\n",
        "# Hay todavía más tipos de todos\n",
        "health_records_list =['health records - CCR XML','health records - PDF or text','image','Microbiome','Color Health']\n",
        "col_id = 'Participant'\n",
        "sep=','"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ7v2jqKVOCD"
      },
      "outputs": [],
      "source": [
        "# Carga el fichero con la encuesta de condiciones cardiovasculares\n",
        "folder = folder_drive + survey_data\n",
        "df_data = extractData.load_csv_to_dataframe(folder, file_circulatoryDisease)\n",
        "\n",
        "# Extraer todas las condiciones del fichero\n",
        "all_cv_conditions = extractData.extract_conditions(df_data,column_name,sep)\n",
        "\n",
        "# Crear dataset de participantes\n",
        "df_participants = extractData.create_conditions_df(df_data, all_cv_conditions,col_id,column_name,sep)\n",
        "\n",
        "# Cargar el dataset general\n",
        "df_general = extractData.load_csv_to_dataframe(folder, file_general)\n",
        "\n",
        "#Enlaza los datos df_general al df_participants\n",
        "df_participants = pd.merge(df_participants, df_general, on='Participant', how='left')\n",
        "\n",
        "# Borra columnas del dataset\n",
        "df_participants = delete_columns_df(df_participants, cols_to_delete)\n",
        "\n",
        "# Cargar el dataset de fenotipos básicos\n",
        "df_basicphenotypes = extractData.load_csv_to_dataframe(folder, file_basic_phenotypes)\n",
        "\n",
        "#Enlaza los datos df_participants\n",
        "df_participants = pd.merge(df_participants, df_basicphenotypes, on='Participant', how='left')\n",
        "\n",
        "# Borra columnas del dataset\n",
        "df_participants = delete_columns_df(df_participants, cols_to_delete)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb81f4ab"
      },
      "outputs": [],
      "source": [
        "# Condiciones de filtrado\n",
        "# Pacientes con hipertensión que hayan subido registros genéticos y de salud\n",
        "condition_hypertension = df_participants['Hypertension'] == 1\n",
        "condition_genetic_data = df_participants[col_name_genetic].isin(['Yes','Yes, I have uploaded genetic data'])\n",
        "condition_health_record = df_participants[col_name_healthRecord] == 'Yes'\n",
        "\n",
        "# Aplicar las condiciones al filtro\n",
        "filtered_participants_df = df_participants[condition_hypertension & condition_genetic_data]\n",
        "\n",
        "# Obtener los IDs de los participantes\n",
        "participants_list = filtered_participants_df['Participant'].unique().tolist()\n",
        "\n",
        "# Número total de participantes\n",
        "total_participants_count = len(participants_list)\n",
        "\n",
        "# Resultados\n",
        "print(f\"Nº total de participantes que cumplen los criterios: {total_participants_count}\")\n",
        "#print(\"\\nListado de participantes que cumplen los criterios:\")\n",
        "#for participant_id in participants_list:\n",
        "#    print(participant_id)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f19ec586"
      },
      "outputs": [],
      "source": [
        "# Crea un directorio para almacenar la información genética\n",
        "if not os.path.exists(genetic_path):\n",
        "    os.makedirs(genetic_path)\n",
        "    print(f\"Directory '{genetic_path}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Directory '{genetic_path}' already exists.\")\n",
        "\n",
        "# Itera por participantes y descarga todos los datos de los participantes con hipertensión\n",
        "# que tienen datos genéticos\n",
        "print(f\"Starting data download for {len(participants_list)} participants.\")\n",
        "data_desc_df = get_data_participants(participants_list,pgp_web_base,pgp_web_profile,genetic_path,data_type_list=[],download = False)\n",
        "\n",
        "print(f\"Nª de participantes con hipertension y que indica en los registros que tienen datos genómicos {len(data_desc_df[\"participant_id\"].unique())}\")\n",
        "data_desc_df.to_csv(folder_drive +'participantes_hipertension_geneticos.csv', sep=',', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Celda de test\n",
        "# participant_list =['hu654B61']\n",
        "participant_list = ['huE31062']\n",
        "\n",
        "# print(get_real_url('https://my.pgp-hms.org/user_file/download/96'))\n",
        "\n",
        "\n",
        "original_url = 'https://my.pgp-hms.org/user_file/download/96'\n",
        "final_url, filename, soup = extractData.get_html_parser(original_url)\n",
        "print(final_url)\n",
        "print(filename)\n",
        "\n",
        "data_desc_df = get_data_participants(participant_list, pgp_web_base, pgp_web_profile, genetic_path, data_type_list=[], download=True)"
      ],
      "metadata": {
        "id": "nnS8rK8eXiCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIZsYZ1_uV8H"
      },
      "outputs": [],
      "source": [
        "# Extrae los ids de los participantes que tienen hipertensión\n",
        "# Busca si tienen datos genéticos usando el crawler\n",
        "# Indica el tipo de datos genéticos que tiene\n",
        "\n",
        "# Listado de IDs de participantes con hipertensión\n",
        "condition_hypertension = df_participants['Hypertension'] == 1\n",
        "\n",
        "# Aplicar las condiciones al filtro\n",
        "filtered_participants_hypertension_df = df_participants[condition_hypertension]\n",
        "\n",
        "# Obtener los IDs de los participantes\n",
        "participants_hypertension_list = filtered_participants_hypertension_df['Participant'].unique().tolist()\n",
        "\n",
        "# Número total de participantes\n",
        "total_participants_count = len(participants_hypertension_list)\n",
        "hypertension_folder = folder_drive + 'hypertension_data/'\n",
        "\n",
        "participant_info_genomic_data = get_data_participants(participants_hypertension_list,pgp_web_base,pgp_web_profile,hypertension_folder,complete_genome_list,download=False)\n",
        "\n",
        "print(f\"Nª de participantes con hipertension y datos genómicos {len(participant_info_genomic_data[\"participant_id\"].unique())}\")\n",
        "\n",
        "participant_hyper_list = participant_info_genomic_data['participant_id'].unique().tolist()\n",
        "\n",
        "participant_info_genomic_data.to_csv(folder_drive +'participantes_hipertension.csv', sep=',', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjZ78WQ7uYjq"
      },
      "outputs": [],
      "source": [
        "# Recoge sólo la información de aquellos pacientes que tienen hipertensión y que tienen \"registros genéticos completos\"\n",
        "participants_hyper_gen_df = filtered_participants_hypertension_df[df_participants['Participant'].isin(participant_info_genomic_data['participant_id'].unique().tolist())]\n",
        "\n",
        "print(len(participants_hyper_gen_df))\n",
        "print(len(participants_hyper_gen_df['Participant'].unique()))\n",
        "\n",
        "participant_hyper_list = participant_info_genomic_data['participant_id'].unique().tolist()\n",
        "\n",
        "print(len(participant_hyper_list))\n",
        "print(participant_hyper_list)\n",
        "\n",
        "print(participants_hyper_gen_df.columns)\n",
        "# print(participants_hyper_gen_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcjhqW-PjH3Y"
      },
      "outputs": [],
      "source": [
        "# Extrae los ids de los participantes que NO tienen hipertensión\n",
        "# Busca si tienen datos genéticos usando el crawler\n",
        "# Indica el tipo de datos genéticos que tiene\n",
        "\n",
        "# Listado de IDs de participantes SIN hipertensión\n",
        "condition = df_participants['Hypertension'] == 0\n",
        "\n",
        "# Aplicar las condiciones al filtro\n",
        "filtered_participants_control_df = df_participants[condition]\n",
        "\n",
        "# Obtener los IDs de los participantes\n",
        "participants_control_list = filtered_participants_control_df['Participant'].unique().tolist()\n",
        "\n",
        "# Número total de participantes\n",
        "total_participants_count = len(participants_control_list)\n",
        "\n",
        "hypertension_folder = folder_drive + 'hypertension_data/'\n",
        "print(f\"Nª de participantes SIN hipertension: {total_participants_count}\")\n",
        "\n",
        "participant_info_genomic_data = get_data_participants(participants_control_list,pgp_web_base,pgp_web_profile,hypertension_folder,complete_genome_list,download=False)\n",
        "\n",
        "print(f\"Nª de participantes SIN hipertension y con datos genómicos {len(participant_info_genomic_data[\"participant_id\"].unique())}\")\n",
        "\n",
        "participant_control_list = participant_info_genomic_data['participant_id'].unique().tolist()\n",
        "\n",
        "participant_info_genomic_data.to_csv(folder_drive +'participantes_control.csv', sep=',', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjchYlhuj4Ee"
      },
      "outputs": [],
      "source": [
        "# Recoge sólo la información de aquellos pacientes que NO tienen hipertensión y que tienen \"registros genéticos completos\"\n",
        "participants_control_gen_df = filtered_participants_control_df[df_participants['Participant'].isin(participant_info_genomic_data['participant_id'].unique().tolist())]\n",
        "\n",
        "print(len(participants_control_gen_df))\n",
        "print(len(participants_control_gen_df['Participant'].unique()))\n",
        "\n",
        "participant_control_list = participant_info_genomic_data['participant_id'].unique().tolist()\n",
        "\n",
        "print(len(participant_control_list))\n",
        "print(participant_control_list)\n",
        "\n",
        "print(participants_control_gen_df.columns)\n",
        "# print(participants_hyper_gen_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc08945e",
        "outputId": "a58757bc-5139-4802-f972-95f7c7658281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of participants in `participants_hypertension_list`: 414\n",
            "First 5 participants: ['hu68A2D7', 'hu46125C', 'hu1EE386', 'huE31062', 'hu7123C1']\n"
          ]
        }
      ],
      "source": [
        "# Seleccionar IDs de participantes con hipertensión\n",
        "condition_hypertension = df_participants['Hypertension'] == 1\n",
        "filtered_participants_hypertension_df = df_participants[condition_hypertension]\n",
        "participants_hypertension_list = filtered_participants_hypertension_df['Participant'].unique().tolist()\n",
        "\n",
        "print(f\"Number of participants in `participants_hypertension_list`: {len(participants_hypertension_list)}\")\n",
        "print(f\"First 5 participants: {participants_hypertension_list[:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "261e0926"
      },
      "outputs": [],
      "source": [
        "public_genetic_data_url = 'https://my.pgp-hms.org/public_genetic_data'\n",
        "print(f\"Public genetic data URL set to: {public_genetic_data_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66c83f34"
      },
      "outputs": [],
      "source": [
        "# Hace un crawler de la página 'https://my.pgp-hms.org/public_genetic_data'\n",
        "# recogiendo los metadatos de todos los ficheros cargados por los usuarios\n",
        "def parse_file_size(size_str):\n",
        "    if pd.isna(size_str):\n",
        "        return None, None\n",
        "    match = re.match(r'(\\d+\\.?\\d*)\\s*([KMGT]?B)', str(size_str))\n",
        "    if match:\n",
        "        value = float(match.group(1))\n",
        "        unit = match.group(2).strip()\n",
        "        return value, unit\n",
        "    return None, None\n",
        "\n",
        "\n",
        "def scrape_public_genetic_data(public_genetic_data_url, pgp_web_base):\n",
        "    data_records = []\n",
        "    try:\n",
        "        orig_url, filename, soup = extractData.get_html_parser(public_genetic_data_url)\n",
        "\n",
        "        table = soup.find('table')\n",
        "        if not table:\n",
        "            print(\"Could not find table on the page.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        headers = [th.get_text(strip=True) for th in table.find('thead').find_all('th')]\n",
        "        print(f\"Found table headers: {headers}\")\n",
        "\n",
        "        # Map actual table headers to desired DataFrame column names\n",
        "        header_mapping_to_target = {\n",
        "            'Participant': 'participant_id',\n",
        "            'Published': 'publication_date',\n",
        "            'Data type': 'data_type',\n",
        "            'Name': 'file_name',\n",
        "            'Download': 'download_url'\n",
        "        }\n",
        "\n",
        "        # Get column indices for easy access based on actual headers\n",
        "        col_indices = {}\n",
        "        found_all_expected_headers = True\n",
        "        for actual_header, target_col_name in header_mapping_to_target.items():\n",
        "            if actual_header in headers:\n",
        "                col_indices[target_col_name] = headers.index(actual_header)\n",
        "            else:\n",
        "                print(f\"Missing expected header in page: '{actual_header}'. Cannot proceed with scraping.\")\n",
        "                found_all_expected_headers = False\n",
        "                break\n",
        "\n",
        "        # Add a mapping for file size that points to the 'Download' column index for extraction\n",
        "        if 'download_url' in col_indices:\n",
        "            col_indices['file_size_source'] = col_indices['download_url'] # Use download column to get file size\n",
        "        else:\n",
        "            print(\"Missing 'Download' header. Cannot extract file size.\")\n",
        "            found_all_expected_headers = False\n",
        "\n",
        "        if not found_all_expected_headers:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        import re\n",
        "        for row in table.find_all('tr'):\n",
        "            cols = row.find_all('td')\n",
        "            if len(cols) > 0: # Ensure it's a data row\n",
        "                record = {}\n",
        "                participant_id = cols[col_indices['participant_id']].get_text(strip=True)\n",
        "                if ',' in participant_id:\n",
        "                  parts = participant_id.split(',', 1) # Split only on the first comma\n",
        "                  record['participant_id'] = parts[0].strip()\n",
        "                  record['source'] = parts[1].strip()\n",
        "                else:\n",
        "                  record['participant_id'] = participant_id\n",
        "                  record['source'] = None\n",
        "\n",
        "                record['publication_date'] = cols[col_indices['publication_date']].get_text(strip=True)\n",
        "                record['data_type'] = cols[col_indices['data_type']].get_text(strip=True)\n",
        "                record['file_name'] = cols[col_indices['file_name']].get_text(strip=True)\n",
        "\n",
        "                # Extract file size\n",
        "                file_size_text = cols[col_indices['file_size_source']].get_text(separator=' ', strip=True)\n",
        "                file_size_match = re.search(r'\\((.*?)\\)', file_size_text)\n",
        "                if file_size_match:\n",
        "                    record['file_size'] = file_size_match.group(1).strip()\n",
        "                    record['file_size_value'], record['file_size_unit'] = parse_file_size(record['file_size'])\n",
        "                else:\n",
        "                    record['file_size'] = None\n",
        "                    record['file_size_value'] = 0\n",
        "                    record['file_size_unit'] = ''\n",
        "\n",
        "                # print(f\"'##### Participant:' {record['participant_id']} '#######'\")\n",
        "                download_link = cols[col_indices['download_url']].find('a')\n",
        "                full_download_url = ''\n",
        "                if download_link and download_link.has_attr('href'):\n",
        "                    relative_href = download_link['href']\n",
        "                    if relative_href.startswith('/'):\n",
        "                        full_download_url = pgp_web_base.rstrip('/') + relative_href\n",
        "                    else:\n",
        "                        full_download_url = relative_href\n",
        "                record['download_url'] = full_download_url\n",
        "\n",
        "                data_records.append(record)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching or parsing URL: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during scraping: {e}\")\n",
        "\n",
        "    public_genetic_df = pd.DataFrame(data_records)\n",
        "    return public_genetic_df\n",
        "\n",
        "# Recorre la página https://my.pgp-hms.org/public_genetic_data y carga en una tabla sus datos\n",
        "public_genetic_df = scrape_public_genetic_data(public_genetic_data_url, pgp_web_base)\n",
        "\n",
        "print(\"Scraped Public Genetic Data:\")\n",
        "print(public_genetic_df.head())\n",
        "print(f\"Total records scraped: {len(public_genetic_df)}\")\n",
        "\n",
        "# Guarda la información en un fichero\n",
        "public_genetic_df.to_csv(folder_drive +'public_genetic_data.csv', sep=',', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}